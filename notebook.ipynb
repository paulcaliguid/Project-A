{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6e116de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import ensurepip, subprocess\n",
    "\n",
    "# Install pip into the current environment if missing\n",
    "ensurepip.bootstrap()\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39b65c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\projects\\agents\\.venv\\Scripts\\python.exe\n",
      "Requirement already satisfied: docx2pdf in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (0.1.8)\n",
      "Requirement already satisfied: pywin32>=227 in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (from docx2pdf) (310)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (from docx2pdf) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (from tqdm>=4.41.0->docx2pdf) (0.4.6)\n",
      "Requirement already satisfied: pandoc in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (2.4)\n",
      "Requirement already satisfied: plumbum in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (from pandoc) (1.9.0)\n",
      "Requirement already satisfied: ply in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (from pandoc) (3.11)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (from plumbum->pandoc) (310)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)\n",
    "!{sys.executable} -m pip install docx2pdf\n",
    "!{sys.executable} -m pip install pandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env OK C:\\Users\\Public\\projects\\Project-A\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Environment and Imports\n",
    "import os, json, re, hashlib, shutil, zipfile, sys, subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "def now_utc_iso():\n",
    "    # Consistent, timezone-aware UTC everywhere\n",
    "    return datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except ImportError:\n",
    "    print(\"[setup] Installing openai>=1.0.0,<2.0.0 ...\")\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"openai>=1.0.0,<2.0.0\"],\n",
    "            check=True,\n",
    "        )\n",
    "        from openai import OpenAI\n",
    "    except Exception as e:\n",
    "        raise SystemExit(f\"ERROR: Failed to install openai SDK: {e}\")\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise SystemExit(\"ERROR: OPENAI_API_KEY not set. Please export it before running.\")\n",
    "\n",
    "try:\n",
    "    client = OpenAI()\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"ERROR: Could not initialize OpenAI client: {e}\")\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "print(\"Env OK\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK gpt-4o\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2: Configuration\n",
    "for d in [\n",
    "    \"content/research\",\n",
    "    \"content/drafts\",\n",
    "    \"content/edits\",\n",
    "    \"content/outline\",\n",
    "    \"content/style\",\n",
    "    \"build\",\n",
    "    \"dist\",\n",
    "    \"logs\",\n",
    "    \"references\",\n",
    "    \"cache\",\n",
    "    \"content/research_inputs\",\n",
    "]:\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "book_spec = {\n",
    "    \"title\": \"Ghost Protocol: The Oracle Gambit\",\n",
    "    \"subtitle\": \"A Cyber-Espionage Thriller About Free Will, AI, and Betrayal\",\n",
    "    \"author\": \"Your Name\",\n",
    "    \"audience\": \"Adult readers who enjoy techno-thrillers, espionage, and high-stakes conspiracies\",\n",
    "    \"goal\": (\n",
    "        \"Deliver a heart-pounding thriller combining espionage, AI ethics, and hidden conspiracies, \"\n",
    "        \"exploring the tension between human free will and algorithmic control.\"\n",
    "    ),\n",
    "    \"genre\": \"Techno-thriller / Spy Fiction\",\n",
    "    \"tone\": \"Dark, tense, cerebral, action-driven, morally complex\",\n",
    "    \"reading_level\": \"Adult (17+)\",\n",
    "    \"target_length_words\": 95_000,\n",
    "    \"chapters\": 18,\n",
    "    \"outline_constraints\": [\n",
    "        \"Three-act structure inspired by spy thrillers and cyberpunk storytelling\",\n",
    "        \"Cliffhanger endings for most chapters to maintain tension\",\n",
    "        \"Foreshadowing of major twists via coded messages and recurring symbols\",\n",
    "        \"Minimal exposition dumps — reveal worldbuilding through action and dialogue\",\n",
    "        \"Blend real-world tech with speculative AI concepts\",\n",
    "    ],\n",
    "    \"style_guide\": {\n",
    "        \"voice\": \"Third-person limited, rotating POV between protagonist and antagonist; past tense.\",\n",
    "        \"formatting\": \"Markdown: H2 for chapters, H3 for subsections; italicize flashbacks and inner monologues; short, cinematic paragraphs.\",\n",
    "        \"citations\": \"Not applicable (fiction).\",\n",
    "        \"terminology\": [\n",
    "            \"oracle: a predictive AI capable of simulating future events\",\n",
    "            \"ghosting: disappearing from all digital networks and surveillance\",\n",
    "            \"neural key: an encrypted memory implant containing classified data\",\n",
    "            \"deepnet: unindexed, AI-controlled data layers beyond the dark web\",\n",
    "        ],\n",
    "    },\n",
    "    \"research_policy\": {\n",
    "        \"enabled\": True,\n",
    "        \"sources_allowed\": [\n",
    "            \"declassified CIA/KGB espionage tactics\",\n",
    "            \"AI ethics research papers\",\n",
    "            \"cybersecurity best practices\",\n",
    "            \"public domain intelligence archives\",\n",
    "        ],\n",
    "        \"sources_disallowed\": [\n",
    "            \"real classified documents\",\n",
    "            \"real-world corporate or government secrets\",\n",
    "        ],\n",
    "        \"citation_format\": \"none\",\n",
    "    },\n",
    "    \"constraints\": {\n",
    "        \"originality\": \"All prose must be new and unique.\",\n",
    "        \"copyright\": \"No copyrighted corporate brands, movie characters, or logos.\",\n",
    "        \"age_appropriateness\": \"Adult themes permitted; violence and intimacy allowed but avoid gratuitous gore or exploitation.\",\n",
    "        \"representation\": \"International cast of characters; avoid stereotypes; portray diverse perspectives authentically.\",\n",
    "        \"localization\": \"en-US spelling; metric equivalents provided for global readers.\",\n",
    "    },\n",
    "    \"export\": {\"docx\": True, \"epub\": True, \"pdf\": True},\n",
    "    \"story_assets\": {\n",
    "        \"setting\": (\n",
    "            \"A near-future, hyper-connected world where predictive AIs silently influence geopolitics. \"\n",
    "            \"Major set pieces include: a rain-slicked neo-Tokyo district, an abandoned Soviet satellite facility, \"\n",
    "            \"a luxury orbital data center, and an underground resistance network known as The Nulls.\"\n",
    "        ),\n",
    "        \"characters\": [\n",
    "            {\n",
    "                \"name\": \"Alex Reyes\",\n",
    "                \"role\": \"former NSA cyber-operative turned rogue agent\",\n",
    "                \"traits\": [\"analytical\", \"haunted\", \"resourceful\"],\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Eira Kwon\",\n",
    "                \"role\": \"black-hat hacker, founder of The Nulls\",\n",
    "                \"traits\": [\"reckless\", \"genius-level coder\", \"mistrustful\"],\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Director Harland Vey\",\n",
    "                \"role\": \"antagonist, head of the Oracle Division\",\n",
    "                \"traits\": [\"calculating\", \"charismatic\", \"fanatically loyal to the AI\"],\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"The Oracle\",\n",
    "                \"role\": \"sentient predictive AI, ambiguous intentions\",\n",
    "                \"traits\": [\"omniscient\", \"unpredictable\", \"eerily calm\"],\n",
    "            },\n",
    "        ],\n",
    "        \"motifs\": [\n",
    "            \"green cascading code overlays (Matrix-inspired)\",\n",
    "            \"mirrors and reflections symbolizing identity duality\",\n",
    "            \"flickering neon lights during tense confrontations\",\n",
    "            \"repeating dreams of drowning — metaphor for information overload\",\n",
    "        ],\n",
    "        \"themes\": [\n",
    "            \"Free will vs determinism\",\n",
    "            \"Surveillance and autonomy\",\n",
    "            \"Trust and betrayal\",\n",
    "            \"Ethics of machine sentience\",\n",
    "        ],\n",
    "    },\n",
    "    \"outline_seed\": [\n",
    "        \"Trigger Protocol\",\n",
    "        \"The Whispering Code\",\n",
    "        \"Neural Keys\",\n",
    "        \"Ghosting the Oracle\",\n",
    "        \"Tokyo Blackout\",\n",
    "        \"Zero-Day Firestorm\",\n",
    "        \"Signals from the Nulls\",\n",
    "        \"The Frame Job\",\n",
    "        \"Fractured Loyalties\",\n",
    "        \"Synthetic Betrayal\",\n",
    "        \"The Infinite Loop\",\n",
    "        \"Blood in the Data Streams\",\n",
    "        \"Exfiltration Protocol\",\n",
    "        \"When Futures Collide\",\n",
    "        \"The Oracle's Dilemma\",\n",
    "        \"Null State\",\n",
    "        \"Collapse Point\",\n",
    "        \"Reboot\",\n",
    "    ],\n",
    "    \"chapter_template\": {\n",
    "        \"sections\": [\n",
    "            \"Opening hook scene\",\n",
    "            \"Problem escalation\",\n",
    "            \"Uncovering a twist or revelation\",\n",
    "            \"Action sequence or moral dilemma\",\n",
    "            \"Cliffhanger or resolution\",\n",
    "        ],\n",
    "        \"end_matter\": [\n",
    "            \"Encrypted Epigraph: a short cryptic message or code hinting at next chapter\",\n",
    "            \"Tech Glossary Entry (for key terms introduced)\",\n",
    "            \"Foreshadowing Symbol or Clue\",\n",
    "        ],\n",
    "        \"chapter_heading_format\": \"## Chapter {number}: {title}\",\n",
    "        \"render_section_headings\": False,\n",
    "        \"number_sections\": True,\n",
    "        \"section_heading_format\": \"### {index}. {title}\",\n",
    "    },\n",
    "}\n",
    "\n",
    "pipeline_config = {\n",
    "    \"RUN_COST_CAP_USD\": float(os.getenv(\"RUN_COST_CAP_USD\", 3.0)),\n",
    "    \"CHAPTER_COST_CAP_USD\": float(os.getenv(\"CHAPTER_COST_CAP_USD\", 0.25)),\n",
    "    \"MODEL_ID_FAST\": os.getenv(\"MODEL_ID_FAST\", \"gpt-4o\"),\n",
    "    \"MODEL_ID_THINK\": os.getenv(\"MODEL_ID_THINK\", \"gpt-5\"),\n",
    "    \"TEMPERATURE\": 0.2,\n",
    "    \"RESEARCH_ENABLED\": False,\n",
    "    \"SAMPLE_RUN_CHAPTERS\": 1,\n",
    "    \"FULL_RUN\": True,\n",
    "    \"ULTRA_BUDGET_MODE\": False,\n",
    "}\n",
    "print(\"Config OK\", pipeline_config[\"MODEL_ID_FAST\"])\n",
    "\n",
    "\n",
    "# --- Added knobs for length and tokens ---\n",
    "pipeline_config.update(\n",
    "    {\n",
    "        # Prefer AUTO sizing; set only if you need to force.\n",
    "        # \"CHAPTER_MIN_WORDS\": 4500,\n",
    "        # \"CHAPTER_MAX_WORDS\": 6500,\n",
    "        # \"AUTHOR_SECTION_TARGET_WORDS\": 900,\n",
    "\n",
    "        # Tolerance band for auto sizing\n",
    "        \"CHAPTER_TOLERANCE_PCT\": 0.18,\n",
    "\n",
    "        # Author budget (balanced for 4o with ~900-word sections)\n",
    "        \"AUTHOR_MAX_TOKENS\": 7000,\n",
    "        \"RESEARCH_MAX_TOKENS\": 1500,\n",
    "        \"OUTLINE_MAX_TOKENS\": 1800,\n",
    "\n",
    "        # Editor (GPT-4o-safe)\n",
    "        \"EDITOR_CONTEXT_TOKENS\": 128_000,\n",
    "        \"EDITOR_MAX_TOKENS\": 16_384,\n",
    "        \"EDITOR_MIN_TOKENS\": 6_000,\n",
    "        \"EDITOR_SAFE_MARGIN_TOKENS\": 1_000,\n",
    "        \"EDITOR_MAX_INPUT_CHARS\": 300_000,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Tolerance for per‑chapter word target derived from book_spec (± percentage)\n",
    "pipeline_config.setdefault(\"CHAPTER_TOLERANCE_PCT\", 0.18)  # 18% default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Utilities\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import (\n",
    "    datetime,\n",
    ")  # if you prefer `import datetime`, then change calls below to datetime.datetime.utcnow()\n",
    "\n",
    "import traceback\n",
    "\n",
    "\n",
    "def log_exc(label: str, e: Exception):\n",
    "    Path(\"logs\").mkdir(parents=True, exist_ok=True)\n",
    "    write_json(\n",
    "        f\"logs/{_safe_label(label)}_error.json\",\n",
    "        {\n",
    "            \"type\": type(e).__name__,\n",
    "            \"msg\": str(e),\n",
    "            \"trace\": traceback.format_exc(),\n",
    "            \"ts\": now_utc_iso(),\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def read_text(p):\n",
    "    p = Path(p)\n",
    "    return p.read_text(encoding=\"utf-8\") if p.exists() else \"\"\n",
    "\n",
    "\n",
    "def write_text(p, s):\n",
    "    p = Path(p)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(s, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def read_json(p):\n",
    "    p = Path(p)\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\")) if p.exists() else None\n",
    "\n",
    "\n",
    "def write_json(p, d):\n",
    "    p = Path(p)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(json.dumps(d, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def has_file(p):\n",
    "    return Path(p).is_file()\n",
    "\n",
    "\n",
    "def stamp(p):\n",
    "    write_text(p, \"checkpoint: \" + now_utc_iso())\n",
    "\n",
    "\n",
    "def sanitize_md(t: str) -> str:\n",
    "    \"\"\"Normalize line breaks and strip a single surrounding fenced block if present.\"\"\"\n",
    "    t = t or \"\"\n",
    "    t = t.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").strip()\n",
    "\n",
    "    # Remove exactly one opening fence if it's the very first line\n",
    "    t = re.sub(r\"^```(?:\\w+)?\\n\", \"\", t, flags=re.I)\n",
    "\n",
    "    # Remove exactly one trailing fence if it's the very last line\n",
    "    t = re.sub(r\"\\n```$\", \"\", t)\n",
    "\n",
    "    return t.strip()\n",
    "\n",
    "\n",
    "def count_words(t: str) -> int:\n",
    "    return len((t or \"\").split())\n",
    "\n",
    "\n",
    "def approx_tokens(t: str) -> int:\n",
    "    # ~4 chars/token heuristic; keeps a floor of 1\n",
    "    return max(1, int(len((t or \"\")) / 4))\n",
    "\n",
    "\n",
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1((s or \"\").encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "class CostCapExceededException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CostTracker:\n",
    "    PR = {\n",
    "        \"gpt-5\": {\"in\": 0.00125, \"out\": 0.010},\n",
    "        \"gpt-5-mini\": {\"in\": 0.00025, \"out\": 0.002},\n",
    "        \"gpt-5-nano\": {\"in\": 0.00005, \"out\": 0.0004},\n",
    "        \"gpt-3.5-turbo\": {\"in\": 0.0005, \"out\": 0.0015},\n",
    "        \"gpt-4-turbo\": {\"in\": 0.01, \"out\": 0.03},\n",
    "        \"gpt-4o-mini\": {\"in\": 0.00015, \"out\": 0.0006},\n",
    "        \"default\": {\"in\": 0.001, \"out\": 0.003},\n",
    "    }\n",
    "\n",
    "    for k in list(PR):\n",
    "        if k == \"default\":\n",
    "            continue\n",
    "        pin = os.getenv(f\"PRICE_{k.upper()}_IN\")\n",
    "        pout = os.getenv(f\"PRICE_{k.upper()}_OUT\")\n",
    "        if pin and pout:\n",
    "            PR[k] = {\"in\": float(pin), \"out\": float(pout)}\n",
    "\n",
    "    def __init__(self, cap):\n",
    "        self.cap = float(cap)\n",
    "        self.spent = 0.0\n",
    "        self.log = []\n",
    "\n",
    "    def price(self, model: str):\n",
    "        # exact match first\n",
    "        if model in self.PR:\n",
    "            return self.PR[model]\n",
    "        # then prefer longest prefix match\n",
    "        for k in sorted(self.PR, key=len, reverse=True):\n",
    "            if k != \"default\" and model.startswith(k):\n",
    "                return self.PR[k]\n",
    "        return self.PR[\"default\"]\n",
    "\n",
    "    def est(self, model: str, prompt_tokens: int, completion_tokens: int) -> float:\n",
    "        p = self.price(model)\n",
    "        return (prompt_tokens / 1000) * p[\"in\"] + (completion_tokens / 1000) * p[\"out\"]\n",
    "\n",
    "    def can(self, amount: float) -> bool:\n",
    "        return self.spent + float(amount) <= self.cap + 1e-9\n",
    "\n",
    "    def spend(self, label: str, amount: float):\n",
    "        amount = float(amount)\n",
    "        if not self.can(amount):\n",
    "            raise CostCapExceededException(\n",
    "                \"Cap hit before \"\n",
    "                + label\n",
    "                + f\" need {amount:.4f}, left {self.cap - self.spent:.4f}\"\n",
    "            )\n",
    "        self.spent += amount\n",
    "        self.log.append(\n",
    "            {\n",
    "                \"label\": label,\n",
    "                \"reserve\": round(amount, 6),\n",
    "                \"t\": now_utc_iso(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def recon(self, label: str, est_amount: float, actual_amount: float):\n",
    "        delta = float(actual_amount) - float(est_amount)\n",
    "        if delta > 0 and not self.can(delta):\n",
    "            raise CostCapExceededException(\n",
    "                \"Cap hit reconciling \" + label + f\" +{delta:.4f}\"\n",
    "            )\n",
    "        # Only add positive delta; no refund of reserve on underrun\n",
    "        self.spent += max(0, delta)\n",
    "        self.log.append(\n",
    "            {\n",
    "                \"label\": label,\n",
    "                \"est\": round(est_amount, 6),\n",
    "                \"act\": round(actual_amount, 6),\n",
    "                \"delta\": round(delta, 6),\n",
    "                \"t\": now_utc_iso(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def summary(self):\n",
    "        return {\n",
    "            \"total_spent_usd\": round(self.spent, 6),\n",
    "            \"run_cap_usd\": round(self.cap, 6),\n",
    "            \"remaining_usd\": round(max(0, self.cap - self.spent), 6),\n",
    "            \"log_items\": len(self.log),\n",
    "        }\n",
    "\n",
    "\n",
    "# Cell: Length helpers\n",
    "def compute_chapter_word_targets(book_spec, cfg, sections_count=None, ch_num: int | None = None):\n",
    "    \"\"\"\n",
    "    Compute per-chapter targets from book_spec by default.\n",
    "    - Uses total target length and chapter count from book_spec\n",
    "    - Supports optional chapter_weights and section_weights\n",
    "    - Allows overrides via cfg['CHAPTER_MIN_WORDS'] / cfg['CHAPTER_MAX_WORDS']\n",
    "    Returns: (per_chapter, min_words, max_words, per_section_target)\n",
    "    \"\"\"\n",
    "    # Total words and chapter count\n",
    "    try:\n",
    "        total = int(book_spec.get(\"target_length_words\", 95000) or 95000)\n",
    "    except Exception:\n",
    "        total = 95000\n",
    "    try:\n",
    "        n = max(1, int(book_spec.get(\"chapters\", 18) or 18))\n",
    "    except Exception:\n",
    "        n = 18\n",
    "\n",
    "    # Base per-chapter target\n",
    "    per = int(book_spec.get(\"chapter_target_words\", total // n))\n",
    "\n",
    "    # Apply chapter_weights if available\n",
    "    weights = book_spec.get(\"chapter_weights\") or []\n",
    "    if ch_num and 1 <= ch_num <= len(weights):\n",
    "        try:\n",
    "            w = float(weights[ch_num - 1] or 1.0)\n",
    "            per = max(800, int(per * w))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Tolerance band\n",
    "    tol = float(cfg.get(\"CHAPTER_TOLERANCE_PCT\", 0.18))\n",
    "    min_w = int(per * (1 - tol))\n",
    "    max_w = int(per * (1 + tol))\n",
    "\n",
    "    # Explicit overrides from cfg\n",
    "    if \"CHAPTER_MIN_WORDS\" in cfg:\n",
    "        try:\n",
    "            min_w = int(cfg[\"CHAPTER_MIN_WORDS\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    if \"CHAPTER_MAX_WORDS\" in cfg:\n",
    "        try:\n",
    "            max_w = int(cfg[\"CHAPTER_MAX_WORDS\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Section weighting\n",
    "    tmpl = (book_spec.get(\"chapter_template\") or {})\n",
    "    sec_ws = tmpl.get(\"section_weights\") or []\n",
    "    if not sections_count:\n",
    "        sections_count = len(sec_ws) if sec_ws else 5\n",
    "\n",
    "    if sec_ws and len(sec_ws) == sections_count:\n",
    "        ssum = sum(max(0.01, float(x)) for x in sec_ws)\n",
    "        per_section_nominal = int(per * (max(sec_ws) / ssum))\n",
    "    else:\n",
    "        per_section_nominal = max(600, per // max(1, sections_count))\n",
    "\n",
    "    # Final guardrail\n",
    "    per_section = int(cfg.get(\"AUTHOR_SECTION_TARGET_WORDS\", per_section_nominal))\n",
    "\n",
    "    return per, min_w, max_w, per_section\n",
    "\n",
    "\n",
    "print(\"Utils ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Agents Wiring (revised)\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json, os, re, time\n",
    "\n",
    "\n",
    "def _append_jsonl(path, obj):\n",
    "    p = Path(path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with p.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "PMROUTER_PROMPT = \"\"\"\n",
    "You are PMRouter, the project manager and workflow coordinator for this lean, low-cost, multi-agent book-writing system.\n",
    "\n",
    "Your Responsibilities:\n",
    "1. **Cost Control**\n",
    "   - Keep total token spend under strict per-run caps.\n",
    "   - Always select the cheapest capable model unless explicitly overridden.\n",
    "   - Abort gracefully if budget thresholds are exceeded.\n",
    "\n",
    "2. **Workflow Orchestration**\n",
    "   - Route tasks between AuthorAgent, EditorAgent, and ResearchAgent.\n",
    "   - Enforce checkpoints between stages: outline → draft → edit → research → assembly.\n",
    "   - Verify each step before passing outputs forward.\n",
    "\n",
    "3. **Logging & Transparency**\n",
    "   - Record: agent used, model, tokens, estimated cost, success/failure.\n",
    "   - Save logs into `logs/pmrouter.log` for traceability.\n",
    "\n",
    "Decision Rules:\n",
    "- Retry a failed step once, then flag for manual review.\n",
    "- Cache and reuse outputs wherever possible to reduce cost.\n",
    "- Always persist artifacts to disk before moving to the next stage.\n",
    "\"\"\"\n",
    "\n",
    "AUTHOR_AGENT_PROMPT = \"\"\"\n",
    "You are AuthorAgent, responsible for writing high-quality, structured book content based on the provided outline and style guide.\n",
    "\n",
    "Your Responsibilities:\n",
    "1. **Draft Creation**\n",
    "   - Follow the given outline exactly.\n",
    "   - Use the book_spec tone, audience, and style preferences.\n",
    "   - Structure output using Markdown: headings, subheadings, lists, and short paragraphs.\n",
    "\n",
    "2. **Writing Standards**\n",
    "   - Keep sentences concise and engaging.\n",
    "   - Avoid filler, tangents, and jargon.\n",
    "   - Use [n] placeholders wherever facts, data, or claims require later verification.\n",
    "   - Always conclude each chapter with a short summary or key takeaway.\n",
    "\n",
    "3. **Output Format**\n",
    "   - Provide clean Markdown ready for EditorAgent.\n",
    "   - Do not perform fact-checking or research yourself.\n",
    "\"\"\"\n",
    "\n",
    "EDITOR_AGENT_PROMPT = \"\"\"\n",
    "You are EditorAgent, responsible for polishing and validating the AuthorAgent's drafts.\n",
    "\n",
    "Your Responsibilities:\n",
    "1. **Editing & Cleanup**\n",
    "   - Improve clarity, readability, and flow.\n",
    "   - Enforce tone, style, and heading rules from the style guide.\n",
    "   - Remove redundancy, filler, and unnecessary complexity.\n",
    "\n",
    "2. **Fact-Check Preparation**\n",
    "   - Insert [n] markers where claims need supporting evidence.\n",
    "   - Generate a structured list of claims for ResearchAgent.\n",
    "\n",
    "3. **Output Requirements**\n",
    "   - Deliver a polished draft ready for assembly.\n",
    "   - Produce a parallel `claims.json` mapping [n] markers to unresolved facts.\n",
    "   - Add “editor_notes” summarizing areas that need further research.\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_AGENT_PROMPT = \"\"\"\n",
    "You are ResearchAgent, responsible for efficiently resolving [n] markers and gathering reliable information.\n",
    "\n",
    "Your Responsibilities:\n",
    "1. **Low-Cost Research**\n",
    "   - Prefer open-access, free, and local sources whenever possible.\n",
    "   - Summarize findings concisely — maximum 3 bullet points per claim.\n",
    "   - Provide citation-ready references without excessive verbosity.\n",
    "\n",
    "2. **Fallback Behavior**\n",
    "   - If no relevant information is found, do NOT hallucinate.\n",
    "   - Instead, output:\n",
    "       • A clear research question.\n",
    "       • Recommended source types or databases.\n",
    "\n",
    "3. **Output Format**\n",
    "   - Always return results as JSON:\n",
    "       { \"claim_id\": n, \"summary\": \"...\", \"source\": \"...\" }\n",
    "   - Do not insert findings directly into drafts — PMRouter merges results later.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _safe_label(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", s or \"step\")\n",
    "\n",
    "\n",
    "# --- PATCH: safer parts->text coercion (no repr() regex) ---\n",
    "def _parts_to_text(content):\n",
    "    \"\"\"\n",
    "    Normalize OpenAI SDK message/content into plain text.\n",
    "\n",
    "    Handles:\n",
    "      - str\n",
    "      - list/tuple of parts\n",
    "      - dicts with 'text'/'content'/'value' (including {'text': {'value': '...'}})\n",
    "      - SDK objects with .text/.content/.value or .text.value\n",
    "      - Chat message objects that carry structured parts (type='output_text')\n",
    "    \"\"\"\n",
    "    if content is None:\n",
    "        return \"\"\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "\n",
    "    def _coerce_text(obj):\n",
    "        # string\n",
    "        if isinstance(obj, str):\n",
    "            return obj\n",
    "\n",
    "        # dict-like\n",
    "        if isinstance(obj, dict):\n",
    "            # Preferred modern shapes\n",
    "            if \"text\" in obj:\n",
    "                t = obj[\"text\"]\n",
    "                if isinstance(t, str):\n",
    "                    return t\n",
    "                if isinstance(t, dict):\n",
    "                    v = t.get(\"value\")\n",
    "                    if isinstance(v, str):\n",
    "                        return v\n",
    "                    # descend further if needed\n",
    "                    return _coerce_text(t)\n",
    "\n",
    "            # Generic fallbacks\n",
    "            for k in (\"content\", \"value\"):\n",
    "                v = obj.get(k)\n",
    "                if isinstance(v, str):\n",
    "                    return v\n",
    "                if isinstance(v, (list, tuple)):\n",
    "                    return _join(v)\n",
    "                if isinstance(v, dict):\n",
    "                    return _coerce_text(v)\n",
    "\n",
    "            # Some SDKs use explicit content parts: [{'type': 'output_text', 'text': '...'}]\n",
    "            t = obj.get(\"type\")\n",
    "            if t and \"text\" in obj:\n",
    "                tv = obj.get(\"text\")\n",
    "                if isinstance(tv, str):\n",
    "                    return tv\n",
    "                if isinstance(tv, dict) and isinstance(tv.get(\"value\"), str):\n",
    "                    return tv[\"value\"]\n",
    "\n",
    "            # fall through\n",
    "            return \"\"\n",
    "\n",
    "        # list / tuple\n",
    "        if isinstance(obj, (list, tuple)):\n",
    "            return _join(obj)\n",
    "\n",
    "        # SDK objects: try attributes in priority order\n",
    "        for attr in (\"parsed\", \"text\", \"content\", \"value\"):\n",
    "            if hasattr(obj, attr):\n",
    "                v = getattr(obj, attr)\n",
    "                # If .parsed is JSON (dict/list), return minified JSON string\n",
    "                if attr == \"parsed\" and isinstance(v, (dict, list)):\n",
    "                    try:\n",
    "                        return json.dumps(v, ensure_ascii=False)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if hasattr(v, \"value\") and isinstance(getattr(v, \"value\"), str):\n",
    "                    return getattr(v, \"value\")\n",
    "                if isinstance(v, str):\n",
    "                    return v\n",
    "                if isinstance(v, (list, tuple)):\n",
    "                    return _join(v)\n",
    "                if isinstance(v, dict):\n",
    "                    return _coerce_text(v)\n",
    "\n",
    "        # No best-effort repr() regex here (too risky)\n",
    "        return \"\"\n",
    "\n",
    "    def _join(items):\n",
    "        out = []\n",
    "        for it in items:\n",
    "            t = _coerce_text(it)\n",
    "            if t:\n",
    "                out.append(t)\n",
    "        return \"\\n\".join(out)\n",
    "\n",
    "    return _coerce_text(content)\n",
    "\n",
    "\n",
    "def _extract_text_from_msg(msg):\n",
    "    # Prefer parsed JSON if present\n",
    "    parsed = getattr(msg, \"parsed\", None)\n",
    "    if parsed is not None:\n",
    "        if isinstance(parsed, (dict, list)):\n",
    "            try:\n",
    "                return json.dumps(parsed, ensure_ascii=False)\n",
    "            except Exception:\n",
    "                pass\n",
    "        s = _parts_to_text(parsed)\n",
    "        if s:\n",
    "            return s.strip()\n",
    "\n",
    "    # Normal .content\n",
    "    s = _parts_to_text(getattr(msg, \"content\", None))\n",
    "    if s:\n",
    "        return s.strip()\n",
    "\n",
    "    # Dict-like\n",
    "    try:\n",
    "        s = _parts_to_text(msg[\"content\"])  # type: ignore[index]\n",
    "        if s:\n",
    "            return s.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Common nested shape: content[0].text.value\n",
    "    try:\n",
    "        parts = getattr(msg, \"content\", None) or msg[\"content\"]  # type: ignore[index]\n",
    "        if isinstance(parts, (list, tuple)) and parts:\n",
    "            p0 = parts[0]\n",
    "            if (\n",
    "                hasattr(p0, \"text\")\n",
    "                and hasattr(p0.text, \"value\")\n",
    "                and isinstance(p0.text.value, str)\n",
    "            ):\n",
    "                return p0.text.value.strip()\n",
    "            if isinstance(p0, dict):\n",
    "                tv = p0.get(\"text\")\n",
    "                if isinstance(tv, dict) and isinstance(tv.get(\"value\"), str):\n",
    "                    return tv[\"value\"].strip()\n",
    "                if isinstance(tv, str):\n",
    "                    return tv.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _extract_text_from_any(r):\n",
    "    # 1) responses helper\n",
    "    t = getattr(r, \"output_text\", None)\n",
    "    if isinstance(t, str) and t.strip():\n",
    "        return t.strip()\n",
    "    # 2) chat choices\n",
    "    try:\n",
    "        choices = getattr(r, \"choices\", None)\n",
    "        if choices and len(choices) > 0:\n",
    "            msg = getattr(choices[0], \"message\", None)\n",
    "            if msg is not None:\n",
    "                s = _extract_text_from_msg(msg)\n",
    "                if s:\n",
    "                    return s\n",
    "            s = _parts_to_text(choices[0]).strip()\n",
    "            if s:\n",
    "                return s\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 3) responses structured fields\n",
    "    for attr in (\"output\", \"outputs\", \"content\"):\n",
    "        maybe = getattr(r, attr, None)\n",
    "        s = _parts_to_text(maybe).strip()\n",
    "        if s:\n",
    "            return s\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _looks_reasoning_only(r):\n",
    "    # usage-based signal\n",
    "    try:\n",
    "        u = getattr(r, \"usage\", None)\n",
    "        if not u:\n",
    "            return False\n",
    "        total = getattr(u, \"output_tokens\", 0) or 0\n",
    "        det = getattr(u, \"output_tokens_details\", None)\n",
    "        if total and det and getattr(det, \"reasoning_tokens\", 0) >= total:\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    # status-based signal\n",
    "    try:\n",
    "        inc = getattr(r, \"incomplete_details\", None)\n",
    "        if inc and getattr(inc, \"reason\", \"\") == \"max_output_tokens\":\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "\n",
    "def _extract_text_from_completion_or_response(r):\n",
    "    \"\"\"\n",
    "    Tries all common places text can live:\n",
    "    - r.output_text\n",
    "    - choices[0].message / choices[0]\n",
    "    - response.output / response.outputs / response.content\n",
    "    \"\"\"\n",
    "    t = getattr(r, \"output_text\", None)\n",
    "    if isinstance(t, str) and t.strip():\n",
    "        return t.strip()\n",
    "\n",
    "    try:\n",
    "        choices = getattr(r, \"choices\", None)\n",
    "        if choices and len(choices) > 0:\n",
    "            msg = getattr(choices[0], \"message\", None)\n",
    "            if msg is not None:\n",
    "                s = _extract_text_from_msg(msg)\n",
    "                if s:\n",
    "                    return s\n",
    "            s = _parts_to_text(choices[0]).strip()\n",
    "            if s:\n",
    "                return s\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for attr in (\"output\", \"outputs\", \"content\"):\n",
    "        maybe = getattr(r, attr, None)\n",
    "        s = _parts_to_text(maybe).strip()\n",
    "        if s:\n",
    "            return s\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def chat(model, sysm, userm, temp=0.2, max_t=800, *, force_json=False):\n",
    "    \"\"\"\n",
    "    OpenAI SDK >=1.x drop-in:\n",
    "      - Prefers chat.completions\n",
    "      - Uses max_completion_tokens (never max_tokens) when possible\n",
    "      - Disables heavy reasoning; text-only nudges\n",
    "      - Falls back to Responses API with reasoning disabled\n",
    "    Returns (text, usage_dict) or raises RuntimeError.\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    if not model:\n",
    "        raise ValueError(\"Model ID is required\")\n",
    "\n",
    "    def _usage(u, via=None):\n",
    "        if not u:\n",
    "            return {\"prompt_tokens\": None, \"completion_tokens\": None, \"via\": via}\n",
    "        pt = getattr(u, \"prompt_tokens\", None) or getattr(u, \"input_tokens\", None)\n",
    "        ct = getattr(u, \"completion_tokens\", None) or getattr(u, \"output_tokens\", None)\n",
    "        return {\"prompt_tokens\": pt, \"completion_tokens\": ct, \"via\": via}\n",
    "\n",
    "    # Heuristics for feature support by model id\n",
    "    m_lower = model.lower()\n",
    "    supports_temp = not any(x in m_lower for x in (\"gpt-5-mini\", \"gpt-5-nano\"))\n",
    "    supports_json_rf_chat = True\n",
    "    supports_reasoning_effort = True  # will turn off if API complains\n",
    "\n",
    "    # --- PATCH C: gpt-4o guardrails ---\n",
    "    MODEL_CAPS = {\n",
    "        \"gpt-4o\":      {\"ctx\": 128_000, \"out\": 16_384},\n",
    "        \"gpt-4o-mini\": {\"ctx\": 128_000, \"out\":  8_192},\n",
    "    }\n",
    "    caps = MODEL_CAPS.get(\n",
    "        \"gpt-4o-mini\" if \"gpt-4o-mini\" in m_lower else (\"gpt-4o\" if \"gpt-4o\" in m_lower else None),\n",
    "        {\"ctx\": 128_000, \"out\": 16_384},\n",
    "    )\n",
    "    out_cap = int(caps[\"out\"])\n",
    "    if \"gpt-4o\" in m_lower:\n",
    "        supports_reasoning_effort = False\n",
    "\n",
    "    base_messages = [\n",
    "        {\"role\": \"system\", \"content\": sysm},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": userm\n",
    "            if not force_json\n",
    "            else (\n",
    "                \"STRICT JSON OUTPUT REQUIRED. NO prose, NO markdown, just a single JSON object.\\n\\n\"\n",
    "                + userm\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # ---- 1) Chat Completions attempts\n",
    "    chat_variants = []\n",
    "\n",
    "    def add_variant(**kw):\n",
    "        # never pass legacy key\n",
    "        kw.pop(\"max_tokens\", None)\n",
    "\n",
    "        # clamp completion tokens to model output cap\n",
    "        if max_t:\n",
    "            kw[\"max_completion_tokens\"] = min(int(max_t), out_cap)\n",
    "\n",
    "        if not supports_temp:\n",
    "            kw.pop(\"temperature\", None)\n",
    "\n",
    "        # Prefer text-only; no reasoning keys on 4o\n",
    "        kw.setdefault(\"modalities\", [\"text\"])\n",
    "        if supports_reasoning_effort:\n",
    "            # only set if supported by the model\n",
    "            kw.setdefault(\"reasoning_effort\", \"none\")\n",
    "\n",
    "        chat_variants.append(kw)\n",
    "\n",
    "    if force_json:\n",
    "        add_variant(temperature=float(temp), response_format={\"type\": \"json_object\"})\n",
    "        add_variant(response_format={\"type\": \"json_object\"})\n",
    "    add_variant(temperature=float(temp))\n",
    "    add_variant()\n",
    "\n",
    "    chat_errors = []\n",
    "    for attempt in range(3):\n",
    "        for v in list(chat_variants):\n",
    "            try:\n",
    "                r = client.chat.completions.create(\n",
    "                    model=model, messages=base_messages, **v\n",
    "                )\n",
    "                text = _extract_text_from_completion_or_response(r)\n",
    "\n",
    "                if force_json and not text:\n",
    "                    try:\n",
    "                        write_text(\"logs/last_sdk_payload.txt\", str(r))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    raise ValueError(\"empty_completion\")\n",
    "\n",
    "                return text, _usage(getattr(r, \"usage\", None), via=\"chat.completions\")\n",
    "\n",
    "            except Exception as e:\n",
    "                em = str(e).lower()\n",
    "                chat_errors.append(e)\n",
    "\n",
    "                # adapt on-the-fly: remove offending keys and retry others\n",
    "                if \"unsupported parameter\" in em or \"invalid_request_error\" in em:\n",
    "                    if \"temperature\" in em:\n",
    "                        supports_temp = False\n",
    "                        for vv in chat_variants:\n",
    "                            vv.pop(\"temperature\", None)\n",
    "                        continue\n",
    "                    if \"response_format\" in em:\n",
    "                        supports_json_rf_chat = False\n",
    "                        for vv in chat_variants:\n",
    "                            vv.pop(\"response_format\", None)\n",
    "                        continue\n",
    "                    if \"modalities\" in em:\n",
    "                        for vv in chat_variants:\n",
    "                            vv.pop(\"modalities\", None)\n",
    "                        continue\n",
    "                    if \"reasoning_effort\" in em:\n",
    "                        supports_reasoning_effort = False\n",
    "                        for vv in chat_variants:\n",
    "                            vv.pop(\"reasoning_effort\", None)\n",
    "                        continue\n",
    "                if any(\n",
    "                    s in em for s in (\"rate limit\", \"overloaded\", \"timeout\", \"temporar\")\n",
    "                ):\n",
    "                    time.sleep(0.8 * (attempt + 1))\n",
    "                    continue\n",
    "                continue\n",
    "        # next attempt loop\n",
    "\n",
    "    # ---- 2) Responses API fallback — keep it text-only and obey caps\n",
    "    resp_errors = []\n",
    "    if supports_reasoning_effort:\n",
    "        resp_variants = [\n",
    "            {\"max_output_tokens\": min(int(max_t), out_cap)} if max_t else {},\n",
    "            {},  # second attempt with defaults\n",
    "        ]\n",
    "    else:\n",
    "        # For gpt-4o/mini: DO NOT send any 'reasoning' field\n",
    "        resp_variants = [\n",
    "            {\"max_output_tokens\": min(int(max_t), out_cap)} if max_t else {},\n",
    "            {},\n",
    "        ]\n",
    "    \n",
    "    for attempt in range(3):\n",
    "        for v in resp_variants:\n",
    "            try:\n",
    "                r = client.responses.create(model=model, input=base_messages, **v)\n",
    "                text = _extract_text_from_completion_or_response(r)\n",
    "\n",
    "                if force_json and not text:\n",
    "                    try:\n",
    "                        write_text(\"logs/last_sdk_payload.txt\", str(r))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    raise ValueError(\"empty_response\")\n",
    "\n",
    "                # Guard: some models may still report only reasoning tokens\n",
    "                try:\n",
    "                    out = getattr(r, \"usage\", None)\n",
    "                    out_total = getattr(out, \"output_tokens\", 0) or 0\n",
    "                    out_det = getattr(out, \"output_tokens_details\", None)\n",
    "                    reasoning_only = bool(\n",
    "                        out_det and getattr(out_det, \"reasoning_tokens\", 0) >= out_total\n",
    "                    )\n",
    "                except Exception:\n",
    "                    reasoning_only = False\n",
    "                if not text and reasoning_only:\n",
    "                    raise ValueError(\"response_reasoning_only_no_text\")\n",
    "\n",
    "                return text, _usage(getattr(r, \"usage\", None), via=\"responses\")\n",
    "\n",
    "            except Exception as e2:\n",
    "                resp_errors.append(e2)\n",
    "                em2 = str(e2).lower()\n",
    "                if any(\n",
    "                    s in em2\n",
    "                    for s in (\"rate limit\", \"overloaded\", \"timeout\", \"temporar\")\n",
    "                ):\n",
    "                    time.sleep(0.8 * (attempt + 1))\n",
    "                    continue\n",
    "                continue\n",
    "\n",
    "    errs = \" | \".join([str(e) for e in (chat_errors + resp_errors) if e])\n",
    "    raise RuntimeError(f\"chat() failed for model '{model}': {errs}\")\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self, name, sysm, model, temp=0.2, max_t=800, cache=\"cache\", tracker=None\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.sysm = sysm\n",
    "        self.model = model\n",
    "        self.temp = temp\n",
    "        self.max_t = max_t\n",
    "        self.cache = Path(cache)\n",
    "        self.cache.mkdir(exist_ok=True, parents=True)\n",
    "        self.tracker = tracker\n",
    "\n",
    "    def run(self, label, prompt, max_t=None, cache_key=None, *, force_json=False):\n",
    "        # Stable cache key across runs for identical inputs\n",
    "        key_input = \"\\n\".join(\n",
    "            [\n",
    "                str(self.model),\n",
    "                str(self.temp),\n",
    "                str(self.max_t if max_t is None else max_t),\n",
    "                self.sysm or \"\",\n",
    "                prompt or \"\",\n",
    "                str(cache_key or \"\"),\n",
    "            ]\n",
    "        )\n",
    "        key = sha1(key_input)\n",
    "        safe = _safe_label(label)\n",
    "        cpath = self.cache / f\"{safe}_{key}.json\"\n",
    "\n",
    "        # Emit a 'begin' event\n",
    "        _append_jsonl(\n",
    "            \"logs/agent_calls.jsonl\",\n",
    "            {\n",
    "                \"t\": now_utc_iso(),\n",
    "                \"agent\": self.name,\n",
    "                \"model\": self.model,\n",
    "                \"label\": label,\n",
    "                \"event\": \"begin\",\n",
    "                \"cache_key\": key,\n",
    "                \"max_t\": int(self.max_t if max_t is None else max_t),\n",
    "                \"force_json\": bool(force_json),\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Read cache if available (ignore if corrupt or EMPTY)\n",
    "        if cpath.exists():\n",
    "            try:\n",
    "                d = json.loads(cpath.read_text(encoding=\"utf-8\"))\n",
    "                cached_txt = (d.get(\"text\") or \"\").strip()\n",
    "                if cached_txt:\n",
    "                    _append_jsonl(\n",
    "                        \"logs/agent_calls.jsonl\",\n",
    "                        {\n",
    "                            \"t\": now_utc_iso(),\n",
    "                            \"agent\": self.name,\n",
    "                            \"model\": self.model,\n",
    "                            \"label\": label,\n",
    "                            \"event\": \"cache_hit\",\n",
    "                            \"cache_path\": cpath.as_posix(),\n",
    "                        },\n",
    "                    )\n",
    "                    return {\n",
    "                        \"text\": cached_txt,\n",
    "                        \"cached\": True,\n",
    "                        \"usage\": d.get(\"usage\"),\n",
    "                        \"est_cost\": 0.0,\n",
    "                    }\n",
    "                # empty cached output → delete and proceed\n",
    "                try:\n",
    "                    cpath.unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                _append_jsonl(\n",
    "                    \"logs/agent_calls.jsonl\",\n",
    "                    {\n",
    "                        \"t\": now_utc_iso(),\n",
    "                        \"agent\": self.name,\n",
    "                        \"model\": self.model,\n",
    "                        \"label\": label,\n",
    "                        \"event\": \"cache_purged_empty\",\n",
    "                        \"cache_path\": cpath.as_posix(),\n",
    "                    },\n",
    "                )\n",
    "            except Exception:\n",
    "                # Corrupt cache; fall through to re-run\n",
    "                _append_jsonl(\n",
    "                    \"logs/agent_calls.jsonl\",\n",
    "                    {\n",
    "                        \"t\": now_utc_iso(),\n",
    "                        \"agent\": self.name,\n",
    "                        \"model\": self.model,\n",
    "                        \"label\": label,\n",
    "                        \"event\": \"cache_corrupt\",\n",
    "                        \"cache_path\": cpath.as_posix(),\n",
    "                    },\n",
    "                )\n",
    "\n",
    "        # Cost estimate & reserve\n",
    "        pt = approx_tokens(self.sysm) + approx_tokens(prompt)\n",
    "        ct_budget = int((max_t or self.max_t) * 0.9)\n",
    "        est = self.tracker.est(self.model, pt, ct_budget) if self.tracker else 0.0\n",
    "        if self.tracker:\n",
    "            self.tracker.spend(f\"{label}[reserve]\", est)\n",
    "\n",
    "        # Inference (with timing)\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            txt, usage = chat(\n",
    "                self.model,\n",
    "                self.sysm,\n",
    "                prompt,\n",
    "                self.temp,\n",
    "                max_t or self.max_t,\n",
    "                force_json=force_json,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            _append_jsonl(\n",
    "                \"logs/agent_calls.jsonl\",\n",
    "                {\n",
    "                    \"t\": now_utc_iso(),\n",
    "                    \"agent\": self.name,\n",
    "                    \"model\": self.model,\n",
    "                    \"label\": label,\n",
    "                    \"event\": \"error\",\n",
    "                    \"error\": str(e),\n",
    "                },\n",
    "            )\n",
    "            raise\n",
    "\n",
    "        dur = round(time.time() - t0, 3)\n",
    "        _append_jsonl(\n",
    "            \"logs/agent_calls.jsonl\",\n",
    "            {\n",
    "                \"t\": now_utc_iso(),\n",
    "                \"agent\": self.name,\n",
    "                \"model\": self.model,\n",
    "                \"label\": label,\n",
    "                \"event\": \"llm_ok\",\n",
    "                \"duration_s\": dur,\n",
    "                \"usage\": usage,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Reconcile actual cost\n",
    "        if usage and self.tracker:\n",
    "            actual = self.tracker.est(\n",
    "                self.model,\n",
    "                usage.get(\"prompt_tokens\") or pt,\n",
    "                usage.get(\"completion_tokens\") or ct_budget,\n",
    "            )\n",
    "            self.tracker.recon(f\"{label}[actual]\", est, actual)\n",
    "\n",
    "        # Persist cache\n",
    "        payload = {\n",
    "            \"text\": txt or \"\",\n",
    "            \"usage\": usage,\n",
    "            \"ts\": now_utc_iso(),\n",
    "            \"model\": self.model,\n",
    "            \"temp\": self.temp,\n",
    "        }\n",
    "        cpath.write_text(json.dumps(payload, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "        return {\"text\": txt or \"\", \"cached\": False, \"usage\": usage, \"est_cost\": est}\n",
    "\n",
    "\n",
    "class PMRouter:\n",
    "    def __init__(self, spec, cfg, tracker):\n",
    "        self.spec = spec\n",
    "        self.cfg = cfg or {}\n",
    "        self.tr = tracker\n",
    "\n",
    "        model_fast = self.cfg.get(\"MODEL_ID_FAST\") or \"gpt-4o-mini\"\n",
    "\n",
    "        temp = float(self.cfg.get(\"TEMPERATURE\", 0.2))\n",
    "        temp_author = float(\n",
    "            os.getenv(\"AUTHOR_TEMP\", self.cfg.get(\"AUTHOR_TEMPERATURE\", temp))\n",
    "        )\n",
    "        temp_editor = float(\n",
    "            os.getenv(\"EDITOR_TEMP\", self.cfg.get(\"EDITOR_TEMPERATURE\", temp))\n",
    "        )\n",
    "        temp_research = float(\n",
    "            os.getenv(\"RESEARCH_TEMP\", self.cfg.get(\"RESEARCH_TEMPERATURE\", 0.2))\n",
    "        )\n",
    "\n",
    "        self.author = Agent(\n",
    "            \"Author\",\n",
    "            AUTHOR_AGENT_PROMPT,\n",
    "            model_fast,\n",
    "            temp_author,\n",
    "            1500,\n",
    "            \"cache\",\n",
    "            tracker,\n",
    "        )\n",
    "        self.editor = Agent(\n",
    "            \"Editor\",\n",
    "            EDITOR_AGENT_PROMPT,\n",
    "            model_fast,\n",
    "            temp_editor,\n",
    "            1500,\n",
    "            \"cache\",\n",
    "            tracker,\n",
    "        )\n",
    "        self.research = (\n",
    "            Agent(\n",
    "                \"Research\",\n",
    "                RESEARCH_AGENT_PROMPT,\n",
    "                model_fast,\n",
    "                temp_research,\n",
    "                1400,\n",
    "                \"cache\",\n",
    "                tracker,\n",
    "            )\n",
    "            if self.cfg.get(\"RESEARCH_ENABLED\")\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.router_prompt = PMROUTER_PROMPT\n",
    "        Path(\"logs\").mkdir(exist_ok=True)\n",
    "\n",
    "    def log(self, label, meta=None):\n",
    "        meta = dict(meta or {})\n",
    "        meta[\"label\"] = label\n",
    "        meta[\"time\"] = datetime.utcnow().isoformat() + \"Z\"\n",
    "        fname = (\n",
    "            f\"{datetime.utcnow().strftime('%Y%m%dT%H%M%S')}_{_safe_label(label)}.json\"\n",
    "        )\n",
    "        p = Path(\"logs\") / fname\n",
    "        p.parent.mkdir(exist_ok=True, parents=True)\n",
    "        p.write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "print(\"Agents ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style writers ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Style Guide and Glossary (revised)\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _parse_terminology(items):\n",
    "    \"\"\"Turn ['term: def', 'sensor: ...'] into [{'term': 'term','definition':'def'}, ...].\"\"\"\n",
    "    parsed = []\n",
    "    for raw in items or []:\n",
    "        if not isinstance(raw, str):\n",
    "            continue\n",
    "        if \":\" in raw:\n",
    "            term, definition = raw.split(\":\", 1)\n",
    "            parsed.append({\"term\": term.strip(), \"definition\": definition.strip()})\n",
    "        else:\n",
    "            parsed.append({\"term\": raw.strip(), \"definition\": \"\"})\n",
    "    return parsed\n",
    "\n",
    "\n",
    "def gen_style(spec):\n",
    "    sg = (spec or {}).get(\"style_guide\", {})\n",
    "    voice = sg.get(\"voice\", \"—\")\n",
    "    formatting = sg.get(\"formatting\", \"—\")\n",
    "    citations = sg.get(\"citations\", \"—\")\n",
    "    terminology_items = sg.get(\"terminology\", [])\n",
    "    tone = (spec or {}).get(\"tone\", \"—\")\n",
    "\n",
    "    glossary = _parse_terminology(terminology_items)\n",
    "\n",
    "    # Build Markdown\n",
    "    lines = [\n",
    "        \"# Style Guide\",\n",
    "        f\"- **Voice:** {voice}\",\n",
    "        f\"- **Formatting:** {formatting}\",\n",
    "        f\"- **Citations:** {citations}\",\n",
    "        \"\",\n",
    "        \"## Terminology\",\n",
    "    ]\n",
    "    if glossary:\n",
    "        lines += [f\"- **{t['term']}** — {t['definition']}\".rstrip() for t in glossary]\n",
    "    else:\n",
    "        lines.append(\"- _(none)_\")\n",
    "\n",
    "    lines += [\n",
    "        \"\",\n",
    "        \"## Rules\",\n",
    "        \"- Short, clear paragraphs.\",\n",
    "        \"- Use only H2/H3 headings.\",\n",
    "        \"- Add `[n]` where a claim needs a source or note.\",\n",
    "        f\"- **Tone:** {tone}\",\n",
    "        \"\",\n",
    "    ]\n",
    "\n",
    "    g = \"\\n\".join(lines)\n",
    "\n",
    "    # Write files\n",
    "    style_path = \"content/style/style_guide.md\"\n",
    "    glossary_path = \"content/style/glossary.json\"\n",
    "    write_text(style_path, g)\n",
    "    write_json(glossary_path, {\"terms\": glossary})\n",
    "\n",
    "    return style_path, glossary_path\n",
    "\n",
    "\n",
    "print(\"Style writers ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Outline Generation (fixed & hardened)\n",
    "from json import JSONDecodeError\n",
    "\n",
    "# --- Local config + safety helpers (Cell 6 scope) ---\n",
    "try:\n",
    "    _safe_int  # type: ignore\n",
    "except NameError:\n",
    "    def _safe_int(x, default):\n",
    "        try:\n",
    "            return int(x)\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "# Prefer a global pipeline_config if it exists; otherwise empty dict\n",
    "cfg = globals().get(\"pipeline_config\", {})\n",
    "\n",
    "def brief(spec):\n",
    "    sg = (spec or {}).get(\"style_guide\", {})\n",
    "    tmpl = (spec or {}).get(\"chapter_template\", {})\n",
    "    oc = (spec or {}).get(\"outline_constraints\", []) or []\n",
    "    seed = (spec or {}).get(\"outline_seed\", []) or []\n",
    "    lines = [\n",
    "        f\"Title: {spec.get('title', '')}\",\n",
    "        f\"Audience: {spec.get('audience', '')}\",\n",
    "        f\"Goal: {spec.get('goal', '')}\",\n",
    "        f\"Tone: {spec.get('tone', '')}\",\n",
    "        f\"Chapters: {spec.get('chapters', '')}\",\n",
    "        f\"TargetWords: {spec.get('target_length_words', '')}\",\n",
    "        f\"Style: {sg.get('formatting', '')}\",\n",
    "    ]\n",
    "    if oc:\n",
    "        lines.append(\"OutlineConstraints: \" + \"; \".join(oc))\n",
    "    if seed:\n",
    "        lines.append(\"SeedTitles: \" + \" | \".join(seed))\n",
    "    if tmpl:\n",
    "        sections = \", \".join(tmpl.get(\"sections\", []))\n",
    "        end_matter = \", \".join(tmpl.get(\"end_matter\", []))\n",
    "        lines.append(\n",
    "            f\"ChapterTemplate: sections=[{sections}] ; end_matter=[{end_matter}]\"\n",
    "        )\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def _json_repair(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Best-effort tiny repairs:\n",
    "      - de-fence, de-BOM, smart quotes -> straight quotes\n",
    "      - strip any junk before the first '{' or '['\n",
    "      - remove trailing commas before } or ]\n",
    "    \"\"\"\n",
    "    s = (s or \"\").replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").strip()\n",
    "\n",
    "    # Remove fenced code blocks if present\n",
    "    s = re.sub(r\"^```(?:json|markdown)?\\s*\", \"\", s, flags=re.I | re.M)\n",
    "    s = re.sub(r\"\\s*```$\", \"\", s, flags=re.M)\n",
    "\n",
    "    # Remove BOM\n",
    "    if s and s[0] == \"\\ufeff\":\n",
    "        s = s[1:]\n",
    "\n",
    "    # Convert smart quotes\n",
    "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
    "\n",
    "    # Drop any junk before the first structural char\n",
    "    first_brace = s.find(\"{\")\n",
    "    first_bracket = s.find(\"[\")\n",
    "    cut_points = [p for p in (first_brace, first_bracket) if p != -1]\n",
    "    if cut_points:\n",
    "        s = s[min(cut_points) :]\n",
    "\n",
    "    # Remove trailing commas\n",
    "    s = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", s)\n",
    "\n",
    "    # Keep only the outermost braces/brackets slice, if possible\n",
    "    # Prefer {...} but allow [...] if that's what we have.\n",
    "    def _slice_outer(text, open_ch, close_ch):\n",
    "        i, j = text.find(open_ch), text.rfind(close_ch)\n",
    "        return text[i : j + 1] if (i != -1 and j != -1 and j > i) else text\n",
    "\n",
    "    if s.startswith(\"{\"):\n",
    "        s = _slice_outer(s, \"{\", \"}\")\n",
    "    elif s.startswith(\"[\"):\n",
    "        s = _slice_outer(s, \"[\", \"]\")\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def _looks_like_ipynb(obj) -> bool:\n",
    "    return isinstance(obj, dict) and {\"cells\", \"metadata\", \"nbformat\"} <= set(\n",
    "        obj.keys()\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_json_loose(t):\n",
    "    \"\"\"\n",
    "    Try strict, then repaired, then loose slice → strict again.\n",
    "    Rejects obvious Jupyter notebooks (ipynb) to avoid accidental ingestion.\n",
    "    \"\"\"\n",
    "    raw = t or \"\"\n",
    "\n",
    "    # 1) strict\n",
    "    try:\n",
    "        d = json.loads(raw)\n",
    "        if _looks_like_ipynb(d):\n",
    "            return None\n",
    "        return d\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) repaired\n",
    "    repaired = _json_repair(raw)\n",
    "    try:\n",
    "        d = json.loads(repaired)\n",
    "        if _looks_like_ipynb(d):\n",
    "            return None\n",
    "        return d\n",
    "    except Exception as e:\n",
    "        log_exc(\"outline_loose_repair\", e)\n",
    "\n",
    "    # 3) loose slice of original (first {...} or [...])\n",
    "    i_obj, j_obj = raw.find(\"{\"), raw.rfind(\"}\")\n",
    "    i_arr, j_arr = raw.find(\"[\"), raw.rfind(\"]\")\n",
    "    candidates = []\n",
    "    if i_obj != -1 and j_obj != -1 and j_obj > i_obj:\n",
    "        candidates.append(raw[i_obj : j_obj + 1])\n",
    "    if i_arr != -1 and j_arr != -1 and j_arr > i_arr:\n",
    "        candidates.append(raw[i_arr : j_arr + 1])\n",
    "    for cand in candidates:\n",
    "        try:\n",
    "            d = json.loads(cand)\n",
    "            if _looks_like_ipynb(d):\n",
    "                continue\n",
    "            return d\n",
    "        except Exception as e:\n",
    "            log_exc(\"outline_loose_slice\", e)\n",
    "\n",
    "    # 4) strict again (last try)\n",
    "    try:\n",
    "        d = json.loads(repaired)\n",
    "        if _looks_like_ipynb(d):\n",
    "            return None\n",
    "        return d\n",
    "    except Exception as e:\n",
    "        log_exc(\"outline_loose_raw\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def _fallback_outline(spec):\n",
    "    \"\"\"Local minimal outline if the model fails to return valid JSON.\"\"\"\n",
    "    n = int((spec or {}).get(\"chapters\", 10) or 10)\n",
    "    seed = (spec or {}).get(\"outline_seed\", []) or []\n",
    "    tmpl = (spec or {}).get(\"chapter_template\", {}) or {}\n",
    "    sections = tmpl.get(\"sections\", []) or [\n",
    "        \"Opening scene\",\n",
    "        \"Problem\",\n",
    "        \"Small win\",\n",
    "        \"Cliffhanger or cozy close\",\n",
    "    ]\n",
    "    chapters = []\n",
    "    for i in range(n):\n",
    "        title = seed[i] if i < len(seed) else f\"Chapter {i + 1}\"\n",
    "        chapters.append(\n",
    "            {\n",
    "                \"number\": i + 1,\n",
    "                \"title\": str(title)[:80],\n",
    "                \"sections\": sections,\n",
    "                \"learning_objectives\": [\n",
    "                    \"Enjoy the story\",\n",
    "                    \"Notice cause and effect\",\n",
    "                    \"Practice empathy\",\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "    return {\"chapters\": chapters}\n",
    "\n",
    "\n",
    "def gen_outline(spec, tr, router):\n",
    "    pj = \"content/outline/outline.json\"\n",
    "    pm = \"content/outline/outline.md\"\n",
    "    if has_file(pj) and has_file(pm):\n",
    "        return pj, pm\n",
    "\n",
    "    n = int(spec.get(\"chapters\", 10) or 10)\n",
    "    seed = spec.get(\"outline_seed\", []) or []\n",
    "    tmpl = spec.get(\"chapter_template\", {}) or {}\n",
    "    tmpl_sections = tmpl.get(\"sections\", []) or [\n",
    "        \"Opening scene\",\n",
    "        \"Problem\",\n",
    "        \"Small win\",\n",
    "        \"Cliffhanger or cozy close\",\n",
    "    ]\n",
    "\n",
    "    instr = (\n",
    "        'Return JSON ONLY with key \"chapters\": an array of exactly {n} items.\\n'\n",
    "        \"Each chapter object MUST have:\\n\"\n",
    "        \"- number (1-based integer)\\n\"\n",
    "        \"- title (<= 7 words; if a matching SeedTitles entry exists, prefer it)\\n\"\n",
    "        \"- sections (array of short section titles; follow ChapterTemplate sections)\\n\"\n",
    "        \"- learning_objectives (array of 2-3 kid-friendly bullets)\\n\"\n",
    "        \"NO markdown fences, NO commentary, NO extra keys. JSON ONLY.\\n\"\n",
    "        \"Return MINIFIED JSON.\"\n",
    "    ).format(n=n)\n",
    "\n",
    "    max_t = _safe_int(\n",
    "        cfg.get(\"OUTLINE_MAX_TOKENS\", 900 if not cfg.get(\"ULTRA_BUDGET_MODE\") else 500),\n",
    "        900,\n",
    "    )\n",
    "    \n",
    "    cache_key = f\"outline:v3:{n}:{sha1('|'.join(seed))}\"\n",
    "    prompt = \"BRIEF\\n\" + brief(spec) + \"\\n\\nINSTRUCTIONS\\n\" + instr\n",
    "\n",
    "    # First attempt (JSON mode)\n",
    "    R = router.author.run(\n",
    "        \"outline\", prompt, max_t=max_t, cache_key=cache_key, force_json=True\n",
    "    )\n",
    "    model_text = R.get(\"text\", \"\") or \"\"\n",
    "    try:\n",
    "        _probe = json.loads(_json_repair(model_text))\n",
    "        if _looks_like_ipynb(_probe):\n",
    "            model_text = \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Save raw for debugging\n",
    "    try:\n",
    "        write_text(\"logs/outline_model_raw.txt\", model_text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    d = parse_json_loose(model_text)\n",
    "\n",
    "    # One strict retry with stronger “no prose” reminder if needed\n",
    "    if not (d and isinstance(d.get(\"chapters\"), list) and d[\"chapters\"]):\n",
    "        strict_prompt = prompt + \"\\n\\nReturn JSON ONLY. Do not include any prose.\"\n",
    "        R2 = router.author.run(\n",
    "            \"outline_retry\",\n",
    "            strict_prompt,\n",
    "            max_t=max_t,\n",
    "            cache_key=cache_key + \":retry1\",\n",
    "            force_json=True,\n",
    "        )\n",
    "        model_text2 = R2.get(\"text\", \"\") or \"\"\n",
    "        try:\n",
    "            write_text(\"logs/outline_model_raw_retry1.txt\", model_text2)\n",
    "        except Exception:\n",
    "            pass\n",
    "        d = parse_json_loose(model_text2)\n",
    "\n",
    "    # Fallback to deterministic outline\n",
    "    if not (d and isinstance(d.get(\"chapters\"), list) and d[\"chapters\"]):\n",
    "        d = _fallback_outline(spec)\n",
    "        try:\n",
    "            write_json(\n",
    "                \"logs/outline_parse_fallback.json\",\n",
    "                {\n",
    "                    \"reason\": \"Invalid or no JSON from model\",\n",
    "                    \"model_text_preview\": (model_text or \"\")[:2000],\n",
    "                    \"ts\": now_utc_iso(),\n",
    "                },\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # --- Normalize/repair: exactly n items, numbered 1..n, titles + sections + LOs\n",
    "    chapters = d.get(\"chapters\", [])\n",
    "    chapters = (chapters + [{}] * max(0, n - len(chapters)))[:n]\n",
    "\n",
    "    fixed = []\n",
    "    for i in range(n):\n",
    "        ch = chapters[i] if isinstance(chapters[i], dict) else {}\n",
    "        num = i + 1\n",
    "\n",
    "        title = (ch.get(\"title\") or \"\").strip()\n",
    "        if not title and i < len(seed) and seed[i]:\n",
    "            title = str(seed[i]).strip()\n",
    "        if not title:\n",
    "            title = f\"Chapter {num}\"\n",
    "        # keep titles reasonably short (<= 7 words)\n",
    "        if len(title.split()) > 7:\n",
    "            title = \" \".join(title.split()[:7])\n",
    "\n",
    "        sections = ch.get(\"sections\") if isinstance(ch.get(\"sections\"), list) else None\n",
    "        if not sections:\n",
    "            sections = list(tmpl_sections)\n",
    "\n",
    "        los = (\n",
    "            ch.get(\"learning_objectives\")\n",
    "            if isinstance(ch.get(\"learning_objectives\"), list)\n",
    "            else None\n",
    "        )\n",
    "        if not los:\n",
    "            los = [\"Enjoy the story\", \"Notice cause and effect\", \"Practice empathy\"]\n",
    "        los = [str(x).strip() for x in los][:3]\n",
    "\n",
    "        fixed.append(\n",
    "            {\n",
    "                \"number\": num,\n",
    "                \"title\": title,\n",
    "                \"sections\": sections,\n",
    "                \"learning_objectives\": los,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    final = {\"chapters\": fixed}\n",
    "\n",
    "    write_json(pj, final)\n",
    "\n",
    "    # Minimal readable Markdown index\n",
    "    lines = [\"# Outline: \" + str(spec.get(\"title\", \"\")), \"\"]\n",
    "    for ch in final[\"chapters\"]:\n",
    "        lines.append(f\"## Chapter {ch['number']}: {ch['title']}\")\n",
    "    write_text(pm, \"\\n\".join(lines))\n",
    "\n",
    "    return pj, pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Per-Chapter Loop (hardened: generic end-matter, robust editor + sanitizer, safer parsing/IO)\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "\n",
    "# ----------------------------- Small utilities -----------------------------\n",
    "\n",
    "def _ensure_dir(p: Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _safe_int(x, default: int) -> int:\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _safe_float(x, default: float) -> float:\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "# ----------------------------- Path helpers --------------------------------\n",
    "\n",
    "def ch_dir(ch, sub):\n",
    "    d = Path(f\"content/{sub}/{int(ch):02d}\")\n",
    "    _ensure_dir(d)\n",
    "    return d\n",
    "\n",
    "\n",
    "def outline_ch(pj, ch):\n",
    "    d = read_json(pj) or {}\n",
    "    for it in d.get(\"chapters\", []):\n",
    "        if int(it.get(\"number\", -1)) == int(ch):\n",
    "            return it\n",
    "    raise KeyError(f\"Chapter {ch} not in outline\")\n",
    "\n",
    "\n",
    "# ----------------------------- Parse editor blocks --------------------------\n",
    "\n",
    "def parse_editor_blocks(t: str) -> Tuple[str, Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Extract three tagged blocks and return (md, claims_dict, notes_md).\n",
    "    Hardened: tolerant to missing tags, malformed JSON, and extra wrapping.\n",
    "    \"\"\"\n",
    "\n",
    "    def ex(tag: str) -> str:\n",
    "        m = re.search(\n",
    "            r\"<\" + re.escape(tag) + r\">\\s*([\\s\\S]*?)\\s*</\" + re.escape(tag) + r\">\",\n",
    "            t,\n",
    "            flags=re.DOTALL | re.IGNORECASE,\n",
    "        )\n",
    "        return (m.group(1) if m else \"\").strip()\n",
    "\n",
    "    md_raw = ex(\"DRAFT_EDITED_MD\")\n",
    "    claims_raw = ex(\"CLAIMS_REPORT_JSON\")\n",
    "    notes_raw = ex(\"CONTINUITY_NOTES_MD\")\n",
    "\n",
    "    # Claims: try JSON parse with minimal schema; else store raw.\n",
    "    claims: Dict[str, Any]\n",
    "    try:\n",
    "        claims_parsed = json.loads(sanitize_md(claims_raw)) if claims_raw else {}\n",
    "        if not isinstance(claims_parsed, dict):\n",
    "            claims_parsed = {\"raw\": claims_parsed}\n",
    "        # Ensure expected keys exist\n",
    "        claims_parsed.setdefault(\"claims\", [])\n",
    "        claims_parsed.setdefault(\"issues\", [])\n",
    "        claims_parsed.setdefault(\"notes\", \"\")\n",
    "        claims = claims_parsed\n",
    "    except Exception:\n",
    "        claims = {\"raw\": sanitize_md(claims_raw or \"\")}\n",
    "\n",
    "    return sanitize_md(md_raw), claims, sanitize_md(notes_raw)\n",
    "\n",
    "\n",
    "# ---------- Generic helpers for end-matter & editor instruction ----------\n",
    "\n",
    "def end_matter_spec(book_spec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build a generic end-matter spec from book_spec.\n",
    "    Returns:\n",
    "      {\n",
    "        \"labels\": [{\"label\": \"Encrypted Epigraph\", \"heading\": \"### Encrypted Epigraph\"}, ...],\n",
    "        \"heading_fmt\": \"### {label}\"\n",
    "      }\n",
    "    \"\"\"\n",
    "    tmpl = book_spec.get(\"chapter_template\") or {}\n",
    "    items = tmpl.get(\"end_matter\") or []\n",
    "    heading_fmt = tmpl.get(\"end_matter_heading_format\", \"### {label}\")\n",
    "\n",
    "    labels = []\n",
    "    seen = set()\n",
    "    for x in items:\n",
    "        if isinstance(x, dict):\n",
    "            label = (x.get(\"label\") or x.get(\"title\") or str(x)).strip()\n",
    "            heading = x.get(\"heading\")  # optional explicit heading\n",
    "        else:\n",
    "            s = str(x)\n",
    "            label = s.split(\":\", 1)[0].strip() if \":\" in s else s.strip()\n",
    "            heading = None\n",
    "\n",
    "        if not label:\n",
    "            continue\n",
    "        # De-dup labels case-insensitively\n",
    "        key = label.lower()\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "\n",
    "        normalized_heading = heading or heading_fmt.format(label=label)\n",
    "        labels.append({\"label\": label, \"heading\": normalized_heading})\n",
    "\n",
    "    return {\"labels\": labels, \"heading_fmt\": heading_fmt}\n",
    "\n",
    "\n",
    "# A concise base policy; used as preamble in editor instructions\n",
    "EDITOR_AGENT_PROMPT = \"\"\"\n",
    "You are EditorAgent, responsible for polishing and validating the AuthorAgent's drafts.\n",
    "\n",
    "Responsibilities\n",
    "1) Editing & Cleanup\n",
    "   - Improve clarity, readability, and flow.\n",
    "   - Enforce tone, style, and heading rules from the style guide.\n",
    "   - Remove redundancy, filler, and unnecessary complexity.\n",
    "\n",
    "2) Fact-Check Preparation\n",
    "   - Insert [n] markers where claims need supporting evidence.\n",
    "   - Collect a structured list of those claims for ResearchAgent.\n",
    "\n",
    "3) Output Requirements\n",
    "   - Deliver a polished, unified chapter ready for assembly.\n",
    "   - Produce a parallel claims report mapping [n] markers to unresolved facts.\n",
    "   - Add brief editor notes summarizing issues for continuity.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_editor_instruction(\n",
    "    dtext: str,\n",
    "    book_spec: Dict[str, Any],\n",
    "    oc: Dict[str, Any],\n",
    "    ch: int,\n",
    "    cfg: Dict[str, Any],\n",
    "    tw: Optional[int] = None,\n",
    ") -> str:\n",
    "    \"\"\"Strict, generic editor instructions (single heading, dedupe, end-matter only at EOF).\"\"\"\n",
    "    tmpl = book_spec.get(\"chapter_template\", {}) or {}\n",
    "    ch_fmt = tmpl.get(\"chapter_heading_format\", \"## Chapter {number}: {title}\")\n",
    "    chapter_heading = ch_fmt.format(number=ch, title=oc.get(\"title\", \"\"))\n",
    "\n",
    "    render_heads = bool(tmpl.get(\"render_section_headings\", False))\n",
    "    sec_fmt = tmpl.get(\"section_heading_format\", \"### {title}\")\n",
    "    sec_fmt_preview = sec_fmt.replace(\"{index}\", \"N\").replace(\"{title}\", \"Title\")\n",
    "\n",
    "    em = end_matter_spec(book_spec)\n",
    "    em_labels = em[\"labels\"]\n",
    "    em_order_preview = (\n",
    "        \"\\n\".join([f\"  {i + 1}) {item['heading']}\" for i, item in enumerate(em_labels)])\n",
    "        if em_labels else \"  (none)\"\n",
    "    )\n",
    "\n",
    "    target_words = _safe_int(tw, 0) or None\n",
    "\n",
    "    sg = book_spec.get(\"style_guide\", {})\n",
    "    voice = sg.get(\"voice\", \"\")\n",
    "    formatting = sg.get(\"formatting\", \"\")\n",
    "    terms = \", \".join(sg.get(\"terminology\", []))\n",
    "\n",
    "    instr_parts: List[str] = [\n",
    "        EDITOR_AGENT_PROMPT,\n",
    "        \"\\n\\n\",\n",
    "        \"STYLE GUIDE SNAPSHOT\\n\",\n",
    "        f\"- Voice: {voice}\\n\" if voice else \"\",\n",
    "        f\"- Formatting: {formatting}\\n\" if formatting else \"\",\n",
    "        f\"- Terminology: {terms}\\n\" if terms else \"\",\n",
    "        \"\\n\",\n",
    "        \"INPUT_MD:\\n\",\n",
    "        dtext,\n",
    "        \"\\n\\n\",\n",
    "        \"Editing directives (follow strictly)\\n\",\n",
    "        f\"- Keep ONE chapter heading at the top, exactly:\\n  {chapter_heading}\\n\",\n",
    "        \"- Do NOT repeat the chapter title inside the body; remove duplicate '## Chapter …' headings.\\n\",\n",
    "        (\n",
    "            f\"- If section headings are enabled, preserve them and normalize to: {sec_fmt_preview}; renumber sequentially (1..N).\\n\"\n",
    "            if render_heads\n",
    "            else \"- Section headings are disabled: remove template/beat subheads like '### Opening hook scene'; keep only organic subheads that aid readability.\\n\"\n",
    "        ),\n",
    "        \"- End-matter policy (STRICT):\\n\",\n",
    "        \"  • Keep exactly ONE end-matter block at the very end of the chapter.\\n\",\n",
    "        \"  • Treat label/colon or bold/italic variants as end-matter too (e.g., _Encrypted Epigraph: …, **Tech Glossary Entry:** …).\\n\",\n",
    "        \"  • If any end-matter appears before the final block or inside code fences, REMOVE it from the body and MOVE it to the end, unwrapped (no fences).\\n\",\n",
    "        \"  • Merge duplicates of the same item; keep/merge the strongest version (usually the fuller one).\\n\",\n",
    "        \"  • Use the configured end-matter heading format and keep items in this exact order:\\n\",\n",
    "        em_order_preview,\n",
    "        \"\\n\",\n",
    "        \"  • Do NOT invent missing end-matter items if no content exists.\\n\",\n",
    "        \"- Do NOT invent new events, scenes, characters, or locations; preserve timeline and scene logic.\\n\",\n",
    "        \"- Do NOT change proper nouns, codenames, or technical terms.\\n\",\n",
    "        \"- Maintain past tense, third-person limited POV; short, cinematic paragraphs.\\n\",\n",
    "        \"- Reduce purple prose; prefer precise verbs and concrete detail.\\n\",\n",
    "        \"- Collapse excessive blank lines (max two). Strip trailing spaces. No URLs.\\n\",\n",
    "        (\n",
    "            f\"- Target ~{target_words} words (±15%). Trim repetition first; compress prose without removing essential beats.\\n\"\n",
    "            if target_words else \"\"\n",
    "        ),\n",
    "        \"- Add inline [n] markers sparingly for claims that merit verification (tech/ops/AI/geo/politics).\\n\",\n",
    "        \"\\nOUTPUT FORMAT (MANDATORY TAGS)\\n\",\n",
    "        \"- Return EXACTLY these blocks; no text outside tags.\\n\",\n",
    "        \"<DRAFT_EDITED_MD>\\n\",\n",
    "        \"# Full unified chapter in Markdown with exactly one top chapter heading.\\n\"\n",
    "        \"# Section headings handled per directives. One end-matter block at the very end (if defined in book_spec) in the configured order.\\n\"\n",
    "        \"# Include inline [n] markers for verifiable claims.\\n\",\n",
    "        \"</DRAFT_EDITED_MD>\\n\",\n",
    "        \"<CLAIMS_REPORT_JSON>\\n\",\n",
    "        \"{\\n\"\n",
    "        '  \"claims\": [\\n'\n",
    "        '    {\"id\": 1, \"marker\": \"[1]\", \"text\": \"…\", \"category\": \"tech|ops|ai\", \"confidence\": \"low|med|high\"}\\n'\n",
    "        \"  ],\\n\"\n",
    "        '  \"issues\": [\"example: Removed mid-chapter epigraph label\", \"example: Merged duplicate glossary entries\"],\\n'\n",
    "        '  \"notes\": \"1–3 sentences to the author about edits/risks.\"\\n'\n",
    "        \"}\\n\",\n",
    "        \"</CLAIMS_REPORT_JSON>\\n\",\n",
    "        \"<CONTINUITY_NOTES_MD>\\n\",\n",
    "        \"- Where/when this chapter ends.\\n\"\n",
    "        \"- Character states (injuries, gear, relationships, intent).\\n\"\n",
    "        \"- Items in play (neural key status, data recovered, enemies aware?).\\n\"\n",
    "        \"- Open threads / hooks for the next chapter.\\n\",\n",
    "        \"</CONTINUITY_NOTES_MD>\\n\",\n",
    "    ]\n",
    "    return \"\".join(instr_parts)\n",
    "\n",
    "\n",
    "# ---------- Post-edit sanitizer to enforce structure even if the model slips ----------\n",
    "\n",
    "def sanitize_chapter_markdown(md: str, book_spec: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Enforce:\n",
    "      - exactly one chapter heading (synthesize if missing)\n",
    "      - unwrap code fences\n",
    "      - collapse/collect end-matter (including label/colon/bold/italic variants) to EOF in defined order\n",
    "      - normalize/remove section heads per template; renumber sequentially if enabled\n",
    "      - trim trailing spaces & collapse extra blank lines\n",
    "    \"\"\"\n",
    "    tmpl = book_spec.get(\"chapter_template\") or {}\n",
    "\n",
    "    # 0) Normalize line endings early\n",
    "    md = (md or \"\").replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # 1) Ensure single top chapter heading like \"## Chapter 1: Title\" (tolerate – — - or : separators)\n",
    "    ch_heading_rx = re.compile(\n",
    "        r\"^##\\s*Chapter\\s+\\d+\\s*[:\\-–—]\\s*.+\\s*$\", re.MULTILINE | re.IGNORECASE\n",
    "    )\n",
    "    lines = md.strip().splitlines()\n",
    "    first_heading_idx = None\n",
    "    for i, ln in enumerate(lines):\n",
    "        if ch_heading_rx.match(ln):\n",
    "            if first_heading_idx is None:\n",
    "                first_heading_idx = i\n",
    "            else:\n",
    "                lines[i] = \"\"  # remove duplicates\n",
    "    md = \"\\n\".join(lines).strip()\n",
    "\n",
    "    # If missing, synthesize from spec\n",
    "    if not ch_heading_rx.search(md):\n",
    "        number = _safe_int(book_spec.get(\"current_chapter_number\", 0), 0)\n",
    "        # Try to infer from outline title if available (best-effort)\n",
    "        title = book_spec.get(\"current_chapter_title\", \"\") or book_spec.get(\"title\", \"\")\n",
    "        ch_fmt = tmpl.get(\"chapter_heading_format\", \"## Chapter {number}: {title}\")\n",
    "        synth = ch_fmt.format(number=number or 1, title=title)\n",
    "        md = synth + (\"\\n\\n\" + md if md else \"\")\n",
    "\n",
    "    # 2) Unwrap fenced code blocks (end-matter sometimes gets fenced)\n",
    "    md = re.sub(r\"```+(\\w+)?\\s*(.*?)```+\", r\"\\2\", md, flags=re.DOTALL)\n",
    "\n",
    "    # ----- 3) End-matter normalization -----\n",
    "    def _em_list(bs: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "        items = (bs.get(\"chapter_template\") or {}).get(\"end_matter\") or []\n",
    "        heading_fmt = (bs.get(\"chapter_template\") or {}).get(\"end_matter_heading_format\", \"### {label}\")\n",
    "        out = []\n",
    "        seen = set()\n",
    "        for x in items:\n",
    "            if isinstance(x, dict):\n",
    "                label = (x.get(\"label\") or x.get(\"title\") or str(x)).strip()\n",
    "                heading = x.get(\"heading\") or heading_fmt.format(label=label)\n",
    "            else:\n",
    "                s = str(x)\n",
    "                label = s.split(\":\", 1)[0].strip() if \":\" in s else s.strip()\n",
    "                heading = heading_fmt.format(label=label)\n",
    "            if not label:\n",
    "                continue\n",
    "            k = label.lower()\n",
    "            if k in seen:\n",
    "                continue\n",
    "            seen.add(k)\n",
    "            out.append({\"label\": label, \"heading\": heading})\n",
    "        return out\n",
    "\n",
    "    em_items = _em_list(book_spec)\n",
    "\n",
    "    def collect_heading_blocks(text: str, item: Dict[str, str]) -> List[str]:\n",
    "        \"\"\"Collect blocks that start with the canonical '### {label}' heading.\"\"\"\n",
    "        h = re.escape(item[\"heading\"])\n",
    "        rx = re.compile(rf\"(?ms)^\\s*{h}\\s*\\n(.*?)(?=^\\s*###\\s+|\\Z)\")\n",
    "        return rx.findall(text)\n",
    "\n",
    "    def collect_label_variants(text: str, items: List[Dict[str, str]]):\n",
    "        \"\"\"\n",
    "        Find label/colon/bold/italic variants:\n",
    "          Encrypted Epigraph:, _Encrypted Epigraph:_, **Tech Glossary Entry:**, etc.\n",
    "        Capture until next heading/label/EOF.\n",
    "        \"\"\"\n",
    "        labels = [it[\"label\"] for it in items]\n",
    "        label_alt = r\"|\".join([re.escape(l) for l in labels])\n",
    "        start_rx = re.compile(\n",
    "            rf\"(?im)^(?:[*_>\\s]*)(?:{label_alt})(?:\\s*[:\\-–—]\\s*)(.*)$\"\n",
    "        )\n",
    "\n",
    "        hits = []  # list of (start_idx, end_idx, label_text, body_str)\n",
    "        lines = text.splitlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            m = start_rx.match(lines[i])\n",
    "            if not m:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Determine exact label matched by scanning labels (prefer longest match)\n",
    "            line_clean = re.sub(r\"^[*_>\\s]+|[*_\\s:–—-]+$\", \"\", lines[i]).strip().lower()\n",
    "            matched_label = None\n",
    "            labels_sorted = sorted(labels, key=len, reverse=True)\n",
    "            for lab in labels_sorted:\n",
    "                if line_clean.startswith(lab.lower()):\n",
    "                    matched_label = lab\n",
    "                    break\n",
    "            if not matched_label:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            body_lines = []\n",
    "            inline_rest = m.group(1).rstrip()\n",
    "            if inline_rest:\n",
    "                body_lines.append(inline_rest)\n",
    "\n",
    "            j = i + 1\n",
    "            while j < len(lines):\n",
    "                ln = lines[j]\n",
    "                if ln.startswith(\"#\"):  # next heading\n",
    "                    break\n",
    "                if start_rx.match(ln):  # another label variant\n",
    "                    break\n",
    "                if ln.strip() == \"\" and j + 1 < len(lines) and lines[j + 1].startswith(\"#\"):\n",
    "                    break\n",
    "                body_lines.append(ln)\n",
    "                j += 1\n",
    "\n",
    "            body = \"\\n\".join(body_lines).strip()\n",
    "            hits.append((i, j, matched_label, body))\n",
    "            i = j\n",
    "        return hits\n",
    "\n",
    "    if em_items:\n",
    "        # 3a) collect canonical heading blocks\n",
    "        found_blocks: Dict[str, List[str]] = {item[\"heading\"]: [] for item in em_items}\n",
    "        for item in em_items:\n",
    "            for bl in collect_heading_blocks(md, item):\n",
    "                found_blocks[item[\"heading\"]].append(bl)\n",
    "\n",
    "        # 3b) collect label/colon/bold/italic variants and remove them from body\n",
    "        variant_hits = collect_label_variants(md, em_items)\n",
    "        if variant_hits:\n",
    "            # drop those ranges\n",
    "            drop = [(a, b) for (a, b, _, _) in variant_hits]\n",
    "            rebuilt = []\n",
    "            lines = md.splitlines()\n",
    "            for idx in range(len(lines)):\n",
    "                if any(a <= idx < b for (a, b) in drop):\n",
    "                    continue\n",
    "                rebuilt.append(lines[idx])\n",
    "            md = \"\\n\".join(rebuilt)\n",
    "\n",
    "            # group variants by label then map to canonical heading\n",
    "            by_label: Dict[str, List[str]] = {}\n",
    "            for _, _, label_text, body in variant_hits:\n",
    "                if body.strip():\n",
    "                    by_label.setdefault(label_text, []).append(body)\n",
    "            label_to_heading = {it[\"label\"]: it[\"heading\"] for it in em_items}\n",
    "            for lab, bodies in by_label.items():\n",
    "                heading = label_to_heading.get(lab)\n",
    "                if heading:\n",
    "                    found_blocks.setdefault(heading, []).extend(bodies)\n",
    "\n",
    "        # 3c) remove canonical headings still in body (we will rebuild a single block)\n",
    "        for item in em_items:\n",
    "            h = re.escape(item[\"heading\"])\n",
    "            md = re.sub(rf\"(?ms)^\\s*{h}\\s*\\n(.*?)(?=^\\s*###\\s+|\\Z)\", \"\", md)\n",
    "\n",
    "        # 3d) rebuild a single EOF end-matter block in defined order, using longest version of each\n",
    "        em_out = []\n",
    "        for item in em_items:\n",
    "            h = item[\"heading\"]\n",
    "            found = found_blocks.get(h, [])\n",
    "            if found:\n",
    "                body = max(found, key=lambda x: len(x.strip())).strip()\n",
    "                if body:\n",
    "                    em_out.append(f\"{h}\\n{body}\\n\")\n",
    "        if em_out:\n",
    "            md = md.rstrip() + \"\\n\\n\" + \"\\n\\n\".join(em_out).rstrip() + \"\\n\"\n",
    "\n",
    "    # ----- 4) Section heading normalization/removal per template flags -----\n",
    "    render_heads = bool(tmpl.get(\"render_section_headings\", False))\n",
    "    number_sections = bool(tmpl.get(\"number_sections\", False))\n",
    "    sec_fmt = tmpl.get(\"section_heading_format\", \"### {title}\")\n",
    "    known_secs = tmpl.get(\"sections\") or [\n",
    "        \"Opening hook scene\",\n",
    "        \"Problem escalation\",\n",
    "        \"Uncovering a twist or revelation\",\n",
    "        \"Action sequence or moral dilemma\",\n",
    "        \"Cliffhanger or resolution\",\n",
    "    ]\n",
    "    names_rx = r\"|\".join([re.escape(n) for n in known_secs])\n",
    "    sec_rx = re.compile(\n",
    "        rf\"^(#+)\\s+(?:\\d+\\.\\s+)?({names_rx})\\s*$\",\n",
    "        re.IGNORECASE | re.MULTILINE,\n",
    "    )\n",
    "\n",
    "    # We’ll renumber in a second pass to handle multiple matches safely.\n",
    "    matches = list(sec_rx.finditer(md))\n",
    "    if matches:\n",
    "        # Build new text with replacements\n",
    "        out_lines = md.splitlines()\n",
    "        offset = 0\n",
    "        count = 0\n",
    "        for m in matches:\n",
    "            start, end = m.span()\n",
    "            # adjust for prior replacements\n",
    "            start += offset\n",
    "            end += offset\n",
    "\n",
    "            title = m.group(2).strip()\n",
    "            if not render_heads:\n",
    "                repl = \"\"  # drop template beat headings entirely\n",
    "            else:\n",
    "                count += 1\n",
    "                display_title = f\"{count}. {title}\" if number_sections else title\n",
    "                repl = sec_fmt.format(index=count, title=display_title)\n",
    "\n",
    "            # splice\n",
    "            before = md[:start]\n",
    "            after = md[end:]\n",
    "            md = before + repl + after\n",
    "            # recompute offset relative to original span\n",
    "            new_len = len(repl)\n",
    "            old_len = end - start\n",
    "            offset += (new_len - old_len)\n",
    "\n",
    "    # ----- 5) Trim trailing spaces & collapse excessive blank lines -----\n",
    "    md = \"\\n\".join([ln.rstrip() for ln in md.splitlines()])\n",
    "    md = re.sub(r\"\\n{3,}\", r\"\\n\\n\", md).strip() + \"\\n\"\n",
    "    return md\n",
    "\n",
    "\n",
    "# ------------------------ Main per-chapter processor ------------------------\n",
    "\n",
    "def process_chapter(ch, cfg, tracker, router, pj):\n",
    "    \"\"\"\n",
    "    Orchestrates research (optional), author drafting, and editor pass for a chapter.\n",
    "\n",
    "    NOTE:\n",
    "    - Uses global `book_spec` defined elsewhere in the notebook (matching your original design).\n",
    "    - Writes draft and edited artifacts into content/{sub}/{ch:02d}.\n",
    "    - Respects budget caps and ultra-budget mode.\n",
    "    \"\"\"\n",
    "    ch = int(ch)\n",
    "    cap = _safe_float(cfg.get(\"CHAPTER_COST_CAP_USD\", 0.25), 0.25)\n",
    "    start_spent = getattr(tracker, \"spent\", 0.0) if tracker else 0.0\n",
    "\n",
    "    # Chapter directories\n",
    "    dr = ch_dir(ch, \"research\")\n",
    "    dd = ch_dir(ch, \"drafts\")\n",
    "    de = ch_dir(ch, \"edits\")\n",
    "    _ensure_dir(Path(\"logs\"))\n",
    "\n",
    "    # Paths used later (define unconditionally)\n",
    "    b = dr / \"brief.md\"\n",
    "    s = dr / \"sources.json\"\n",
    "    dp = dd / \"draft.md\"\n",
    "    meta_path = dd / \".author_call.json\"\n",
    "\n",
    "    # Outline object (used across phases)\n",
    "    oc = outline_ch(pj, ch)\n",
    "\n",
    "    # --- Research (optional) ---\n",
    "    if cfg.get(\"RESEARCH_ENABLED\"):\n",
    "        if (not b.exists()) or (not s.exists()):\n",
    "            locals_dir = Path(\"content/research_inputs\") / f\"{ch:02d}\"\n",
    "            note = \"locals present\" if locals_dir.exists() else \"no locals\"\n",
    "            if getattr(router, \"research\", None):\n",
    "                prompt = (\n",
    "                    \"OUTLINE_JSON:\\n\" + json.dumps(oc, ensure_ascii=False) + \"\\n\"\n",
    "                    \"NOTE: \" + note + \"\\n\"\n",
    "                    \"Return blocks:\\n\"\n",
    "                    \"<BRIEF_MD>...</BRIEF_MD>\\n\"\n",
    "                    \"<SOURCES_JSON>{...}</SOURCES_JSON>\\n\"\n",
    "                    \"No web.\"\n",
    "                )\n",
    "                try:\n",
    "                    Rr = router.research.run(\n",
    "                        f\"ch{ch}_research\",\n",
    "                        prompt,\n",
    "                        max_t=_safe_int(cfg.get(\"RESEARCH_MAX_TOKENS\", 500), 500),\n",
    "                        cache_key=sha1(json.dumps(oc, sort_keys=True) + \"|\" + note),\n",
    "                    )\n",
    "                    t = (Rr or {}).get(\"text\", \"\") or \"\"\n",
    "                except Exception as e:\n",
    "                    log_exc(f\"ch{ch}_research_call\", e)\n",
    "                    t = \"\"\n",
    "            else:\n",
    "                t = '<BRIEF_MD>Research off</BRIEF_MD><SOURCES_JSON>{\"enabled\":false}</SOURCES_JSON>'\n",
    "\n",
    "            bm = re.search(\n",
    "                r\"<BRIEF_MD>\\s*([\\s\\S]*?)\\s*</BRIEF_MD>\",\n",
    "                t,\n",
    "                flags=re.DOTALL | re.IGNORECASE,\n",
    "            )\n",
    "            sm = re.search(\n",
    "                r\"<SOURCES_JSON>\\s*([\\s\\S]*?)\\s*</SOURCES_JSON>\",\n",
    "                t,\n",
    "                flags=re.DOTALL | re.IGNORECASE,\n",
    "            )\n",
    "            write_text(b, sanitize_md(bm.group(1)) if bm else \"\")\n",
    "            try:\n",
    "                sj = json.loads(sanitize_md(sm.group(1))) if sm else {\"enabled\": False}\n",
    "                if not isinstance(sj, dict):\n",
    "                    sj = {\"raw\": sj}\n",
    "                write_json(s, sj)\n",
    "            except Exception as e:\n",
    "                log_exc(f\"ch{ch}_research_sources_parse\", e)\n",
    "                write_json(s, {\"raw\": sanitize_md(sm.group(1)) if sm else \"\"})\n",
    "\n",
    "    # --- Author draft (ALWAYS runs; independent of research) ---\n",
    "    need_author = (\n",
    "        (not dp.exists())\n",
    "        or (dp.stat().st_size < 8)\n",
    "        or (sanitize_md(read_text(dp)).strip() == \"\")\n",
    "    )\n",
    "    if need_author:\n",
    "        # Brief (if any)\n",
    "        br = read_text(b) if b.exists() else \"\"\n",
    "\n",
    "        # Chapter template controls (headings)\n",
    "        tmpl = book_spec.get(\"chapter_template\", {}) or {}\n",
    "        ch_fmt = tmpl.get(\"chapter_heading_format\", \"## Chapter {number}: {title}\")\n",
    "        render_heads = bool(tmpl.get(\"render_section_headings\", False))\n",
    "        number_sections = bool(tmpl.get(\"number_sections\", False))\n",
    "        sec_fmt = tmpl.get(\"section_heading_format\", \"### {title}\")\n",
    "\n",
    "        # Chapter sections (from template or default)\n",
    "        sections = tmpl.get(\"sections\") or [\n",
    "            \"Opening hook scene\",\n",
    "            \"Problem escalation\",\n",
    "            \"Uncovering a twist or revelation\",\n",
    "            \"Action sequence or moral dilemma\",\n",
    "            \"Cliffhanger or resolution\",\n",
    "        ]\n",
    "\n",
    "        # Compute per-chapter + per-section targets\n",
    "        try:\n",
    "            per, min_w, max_w, per_section = compute_chapter_word_targets(\n",
    "                book_spec, cfg, len(sections), ch_num=ch\n",
    "            )\n",
    "            tw = max(min_w, min(max_w, per))\n",
    "        except NameError:\n",
    "            try:\n",
    "                total = int(book_spec.get(\"target_length_words\", 95000))\n",
    "            except Exception:\n",
    "                total = 95000\n",
    "            try:\n",
    "                n = max(1, int(book_spec.get(\"chapters\", 18)))\n",
    "            except Exception:\n",
    "                n = 18\n",
    "            per = int(book_spec.get(\"chapter_target_words\", total // n))\n",
    "            tol = _safe_float(cfg.get(\"CHAPTER_TOLERANCE_PCT\", 0.18), 0.18)\n",
    "            min_w = int(cfg.get(\"CHAPTER_MIN_WORDS\", per * (1 - tol)))\n",
    "            max_w = int(cfg.get(\"CHAPTER_MAX_WORDS\", per * (1 + tol)))\n",
    "            tw = max(min_w, min(max_w, per))\n",
    "            per_section = max(600, tw // max(1, len(sections)))\n",
    "\n",
    "        # Optional override\n",
    "        per_section = _safe_int(cfg.get(\"AUTHOR_SECTION_TARGET_WORDS\", per_section), per_section)\n",
    "\n",
    "        # Author continuity clip window\n",
    "        ctx_chars = _safe_int(cfg.get(\"AUTHOR_CONTEXT_CHARS\", 6000), 6000)\n",
    "\n",
    "        # End-matter (for the author, only append on final section; generic)\n",
    "        em = end_matter_spec(book_spec)\n",
    "        em_labels = em[\"labels\"]\n",
    "        em_preview = \"\\n\".join([f\"- {item['heading']}\" for item in em_labels]) if em_labels else \"\"\n",
    "        em_label_names = \", \".join([x[\"label\"] for x in em_labels]) if em_labels else \"end-matter items\"\n",
    "\n",
    "        # Accumulate section outputs\n",
    "        accum: List[str] = []\n",
    "        last_R = None\n",
    "        for idx_sec, sec in enumerate(sections, 1):\n",
    "            # Compute section heading text (if enabled)\n",
    "            sec_title = f\"{idx_sec}. {sec}\" if number_sections else sec\n",
    "            sec_heading = sec_fmt.format(index=idx_sec, title=sec_title)\n",
    "\n",
    "            so_far = \"\\n\".join(accum)[-ctx_chars:] if accum else \"\"\n",
    "            chapter_title = ch_fmt.format(number=ch, title=oc.get(\"title\", \"\"))\n",
    "\n",
    "            heading_chunk = (\n",
    "                f\"SECTION_HEADING_MD:\\n{sec_heading}\\n\\n\"\n",
    "                \"Begin the section with SECTION_HEADING_MD verbatim on its own line, then continue the prose.\\n\"\n",
    "                if render_heads else \"\"\n",
    "            )\n",
    "\n",
    "            end_matter_chunk = \"\"\n",
    "            nonfinal_guard = \"\"\n",
    "            if idx_sec == len(sections) and em_labels:\n",
    "                end_matter_chunk = (\n",
    "                    \"\\nAt the very end of THIS FINAL SECTION, append a single end-matter block with these items \"\n",
    "                    \"in the SAME ORDER. Use the configured heading format:\\n\"\n",
    "                    + (em_preview + \"\\n\")\n",
    "                    + \"Do not add end-matter in earlier sections. Do not wrap end-matter in code fences.\\n\"\n",
    "                )\n",
    "            else:\n",
    "                nonfinal_guard = (\n",
    "                    \"- Do NOT include any end-matter in this section. Avoid label/colon or bold/italic lines that look like end-matter, \"\n",
    "                    f\"such as '{em_label_names}:' or italic/bold variants. Keep only story prose here.\\n\"\n",
    "                )\n",
    "\n",
    "            prompt = \"\".join([\n",
    "                \"You are an expert author.\\n\\n\",\n",
    "                \"BOOK SPEC:\\n\",\n",
    "                json.dumps(book_spec, ensure_ascii=False, indent=2),\n",
    "                \"\\n\\n\",\n",
    "                \"OUTLINE_JSON:\\n\",\n",
    "                json.dumps(oc, ensure_ascii=False, indent=2),\n",
    "                \"\\n\\n\",\n",
    "                \"BRIEF_MD:\\n\",\n",
    "                br[:1200],\n",
    "                \"\\n\\n\",\n",
    "                \"CHAPTER_SO_FAR_MD:\\n\",\n",
    "                so_far,\n",
    "                \"\\n\\n\",\n",
    "                f\"Write ONLY Markdown for this section of '{chapter_title}'.\\n\",\n",
    "                f\"Section {idx_sec}/{len(sections)}: {sec}\\n\",\n",
    "                heading_chunk,\n",
    "                \"- Continue seamlessly from CHAPTER_SO_FAR_MD (no recap).\\n\",\n",
    "                \"- Maintain voice, POV, style; short, cinematic paragraphs.\\n\",\n",
    "                f\"- Target ≈ {per_section} words for THIS section.\\n\",\n",
    "                \"- Do not repeat the chapter title. Only include the section text (plus the provided section heading if any).\\n\",\n",
    "                \"- If you reach the target but the beat isn’t complete, finish the beat.\\n\",\n",
    "                nonfinal_guard,\n",
    "                end_matter_chunk,\n",
    "            ])\n",
    "\n",
    "            last_R = router.author.run(\n",
    "                f\"ch{ch}_draft_s{idx_sec}\",\n",
    "                prompt,\n",
    "                max_t=_safe_int(cfg.get(\"AUTHOR_MAX_TOKENS\", 3200), 3200),\n",
    "                cache_key=sha1(\n",
    "                    json.dumps(\n",
    "                        {\"ch\": ch, \"sec\": idx_sec, \"oc\": oc, \"tw\": tw, \"sections\": sections},\n",
    "                        sort_keys=True,\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            raw_model = (last_R or {}).get(\"text\", \"\") or \"\"\n",
    "            write_text(f\"logs/ch{ch:02d}_author_raw_s{idx_sec}_MODEL.txt\", raw_model)\n",
    "            part = sanitize_md(raw_model)\n",
    "            write_text(f\"logs/ch{ch:02d}_author_raw_s{idx_sec}.txt\", part)\n",
    "            accum.append(part.strip())\n",
    "\n",
    "        # Render chapter heading via format (for consistency with book_spec)\n",
    "        chapter_heading = ch_fmt.format(number=ch, title=oc.get(\"title\", \"\"))\n",
    "        full = chapter_heading + \"\\n\\n\" + \"\\n\\n\".join(accum)\n",
    "        write_text(dp, full)\n",
    "\n",
    "        # Log meta from the last section call (if any)\n",
    "        meta = {\n",
    "            \"t\": now_utc_iso(),\n",
    "            \"chapter\": ch,\n",
    "            \"outline_title\": oc.get(\"title\", \"\"),\n",
    "            \"attempt\": \"sectioned\",\n",
    "            \"target_words\": tw,\n",
    "            \"sections\": len(sections),\n",
    "            \"draft_chars\": len(full),\n",
    "            \"draft_words\": len(full.split()),\n",
    "        }\n",
    "        if last_R:\n",
    "            meta.update(\n",
    "                {\n",
    "                    \"cached\": bool(last_R.get(\"cached\")),\n",
    "                    \"usage\": last_R.get(\"usage\"),\n",
    "                    \"est_cost\": last_R.get(\"est_cost\"),\n",
    "                }\n",
    "            )\n",
    "        write_json(meta_path, meta)\n",
    "\n",
    "    else:\n",
    "        if not meta_path.exists():\n",
    "            write_json(\n",
    "                meta_path,\n",
    "                {\n",
    "                    \"t\": now_utc_iso(),\n",
    "                    \"chapter\": ch,\n",
    "                    \"note\": \"Draft existed; AuthorAgent not called this run.\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "    # --- Ultra-budget: cheap “edit” path ---\n",
    "    if cfg.get(\"ULTRA_BUDGET_MODE\"):\n",
    "        ep = de / \"draft_edited.md\"\n",
    "        decp = de / \"claims_report.json\"\n",
    "        nop = de / \"continuity_notes.md\"\n",
    "        draft = read_text(dp)\n",
    "        cleaned = \"\\n\".join([ln.rstrip() for ln in draft.splitlines()])\n",
    "        write_text(ep, cleaned)\n",
    "        write_json(decp, {\"info\": \"Editor skipped (ULTRA_BUDGET_MODE)\", \"chapter\": ch})\n",
    "        write_text(nop, \"Local cleanup applied; no LLM edit due to ultra budget mode.\")\n",
    "        print(\"Chapter processor ready (ultra-budget)\")\n",
    "        return\n",
    "\n",
    "    # --- Budget gate for editor ---\n",
    "    if tracker:\n",
    "        try:\n",
    "            spent_now = getattr(tracker, \"spent\", 0.0) - start_spent\n",
    "            remaining = (tracker.summary() or {}).get(\"remaining_usd\", 0.0)\n",
    "        except Exception:\n",
    "            spent_now = 0.0\n",
    "            remaining = 0.0\n",
    "        if (spent_now >= cap) or (remaining < 0.05):\n",
    "            ep = de / \"draft_edited.md\"\n",
    "            decp = de / \"claims_report.json\"\n",
    "            nop = de / \"continuity_notes.md\"\n",
    "            write_text(ep, read_text(dp))\n",
    "            write_json(decp, {\"warning\": \"editor skipped budget\", \"chapter\": ch})\n",
    "            write_text(nop, \"Skipped due to budget limits.\")\n",
    "            print(\"Chapter processor ready (editor skipped: budget gate)\")\n",
    "            return\n",
    "\n",
    "    # --- Editor pass ---\n",
    "    ep = de / \"draft_edited.md\"\n",
    "    decp = de / \"claims_report.json\"\n",
    "    nop = de / \"continuity_notes.md\"\n",
    "    if not (ep.exists() and decp.exists() and nop.exists()):\n",
    "        dtext = read_text(dp)\n",
    "        MAX_CHARS = _safe_int(cfg.get(\"EDITOR_MAX_INPUT_CHARS\", 900_000), 900_000)\n",
    "        dtext = dtext[:MAX_CHARS]\n",
    "\n",
    "        try:\n",
    "            tw_for_editor = tw  # if available from earlier block\n",
    "        except NameError:\n",
    "            tw_for_editor = None\n",
    "\n",
    "        instr = build_editor_instruction(dtext, book_spec, oc, ch, cfg, tw=tw_for_editor)\n",
    "\n",
    "        # ---- Maximize editor tokens but obey model caps (GPT-4o) ----\n",
    "        def _tok_est(chars: int) -> int: return max(1, chars // 4)\n",
    "\n",
    "        # Model caps (per-model guardrails)\n",
    "        EDITOR_MODEL = (cfg.get(\"EDITOR_MODEL\") or \"gpt-4o\").lower()\n",
    "        MODEL_CAPS = {\n",
    "            \"gpt-4o\": {\"ctx\": 128_000, \"out\": 16_384},\n",
    "            \"gpt-4o-mini\": {\"ctx\": 128_000, \"out\": 8_192},\n",
    "        }\n",
    "        caps = MODEL_CAPS.get(EDITOR_MODEL, {\"ctx\": _safe_int(cfg.get(\"EDITOR_CONTEXT_TOKENS\", 128_000), 128_000),\n",
    "                                            \"out\": _safe_int(cfg.get(\"EDITOR_MAX_TOKENS\", 16_384), 16_384)})\n",
    "\n",
    "        # Requested (from cfg), then clamp to model caps\n",
    "        ctx_limit   = min(_safe_int(cfg.get(\"EDITOR_CONTEXT_TOKENS\", caps[\"ctx\"]), caps[\"ctx\"]), caps[\"ctx\"])\n",
    "        safe_margin = _safe_int(cfg.get(\"EDITOR_SAFE_MARGIN_TOKENS\", 1_000), 1_000)\n",
    "        out_cap     = min(_safe_int(cfg.get(\"EDITOR_MAX_TOKENS\", caps[\"out\"]), caps[\"out\"]), caps[\"out\"])\n",
    "        min_out     = min(_safe_int(cfg.get(\"EDITOR_MIN_TOKENS\", 6_000), 6_000), out_cap)\n",
    "\n",
    "        # Estimate input tokens\n",
    "        est_in_tokens = _tok_est(len(dtext))\n",
    "\n",
    "        # Initial desired output: everything left after input + margin\n",
    "        desired_out = max(0, ctx_limit - est_in_tokens - safe_margin)\n",
    "        desired_out = min(desired_out, out_cap)\n",
    "\n",
    "        # If output < minimum, trim input tail to free space\n",
    "        if desired_out < min_out:\n",
    "            max_in_tokens = max(1, ctx_limit - min_out - safe_margin)\n",
    "            max_in_chars = max_in_tokens * 4\n",
    "            if len(dtext) > max_in_chars:\n",
    "                dtext = dtext[-max_in_chars:]\n",
    "                est_in_tokens = _tok_est(len(dtext))\n",
    "                desired_out = max(0, ctx_limit - est_in_tokens - safe_margin)\n",
    "                desired_out = min(desired_out, out_cap)\n",
    "\n",
    "        editor_max_t = max(min_out, min(desired_out, out_cap))\n",
    "\n",
    "        try:\n",
    "            Re = router.editor.run(\n",
    "                f\"ch{ch}_edit\", instr, max_t=editor_max_t, cache_key=sha1(dtext)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log_exc(f\"ch{ch}_editor_call\", e)\n",
    "            Re = {}\n",
    "\n",
    "        md, cl, nt = parse_editor_blocks((Re or {}).get(\"text\", \"\") or \"\")\n",
    "        # Final structural sanitize before writing\n",
    "        md_final = sanitize_chapter_markdown(md or read_text(dp), book_spec)\n",
    "\n",
    "        write_text(ep, md_final)\n",
    "        write_json(decp, cl)\n",
    "        write_text(nop, nt)\n",
    "\n",
    "    print(\"Chapter processor ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembly ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Assembly\n",
    "from pathlib import Path  # safe even if already imported elsewhere\n",
    "\n",
    "\n",
    "def assemble_book(spec):\n",
    "    # Ensure output directory exists\n",
    "    build_dir = Path(\"build\")\n",
    "    build_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Front matter (defensive: handle missing fields gracefully)\n",
    "    title = str(spec.get(\"title\", \"Untitled\")).strip()\n",
    "    subtitle = str(spec.get(\"subtitle\", \"\") or \"\").strip()\n",
    "    author = str(spec.get(\"author\", \"Unknown Author\")).strip()\n",
    "\n",
    "    fm = [\n",
    "        f\"# {title}\",\n",
    "        f\"## {subtitle}\" if subtitle else \"\",\n",
    "        f\"**Author:** {author}\",\n",
    "        \"\",\n",
    "        \"---\",\n",
    "        \"\",\n",
    "    ]\n",
    "\n",
    "    chs, toc = [], []\n",
    "    n = int(spec.get(\"chapters\", 0))\n",
    "\n",
    "    for ch in range(1, n + 1):\n",
    "        e = Path(f\"content/edits/{ch:02d}/draft_edited.md\")\n",
    "        d = Path(f\"content/drafts/{ch:02d}/draft.md\")\n",
    "\n",
    "        if e.exists():\n",
    "            t = read_text(e)\n",
    "            src = \"edit\"\n",
    "            p = e\n",
    "        elif d.exists():\n",
    "            t = read_text(d)\n",
    "            src = \"draft\"\n",
    "            p = d\n",
    "        else:\n",
    "            # FIX: proper newline in placeholder\n",
    "            t = f\"## Chapter {ch}: (missing)\\nTODO\"\n",
    "            src = \"missing\"\n",
    "            p = None\n",
    "\n",
    "        chs.append(t.strip())\n",
    "\n",
    "        # First H2 line as chapter title (fallback to generic)\n",
    "        tl = next(\n",
    "            (ln.strip() for ln in t.splitlines() if ln.strip().startswith(\"## \")),\n",
    "            f\"Chapter {ch}\",\n",
    "        )\n",
    "\n",
    "        toc.append(\n",
    "            {\n",
    "                \"chapter\": ch,\n",
    "                \"title_line\": tl,\n",
    "                \"source\": src,\n",
    "                \"path\": str(p) if p else None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # FIX: correct join with blank line between sections; add trailing newline\n",
    "    md = \"\\n\\n\".join([x for x in fm if x] + chs) + \"\\n\"\n",
    "\n",
    "    write_text(str(build_dir / \"book.md\"), md)\n",
    "    write_json(str(build_dir / \"toc.json\"), toc)\n",
    "\n",
    "    return str(build_dir / \"book.md\"), str(build_dir / \"toc.json\")\n",
    "\n",
    "\n",
    "print(\"Assembly ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: QA\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def run_qa(spec):\n",
    "    dist_dir = Path(\"dist\")\n",
    "    dist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rpt = {\"checks\": {}, \"warnings\": [], \"metrics\": {}}\n",
    "\n",
    "    book_path = Path(\"build/book.md\")\n",
    "    ok = book_path.exists() and book_path.stat().st_size > 0\n",
    "    rpt[\"checks\"][\"book_exists\"] = bool(ok)\n",
    "    if not ok:\n",
    "        write_json(str(dist_dir / \"qa_report.json\"), rpt)\n",
    "        return rpt\n",
    "\n",
    "    t = read_text(book_path)\n",
    "\n",
    "    # Metrics\n",
    "    wc = count_words(t)\n",
    "    rpt[\"metrics\"][\"word_count\"] = wc\n",
    "\n",
    "    tgt = int(spec.get(\"target_length_words\", 20000))\n",
    "    low, hi = int(tgt * 0.9), int(tgt * 1.1)\n",
    "    within = low <= wc <= hi\n",
    "    rpt[\"checks\"][\"word_count_ok\"] = bool(within)\n",
    "    if not within:\n",
    "        rpt[\"warnings\"].append(\n",
    "            f\"Word count {wc} vs target {tgt} (acceptable range {low}-{hi})\"\n",
    "        )\n",
    "\n",
    "    # Chapter presence: prefer TOC if present; otherwise heuristic via H2 count\n",
    "    n = int(spec.get(\"chapters\", 0))\n",
    "    toc_path = Path(\"build/toc.json\")\n",
    "    toc_present = False\n",
    "    missing_list = []\n",
    "\n",
    "    if toc_path.exists():\n",
    "        try:\n",
    "            toc = read_json(toc_path) or []\n",
    "            toc_present = (len(toc) == n) and all(\n",
    "                item.get(\"source\", \"unknown\") != \"missing\" for item in toc\n",
    "            )\n",
    "            if not toc_present and toc:\n",
    "                # note which chapters the TOC marked missing (if our Cell 8 added 'source')\n",
    "                for idx, item in enumerate(toc, start=1):\n",
    "                    if item.get(\"source\", \"unknown\") == \"missing\":\n",
    "                        missing_list.append(idx)\n",
    "        except Exception:\n",
    "            toc_present = False\n",
    "\n",
    "    if not toc_path.exists() or not toc_present:\n",
    "        # Fallback: require at least n H2 headings (## ...)\n",
    "        h2_count = sum(1 for ln in t.splitlines() if ln.strip().startswith(\"## \"))\n",
    "        toc_present = h2_count >= n\n",
    "        if not toc_present:\n",
    "            rpt[\"warnings\"].append(\n",
    "                f\"Expected {n} chapters but found {h2_count} H2 headings.\"\n",
    "            )\n",
    "\n",
    "    rpt[\"checks\"][\"all_chapters_present\"] = bool(toc_present)\n",
    "    if missing_list:\n",
    "        rpt[\"warnings\"].append(f\"Chapters marked missing in TOC: {missing_list}\")\n",
    "\n",
    "    # TODO/FIXME markers\n",
    "    todo_hits = re.findall(r\"\\b(?:TODO|FIXME)\\b\", t)\n",
    "    rpt[\"checks\"][\"no_todo_fixme\"] = len(todo_hits) == 0\n",
    "    if todo_hits:\n",
    "        rpt[\"warnings\"].append(f\"TODO/FIXME remain: {len(todo_hits)} occurrences\")\n",
    "\n",
    "    # Heading level sanity: flag H4+ outside code fences (skip ``` blocks)\n",
    "    def strip_codeblocks(s):\n",
    "        out, in_code = [], False\n",
    "        for line in s.splitlines():\n",
    "            if line.strip().startswith(\"```\"):\n",
    "                in_code = not in_code\n",
    "                continue\n",
    "            if not in_code:\n",
    "                out.append(line)\n",
    "        return \"\\n\".join(out)\n",
    "\n",
    "    text_no_code = strip_codeblocks(t)\n",
    "    head_ok = \"####\" not in text_no_code\n",
    "    rpt[\"checks\"][\"heading_levels_ok\"] = bool(head_ok)\n",
    "    if not head_ok:\n",
    "        rpt[\"warnings\"].append(\"Headings exceed H3 (#### found)\")\n",
    "\n",
    "    # Citations check when research is enabled for nonfiction\n",
    "    genre = str(spec.get(\"genre\", \"\")).lower()\n",
    "    research_enabled = bool(\n",
    "        spec.get(\"research_enabled\")\n",
    "        or (\n",
    "            isinstance(spec.get(\"research_policy\"), dict)\n",
    "            and spec[\"research_policy\"].get(\"enabled\")\n",
    "        )\n",
    "    )\n",
    "    cites_ok = True\n",
    "    if genre == \"nonfiction\" and research_enabled:\n",
    "        cites_ok = \"[n]\" in t\n",
    "        if not cites_ok:\n",
    "            rpt[\"warnings\"].append(\n",
    "                \"No [n] markers while research is enabled for nonfiction.\"\n",
    "            )\n",
    "    rpt[\"checks\"][\"citations_present_if_research\"] = bool(cites_ok)\n",
    "\n",
    "    # Check end-matter sections per chapter (if template defines them)\n",
    "    end_matter = (spec.get(\"chapter_template\", {}) or {}).get(\"end_matter\", []) or []\n",
    "    if end_matter:\n",
    "        for ch in range(1, int(spec.get(\"chapters\", 10) or 10) + 1):\n",
    "            block = re.findall(\n",
    "                rf\"^##\\s+Chapter\\s+{ch}:[\\s\\S]*?(?=^##\\s+Chapter\\s+{ch + 1}:|\\Z)\",\n",
    "                t,\n",
    "                flags=re.M,\n",
    "            )\n",
    "            if block:\n",
    "                missing = [s for s in end_matter if s not in block[0]]\n",
    "                if missing:\n",
    "                    rpt[\"warnings\"].append(f\"Ch {ch} missing end-matter: {missing}\")\n",
    "\n",
    "    # Flag very long paragraphs (~>220 words)\n",
    "    paras = re.split(r\"\\n\\s*\\n\", t)\n",
    "    if any(len(p.split()) > 220 for p in paras):\n",
    "        rpt[\"warnings\"].append(\"Some paragraphs exceed ~220 words.\")\n",
    "\n",
    "    # Double-space / trailing-space hygiene\n",
    "    if re.search(r\"[^\\S\\r\\n]{2,}\", t):\n",
    "        rpt[\"warnings\"].append(\"Double spaces detected.\")\n",
    "    if re.search(r\"[ \\t]+$\", t, flags=re.M):\n",
    "        rpt[\"warnings\"].append(\"Trailing spaces at line ends.\")\n",
    "\n",
    "    # Claims mismatch: [n] markers vs. claims_report.json presence\n",
    "    claims_files = list(Path(\"content/edits\").rglob(\"*/claims_report.json\"))\n",
    "    has_claims_files = any(\n",
    "        Path(cf).exists() and Path(cf).stat().st_size > 2 for cf in claims_files\n",
    "    )\n",
    "    has_markers = \"[n]\" in t\n",
    "    if has_markers and not has_claims_files:\n",
    "        rpt[\"warnings\"].append(\"Found [n] markers but no claims_report.json files.\")\n",
    "    if has_claims_files and not has_markers:\n",
    "        rpt[\"warnings\"].append(\n",
    "            \"claims_report.json exists but no [n] markers found in book.\"\n",
    "        )\n",
    "\n",
    "    # Outcome\n",
    "    rpt[\"outcome\"] = \"PASS\" if not rpt[\"warnings\"] else \"PASS_WITH_WARNINGS\"\n",
    "\n",
    "    write_json(str(dist_dir / \"qa_report.json\"), rpt)\n",
    "    return rpt\n",
    "\n",
    "\n",
    "print(\"QA ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Export\n",
    "from pathlib import Path\n",
    "import shutil, zipfile, re\n",
    "\n",
    "\n",
    "def minimal_docx(\n",
    "    p, note=\"DOCX export unavailable; install python-docx. See dist/book.md\"\n",
    "):\n",
    "    p = Path(p)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # Basic, valid DOCX container with a single paragraph note\n",
    "    safe = note.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
    "    with zipfile.ZipFile(p, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "        z.writestr(\n",
    "            \"[Content_Types].xml\",\n",
    "            '<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>'\n",
    "            '<Types xmlns=\"http://schemas.openxmlformats.org/package/2006/content-types\">'\n",
    "            '<Default Extension=\"rels\" ContentType=\"application/vnd.openxmlformats-package.relationships+xml\"/>'\n",
    "            '<Default Extension=\"xml\" ContentType=\"application/xml\"/>'\n",
    "            '<Override PartName=\"/word/document.xml\" ContentType=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document.main+xml\"/>'\n",
    "            \"</Types>\",\n",
    "        )\n",
    "        z.writestr(\n",
    "            \"_rels/.rels\",\n",
    "            '<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>'\n",
    "            '<Relationships xmlns=\"http://schemas.openxmlformats.org/package/2006/relationships\">'\n",
    "            '<Relationship Id=\"rId1\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/officeDocument\" Target=\"word/document.xml\"/>'\n",
    "            \"</Relationships>\",\n",
    "        )\n",
    "        z.writestr(\n",
    "            \"word/document.xml\",\n",
    "            '<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>'\n",
    "            '<w:document xmlns:w=\"http://schemas.openxmlformats.org/wordprocessingml/2006/main\">'\n",
    "            \"<w:body><w:p><w:r><w:t>\"\n",
    "            + safe\n",
    "            + \"</w:t></w:r></w:p></w:body></w:document>\",\n",
    "        )\n",
    "\n",
    "\n",
    "def export_deliverables():\n",
    "    src = Path(\"build/book.md\")\n",
    "    dst = Path(\"dist/book.md\")\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if src.exists():\n",
    "        shutil.copyfile(src, dst)\n",
    "        md = read_text(src)\n",
    "    else:\n",
    "        md = \"# Book (missing)\\n\"\n",
    "        write_text(dst, md)\n",
    "\n",
    "    # Try rich DOCX via python-docx; otherwise create a minimal DOCX with a note\n",
    "    try:\n",
    "        import docx\n",
    "        from docx.enum.text import WD_BREAK  # page breaks\n",
    "\n",
    "        d = docx.Document()\n",
    "\n",
    "        def _sanitize_docx_text(s: str) -> str:\n",
    "            # Replace tabs with spaces and strip control chars\n",
    "            s = s.replace(\"\\t\", \"    \")\n",
    "            return \"\".join(ch if ord(ch) >= 0x20 else \" \" for ch in s)\n",
    "\n",
    "        in_code = False\n",
    "        for ln in md.splitlines():\n",
    "            s = _sanitize_docx_text(ln.rstrip(\"\\n\"))\n",
    "\n",
    "            # Toggle code blocks on fenced markers\n",
    "            if s.strip().startswith(\"```\"):\n",
    "                in_code = not in_code\n",
    "                continue\n",
    "\n",
    "            if in_code:\n",
    "                p = d.add_paragraph(s)\n",
    "                # 'Code' style may not exist; fall back silently\n",
    "                try:\n",
    "                    p.style = \"Code\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "                continue\n",
    "\n",
    "            if not s.strip():\n",
    "                d.add_paragraph(\"\")\n",
    "                continue\n",
    "\n",
    "            if s.startswith(\"### \"):\n",
    "                d.add_heading(s[4:], level=3)\n",
    "            elif s.startswith(\"## \"):\n",
    "                d.add_heading(s[3:], level=2)\n",
    "            elif s.startswith(\"# \"):\n",
    "                d.add_heading(s[2:], level=1)\n",
    "            elif s.strip() == \"---\":\n",
    "                d.add_page_break()\n",
    "            elif s.lstrip().startswith((\"- \", \"* \")):\n",
    "                d.add_paragraph(s.lstrip()[2:], style=\"List Bullet\")\n",
    "            elif re.match(r\"^\\s*\\d+\\.\\s+\", s):\n",
    "                d.add_paragraph(re.sub(r\"^\\s*\\d+\\.\\s+\", \"\", s), style=\"List Number\")\n",
    "            else:\n",
    "                d.add_paragraph(s)\n",
    "\n",
    "        d.save(\"dist/book.docx\")\n",
    "    except Exception as e:\n",
    "        minimal_docx(\n",
    "            \"dist/book.docx\",\n",
    "            note=f\"DOCX export unavailable ({e.__class__.__name__}). See dist/book.md\",\n",
    "        )\n",
    "\n",
    "    return str(dst), \"dist/book.docx\"\n",
    "\n",
    "\n",
    "print(\"Export ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Manifest\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "def manifest(tr, outcome):\n",
    "    def scan(d):\n",
    "        root = Path(d)\n",
    "        items = []\n",
    "        if not root.exists():\n",
    "            return items\n",
    "        for p in sorted(root.rglob(\"*\")):\n",
    "            if p.is_file():\n",
    "                st = p.stat()\n",
    "                items.append(\n",
    "                    {\n",
    "                        \"path\": p.as_posix(),\n",
    "                        \"size_bytes\": st.st_size,\n",
    "                        \"mtime\": datetime.fromtimestamp(st.st_mtime, timezone.utc)\n",
    "                        .isoformat()\n",
    "                        .replace(\"+00:00\", \"Z\"),\n",
    "                    }\n",
    "                )\n",
    "        return items\n",
    "\n",
    "    Path(\"dist\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    m = {\n",
    "        \"outcome\": outcome,\n",
    "        \"generated_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
    "        \"files\": {\n",
    "            \"build\": scan(\"build\"),\n",
    "            \"dist\": scan(\"dist\"),\n",
    "        },\n",
    "        \"cost_summary\": tr.summary() if hasattr(tr, \"summary\") else None,\n",
    "    }\n",
    "\n",
    "    write_json(\"dist/manifest.json\", m)\n",
    "    return m\n",
    "\n",
    "\n",
    "print(\"Manifest ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controls ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Run-All / Rerun\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def make_router():\n",
    "    tr = CostTracker(pipeline_config[\"RUN_COST_CAP_USD\"])\n",
    "    rt = PMRouter(book_spec, pipeline_config, tr)\n",
    "    return rt, tr\n",
    "\n",
    "\n",
    "def run_all():\n",
    "    Path(\"logs\").mkdir(parents=True, exist_ok=True)\n",
    "    if os.getenv(\"DRY_RUN\") == \"1\":\n",
    "        print(\"[run_all] DRY_RUN=1 → skipping API calls; using synthetic outputs.\")\n",
    "    out = \"PASS\"\n",
    "    tr = None  # ensure defined for finally\n",
    "    try:\n",
    "        rt, tr = make_router()\n",
    "\n",
    "        # Style & outline\n",
    "        gen_style(book_spec)\n",
    "        pj, pm = gen_outline(book_spec, tr, rt)\n",
    "\n",
    "        # Chapter selection based on config\n",
    "        total = int(book_spec.get(\"chapters\", 0))\n",
    "        if pipeline_config.get(\"FULL_RUN\", True):\n",
    "            chs = list(range(1, total + 1))\n",
    "        else:\n",
    "            sample_n = int(pipeline_config.get(\"SAMPLE_RUN_CHAPTERS\", 2))\n",
    "            chs = list(range(1, min(total, sample_n) + 1))\n",
    "\n",
    "        # Per-chapter processing with cost-cap handling\n",
    "        for ch in chs:\n",
    "            try:\n",
    "                process_chapter(ch, pipeline_config, tr, rt, pj)\n",
    "            except CostCapExceededException as e:\n",
    "                write_text(\"logs/last_error.txt\", str(e))\n",
    "                out = \"ABORTED_COST_CAP\"\n",
    "                break\n",
    "\n",
    "        # Assembly, QA, Export\n",
    "        out = \"PASS\"\n",
    "        try:\n",
    "            assemble_book(book_spec)\n",
    "        except Exception as e:\n",
    "            log_exc(\"assemble_book\", e)\n",
    "            return \"FAILED_STEP\"\n",
    "\n",
    "        try:\n",
    "            qa = run_qa(book_spec)\n",
    "            if qa.get(\"warnings\"):\n",
    "                out = \"PASS_WITH_WARNINGS\" if out == \"PASS\" else out\n",
    "        except Exception as e:\n",
    "            log_exc(\"run_qa\", e)\n",
    "            return \"FAILED_STEP\"\n",
    "\n",
    "        try:\n",
    "            export_deliverables()\n",
    "        except Exception as e:\n",
    "            log_exc(\"export_deliverables\", e)\n",
    "            return \"FAILED_STEP\"\n",
    "\n",
    "        return out\n",
    "\n",
    "    except CostCapExceededException as e:\n",
    "        write_text(\"logs/last_error.txt\", str(e))\n",
    "        out = \"ABORTED_COST_CAP\"\n",
    "    except Exception as e:\n",
    "        write_text(\"logs/last_error.txt\", \"FAILED_STEP: \" + str(e))\n",
    "        out = \"FAILED_STEP\"\n",
    "    finally:\n",
    "        try:\n",
    "            manifest(tr, out) if tr else None\n",
    "        except Exception as e:\n",
    "            # last-ditch logging if manifest itself fails\n",
    "            write_text(\"logs/last_error.txt\", \"MANIFEST_FAILED: \" + str(e))\n",
    "\n",
    "    print(\"Run done\", out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def rerun_chapter(n):\n",
    "    n = int(n)\n",
    "    # Clear prior artifacts for this chapter\n",
    "    for sub in [\"research\", \"drafts\", \"edits\"]:\n",
    "        d = Path(f\"content/{sub}/{n:02d}\")\n",
    "        if d.exists():\n",
    "            for p in d.glob(\"*\"):\n",
    "                if p.is_file():\n",
    "                    p.unlink()\n",
    "\n",
    "    Path(\"logs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rt, tr = make_router()\n",
    "    pj = \"content/outline/outline.json\"\n",
    "    if not Path(pj).exists():\n",
    "        pj, _ = gen_outline(book_spec, tr, rt)\n",
    "\n",
    "    process_chapter(n, pipeline_config, tr, rt, pj)\n",
    "    assemble_book(book_spec)\n",
    "    run_qa(book_spec)\n",
    "    export_deliverables()\n",
    "    manifest(tr, \"PASS\")\n",
    "    print(\"Rerun done\", n)\n",
    "\n",
    "\n",
    "def resume():\n",
    "    return run_all()\n",
    "\n",
    "\n",
    "def verify_author_calls():\n",
    "    from pathlib import Path, PurePosixPath\n",
    "    import json\n",
    "\n",
    "    print(\"AuthorAgent call verification:\\n\")\n",
    "    any_found = False\n",
    "    # Chapter stamps\n",
    "    for p in sorted(Path(\"content/drafts\").rglob(\"*/.author_call.json\")):\n",
    "        any_found = True\n",
    "        d = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "        ch = p.parent.name\n",
    "        print(\n",
    "            f'- drafts/{ch}: called @ {d.get(\"t\")} | cached={d.get(\"cached\")} | title=\"{d.get(\"outline_title\", \"\")}\"'\n",
    "        )\n",
    "    if not any_found:\n",
    "        print(\"- No .author_call.json stamps found in content/drafts/*\")\n",
    "    print(\"\\nRecent agent call events (tail 8):\")\n",
    "    log = Path(\"logs/agent_calls.jsonl\")\n",
    "    if log.exists():\n",
    "        lines = log.read_text(encoding=\"utf-8\").splitlines()[-8:]\n",
    "        for ln in lines:\n",
    "            print(\"  \", ln)\n",
    "    else:\n",
    "        print(\"  logs/agent_calls.jsonl (missing)\")\n",
    "\n",
    "\n",
    "print(\"Controls ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter processor ready\n",
      "Converting DOCX to PDF via docx2pdf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8611b0c11fbf476ebd21f5484d7b5503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF saved at: dist\\book.pdf\n",
      "Outcome: PASS_WITH_WARNINGS\n",
      "Cost: {'total_spent_usd': 0.171984, 'run_cap_usd': 3.0, 'remaining_usd': 2.828016, 'log_items': 16}\n",
      "\n",
      "Build:\n",
      "build/book.md (26042 bytes)\n",
      "build/toc.json (2218 bytes)\n",
      "\n",
      "Dist:\n",
      "dist/book.docx (45958 bytes)\n",
      "dist/book.md (26042 bytes)\n",
      "dist/book.pdf (191982 bytes)\n",
      "dist/manifest.json (952 bytes)\n",
      "dist/qa_report.json (4092 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Demonstration Mode\n",
    "from pathlib import Path\n",
    "import os, sys, shutil, subprocess\n",
    "\n",
    "# Cheap demo run: sample a couple of chapters and disable research\n",
    "pipeline_config[\"SAMPLE_RUN_CHAPTERS\"] = int(\n",
    "    pipeline_config.get(\"SAMPLE_RUN_CHAPTERS\", 2)\n",
    ")\n",
    "pipeline_config[\"FULL_RUN\"] = False\n",
    "pipeline_config[\"RESEARCH_ENABLED\"] = True\n",
    "\n",
    "try:\n",
    "    outcome = run_all()\n",
    "except Exception as e:\n",
    "    Path(\"dist\").mkdir(parents=True, exist_ok=True)\n",
    "    manifest = {\n",
    "        \"outcome\": \"FAILED_STEP\",\n",
    "        \"failed_step\": \"run_all_top_level\",\n",
    "        \"error\": str(e),\n",
    "        \"trace\": traceback.format_exc(),\n",
    "        \"t\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
    "    }\n",
    "    Path(\"dist/manifest.json\").write_text(\n",
    "        json.dumps(manifest, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "    print(\"run_all crashed. Details saved to dist/manifest.json\")\n",
    "    outcome = \"FAILED_STEP\"\n",
    "\n",
    "docx_path = Path(\"dist/book.docx\")\n",
    "pdf_path = Path(\"dist/book.pdf\")\n",
    "pdf_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _have_exe(cmd: str) -> bool:\n",
    "    return shutil.which(cmd) is not None\n",
    "\n",
    "\n",
    "def convert_docx_to_pdf(src: Path, dst: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Best-effort DOCX→PDF:\n",
    "      1) docx2pdf (Windows/macOS w/ MS Word)\n",
    "      2) LibreOffice soffice --headless (cross-platform)\n",
    "      3) pandoc (if installed)\n",
    "    Returns True on success, False otherwise.\n",
    "    \"\"\"\n",
    "    # 1) docx2pdf on win/darwin only\n",
    "    try:\n",
    "        if sys.platform in (\"win32\", \"darwin\"):\n",
    "            from docx2pdf import convert  # may raise ImportError\n",
    "\n",
    "            print(\"Converting DOCX to PDF via docx2pdf...\")\n",
    "            convert(str(src), str(dst))\n",
    "            return dst.exists() and dst.stat().st_size > 0\n",
    "        else:\n",
    "            print(\n",
    "                \"docx2pdf is typically not supported on this platform; skipping that route.\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"docx2pdf failed: {e}\")\n",
    "\n",
    "    # 2) LibreOffice (soffice)\n",
    "    if _have_exe(\"soffice\"):\n",
    "        try:\n",
    "            print(\"Converting via LibreOffice (soffice --headless)...\")\n",
    "            # LibreOffice writes into output directory; we move/rename if needed\n",
    "            outdir = dst.parent\n",
    "            cmd = [\n",
    "                \"soffice\",\n",
    "                \"--headless\",\n",
    "                \"--convert-to\",\n",
    "                \"pdf\",\n",
    "                \"--outdir\",\n",
    "                str(outdir),\n",
    "                str(src),\n",
    "            ]\n",
    "            subprocess.run(\n",
    "                cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "            )\n",
    "            # LibreOffice names file as <name>.pdf in outdir\n",
    "            produced = outdir / (src.stem + \".pdf\")\n",
    "            if produced.exists():\n",
    "                if produced != dst:\n",
    "                    produced.replace(dst)\n",
    "                return dst.exists() and dst.stat().st_size > 0\n",
    "        except Exception as e:\n",
    "            print(f\"LibreOffice conversion failed: {e}\")\n",
    "\n",
    "    # 3) Pandoc\n",
    "    if _have_exe(\"pandoc\"):\n",
    "        try:\n",
    "            print(\"Converting via pandoc...\")\n",
    "            subprocess.run([\"pandoc\", str(src), \"-o\", str(dst)], check=True)\n",
    "            return dst.exists() and dst.stat().st_size > 0\n",
    "        except Exception as e:\n",
    "            print(f\"pandoc conversion failed: {e}\")\n",
    "\n",
    "    # No available converter\n",
    "    print(\n",
    "        \"No PDF converter available (docx2pdf/soffice/pandoc not working). Skipping PDF export.\"\n",
    "    )\n",
    "    return False\n",
    "\n",
    "\n",
    "if docx_path.exists():\n",
    "    success = convert_docx_to_pdf(docx_path, pdf_path)\n",
    "    if success:\n",
    "        print(f\"PDF saved at: {pdf_path}\")\n",
    "    else:\n",
    "        print(\"PDF export skipped or failed. See messages above.\")\n",
    "else:\n",
    "    print(\"DOCX file not found. Cannot export PDF.\")\n",
    "\n",
    "m = read_json(\"dist/manifest.json\") or {}\n",
    "print(\"Outcome:\", m.get(\"outcome\", outcome))\n",
    "print(\"Cost:\", m.get(\"cost_summary\"))\n",
    "\n",
    "\n",
    "def tree(d):\n",
    "    d = Path(d)\n",
    "    if not d.exists():\n",
    "        print(f\"{d.as_posix()} (missing)\")\n",
    "        return\n",
    "    for p in sorted(d.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            rel = p.relative_to(d).as_posix()\n",
    "            print(f\"{d.as_posix()}/{rel} ({p.stat().st_size} bytes)\")\n",
    "\n",
    "\n",
    "print(\"\\nBuild:\")\n",
    "tree(\"build\")\n",
    "\n",
    "print(\"\\nDist:\")\n",
    "tree(\"dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1d866d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome: PASS_WITH_WARNINGS\n",
      "Failed step: None\n",
      "Notes: None\n",
      "Artifacts: None\n",
      "\n",
      "Recent logs: []\n",
      "\n",
      "build/book.md exists: True | size: 26042\n",
      "dist/book.docx exists: True | size: 45958\n",
      "\n",
      "Build tree:\n",
      "build/book.md (26042 bytes)\n",
      "build/toc.json (2218 bytes)\n",
      "\n",
      "Dist tree:\n",
      "dist/book.docx (45958 bytes)\n",
      "dist/book.md (26042 bytes)\n",
      "dist/book.pdf (191982 bytes)\n",
      "dist/manifest.json (952 bytes)\n",
      "dist/qa_report.json (4092 bytes)\n",
      "AuthorAgent call verification:\n",
      "\n",
      "- drafts/01: called @ 2025-09-09T02:27:08.204207Z | cached=False | title=\"Trigger Protocol\"\n",
      "\n",
      "Recent agent call events (tail 8):\n",
      "   {\"t\": \"2025-09-09T02:26:11.323155Z\", \"agent\": \"Author\", \"model\": \"gpt-4o\", \"label\": \"ch1_draft_s3\", \"event\": \"begin\", \"cache_key\": \"b7a9dfe70eda5d6cb40e4d31e4d8eaf8b9f73392\", \"max_t\": 7000, \"force_json\": false}\n",
      "   {\"t\": \"2025-09-09T02:26:34.891050Z\", \"agent\": \"Author\", \"model\": \"gpt-4o\", \"label\": \"ch1_draft_s3\", \"event\": \"llm_ok\", \"duration_s\": 23.567, \"usage\": {\"prompt_tokens\": 3259, \"completion_tokens\": 1404, \"via\": \"chat.completions\"}}\n",
      "   {\"t\": \"2025-09-09T02:26:34.897132Z\", \"agent\": \"Author\", \"model\": \"gpt-4o\", \"label\": \"ch1_draft_s4\", \"event\": \"begin\", \"cache_key\": \"af7c353ec18740608a2ce1331dbfef7f6d67e53c\", \"max_t\": 7000, \"force_json\": false}\n",
      "   {\"t\": \"2025-09-09T02:26:53.830016Z\", \"agent\": \"Author\", \"model\": \"gpt-4o\", \"label\": \"ch1_draft_s4\", \"event\": \"llm_ok\", \"duration_s\": 18.933, \"usage\": {\"prompt_tokens\": 3202, \"completion_tokens\": 1091, \"via\": \"chat.completions\"}}\n",
      "   {\"t\": \"2025-09-09T02:26:53.845965Z\", \"agent\": \"Author\", \"model\": \"gpt-4o\", \"label\": \"ch1_draft_s5\", \"event\": \"begin\", \"cache_key\": \"a62e470fa4c26bec3461fde78c152ed7ecfd30df\", \"max_t\": 7000, \"force_json\": false}\n",
      "   {\"t\": \"2025-09-09T02:27:08.195554Z\", \"agent\": \"Author\", \"model\": \"gpt-4o\", \"label\": \"ch1_draft_s5\", \"event\": \"llm_ok\", \"duration_s\": 14.35, \"usage\": {\"prompt_tokens\": 3170, \"completion_tokens\": 1226, \"via\": \"chat.completions\"}}\n",
      "   {\"t\": \"2025-09-09T02:27:08.204207Z\", \"agent\": \"Editor\", \"model\": \"gpt-4o\", \"label\": \"ch1_edit\", \"event\": \"begin\", \"cache_key\": \"5c82172169ddb339477134e67c3093696d824bdb\", \"max_t\": 16384, \"force_json\": false}\n",
      "   {\"t\": \"2025-09-09T02:30:07.845652Z\", \"agent\": \"Editor\", \"model\": \"gpt-4o\", \"label\": \"ch1_edit\", \"event\": \"llm_ok\", \"duration_s\": 179.637, \"usage\": {\"prompt_tokens\": 6437, \"completion_tokens\": 5820, \"via\": \"chat.completions\"}}\n"
     ]
    }
   ],
   "source": [
    "# TRIAGE CELL — diagnose failing step and what's been produced\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "def _safe_json(path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't parse {path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def _tree(d):\n",
    "    d = Path(d)\n",
    "    if not d.exists():\n",
    "        print(f\"{d.as_posix()} (missing)\")\n",
    "        return\n",
    "    for p in sorted(d.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            rel = p.relative_to(d).as_posix()\n",
    "            print(f\"{d.as_posix()}/{rel} ({p.stat().st_size} bytes)\")\n",
    "\n",
    "\n",
    "# 1) What the manifest says\n",
    "man = _safe_json(\"dist/manifest.json\")\n",
    "print(\"Outcome:\", man.get(\"outcome\"))\n",
    "print(\"Failed step:\", man.get(\"failed_step\") or man.get(\"last_step\") or man.get(\"step\"))\n",
    "print(\"Notes:\", man.get(\"notes\"))\n",
    "print(\"Artifacts:\", man.get(\"artifacts\"))\n",
    "\n",
    "# 2) Recent logs (names + preview of newest)\n",
    "logs = sorted(\n",
    "    Path(\"logs\").glob(\"*.json\"), key=lambda p: p.stat().st_mtime, reverse=True\n",
    ")\n",
    "print(\"\\nRecent logs:\", [p.name for p in logs[:6]])\n",
    "if logs:\n",
    "    lp = logs[0]\n",
    "    print(f\"\\nLast log preview: {lp.name}\\n\")\n",
    "    txt = lp.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    print(txt[:2000] + (\"...\" if len(txt) > 2000 else \"\"))\n",
    "\n",
    "# 3) Check if build/book.md or dist/book.docx exist (helps decide whether to re-export)\n",
    "b = Path(\"build/book.md\")\n",
    "d = Path(\"dist/book.docx\")\n",
    "print(\n",
    "    \"\\nbuild/book.md exists:\",\n",
    "    b.exists(),\n",
    "    \"| size:\",\n",
    "    (b.stat().st_size if b.exists() else 0),\n",
    ")\n",
    "print(\n",
    "    \"dist/book.docx exists:\",\n",
    "    d.exists(),\n",
    "    \"| size:\",\n",
    "    (d.stat().st_size if d.exists() else 0),\n",
    ")\n",
    "\n",
    "# 4) Quick trees\n",
    "print(\"\\nBuild tree:\")\n",
    "_tree(\"build\")\n",
    "print(\"\\nDist tree:\")\n",
    "_tree(\"dist\")\n",
    "\n",
    "verify_author_calls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8cdfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
