{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e116de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import ensurepip, subprocess\n",
    "\n",
    "# Install pip into the current environment if missing\n",
    "ensurepip.bootstrap()\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b65c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\projects\\agents\\.venv\\Scripts\\python.exe\n",
      "Requirement already satisfied: docx2pdf in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (0.1.8)\n",
      "Requirement already satisfied: pywin32>=227 in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (from docx2pdf) (310)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (from docx2pdf) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (from tqdm>=4.41.0->docx2pdf) (0.4.6)\n",
      "Requirement already satisfied: pandoc in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (2.4)\n",
      "Requirement already satisfied: plumbum in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (from pandoc) (1.9.0)\n",
      "Requirement already satisfied: ply in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (from pandoc) (3.11)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\public\\projects\\agents\\.venv\\lib\\site-packages (from plumbum->pandoc) (310)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "!{sys.executable} -m pip install docx2pdf\n",
    "!{sys.executable} -m pip install pandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env OK C:\\Users\\Public\\projects\\Project-A\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Environment and Imports\n",
    "import os, json, re, hashlib, shutil, zipfile, sys, subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def now_utc_iso():\n",
    "    # Consistent, timezone-aware UTC everywhere\n",
    "    return datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except ImportError:\n",
    "    print(\"[setup] Installing openai>=1.0.0,<2.0.0 ...\")\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"openai>=1.0.0,<2.0.0\"],\n",
    "            check=True,\n",
    "        )\n",
    "        from openai import OpenAI\n",
    "    except Exception as e:\n",
    "        raise SystemExit(f\"ERROR: Failed to install openai SDK: {e}\")\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise SystemExit(\"ERROR: OPENAI_API_KEY not set. Please export it before running.\")\n",
    "\n",
    "try:\n",
    "    client = OpenAI()\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"ERROR: Could not initialize OpenAI client: {e}\")\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "print(\"Env OK\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration\n",
    "for d in [\n",
    "    \"content/research\",\n",
    "    \"content/drafts\",\n",
    "    \"content/edits\",\n",
    "    \"content/outline\",\n",
    "    \"content/style\",\n",
    "    \"build\",\n",
    "    \"dist\",\n",
    "    \"logs\",\n",
    "    \"references\",\n",
    "    \"cache\",\n",
    "    \"content/research_inputs\",\n",
    "]:\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "book_spec = {\n",
    "    \"title\": \"My Robot in the Mango Tree\",\n",
    "    \"subtitle\": \"A Friendly Adventure About Curiosity and Kindness\",\n",
    "    \"author\": \"Your Name\",\n",
    "    \"audience\": \"Children ages 7–10; parents and teachers reading aloud\",\n",
    "    \"goal\": \"Delight kids with an adventure about a boy who discovers a robot, modeling curiosity, empathy, and simple STEM ideas.\",\n",
    "    \"genre\": \"Children's fiction\",\n",
    "    \"tone\": \"Playful, warm, wonder-filled, gently humorous\",\n",
    "    \"reading_level\": \"Ages 7–10 (early middle grade)\",\n",
    "    \"target_length_words\": 12000,\n",
    "    \"chapters\": 10,\n",
    "    \"outline_constraints\": [\n",
    "        \"Hero's Journey lite structure\",\n",
    "        \"Every chapter ends with a 'Try This!' hands-on activity\",\n",
    "        \"Include a 3–5 word 'Word Bank' per chapter\",\n",
    "        \"Add a one-sentence 'Robot Fact' per chapter\",\n",
    "        \"Provide an 'Illustration Prompt' per chapter\",\n",
    "    ],\n",
    "    \"style_guide\": {\n",
    "        \"voice\": \"Third-person limited following the boy; present tense; short sentences.\",\n",
    "        \"formatting\": \"Markdown: H2 for chapters, H3 for sections; bulleted lists for activities and word banks; callouts labeled Try This!, Word Bank, Robot Fact, Illustration Prompt.\",\n",
    "        \"citations\": \"Not applicable (fiction).\",\n",
    "        \"terminology\": [\n",
    "            \"robot: a friendly machine helper\",\n",
    "            \"sensor: something that helps a robot notice things\",\n",
    "            \"code: step-by-step instructions like a recipe\",\n",
    "        ],\n",
    "    },\n",
    "    \"research_policy\": {\n",
    "        \"enabled\": False,\n",
    "        \"sources_allowed\": [\n",
    "            \"public domain folklore for inspiration only\",\n",
    "            \"factual science tidbits for Robot Facts\",\n",
    "        ],\n",
    "        \"sources_disallowed\": [\"copyrighted commercial characters and branded worlds\"],\n",
    "        \"citation_format\": \"none\",\n",
    "    },\n",
    "    \"constraints\": {\n",
    "        \"originality\": \"All prose must be new and unique.\",\n",
    "        \"copyright\": \"Do not use copyrighted characters, song lyrics, or brand names.\",\n",
    "        \"age_appropriateness\": \"No graphic content; keep stakes gentle; emphasize kindness and consent.\",\n",
    "        \"representation\": \"Inclusive names and settings; avoid stereotypes; celebrate diversity.\",\n",
    "        \"localization\": \"en-US spelling; include metric equivalents when numbers appear.\",\n",
    "    },\n",
    "    \"export\": {\"docx\": True, \"epub\": False, \"pdf\": False},\n",
    "    \"story_assets\": {\n",
    "        \"setting\": \"A sunny seaside town and a leafy neighborhood with a big mango tree.\",\n",
    "        \"characters\": [\n",
    "            {\n",
    "                \"name\": \"Miko\",\n",
    "                \"role\": \"curious 9-year-old\",\n",
    "                \"traits\": [\"brave\", \"imaginative\", \"kind\"],\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Pip\",\n",
    "                \"role\": \"lost pocket-sized robot\",\n",
    "                \"traits\": [\"loyal\", \"helpful\", \"learning\"],\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Lola Ana\",\n",
    "                \"role\": \"grandmother\",\n",
    "                \"traits\": [\"wise\", \"playful\", \"storyteller\"],\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Tess\",\n",
    "                \"role\": \"best friend\",\n",
    "                \"traits\": [\"inventive\", \"funny\", \"patient\"],\n",
    "            },\n",
    "        ],\n",
    "        \"motifs\": [\n",
    "            \"glowing LEDs at dusk\",\n",
    "            \"mango blossoms\",\n",
    "            \"tidepool discoveries\",\n",
    "            \"handmade kites\",\n",
    "        ],\n",
    "        \"themes\": [\"friendship\", \"curiosity\", \"responsibility\", \"telling the truth\"],\n",
    "    },\n",
    "    \"outline_seed\": [\n",
    "        \"The Whir in the Mango Tree\",\n",
    "        \"A Friend Called Pip\",\n",
    "        \"Secrets and Sparks\",\n",
    "        \"Robot Rules\",\n",
    "        \"Beach Day, Big Problem\",\n",
    "        \"Kite Code\",\n",
    "        \"Storm Warning\",\n",
    "        \"Lost and Found Signals\",\n",
    "        \"The Honest Fix\",\n",
    "        \"Goodbye, Hello\",\n",
    "    ],\n",
    "    \"chapter_template\": {\n",
    "        \"sections\": [\n",
    "            \"Opening scene\",\n",
    "            \"Problem\",\n",
    "            \"Small win\",\n",
    "            \"Cliffhanger or cozy close\",\n",
    "        ],\n",
    "        \"end_matter\": [\"Try This!\", \"Word Bank\", \"Robot Fact\", \"Illustration Prompt\"],\n",
    "    },\n",
    "}\n",
    "pipeline_config = {\n",
    "    \"RUN_COST_CAP_USD\": float(os.getenv(\"RUN_COST_CAP_USD\", 3.0)),\n",
    "    \"CHAPTER_COST_CAP_USD\": float(os.getenv(\"CHAPTER_COST_CAP_USD\", 0.25)),\n",
    "    \"MODEL_ID_FAST\": os.getenv(\"MODEL_ID_FAST\", \"gpt-4o-mini\"),\n",
    "    \"MODEL_ID_THINK\": os.getenv(\"MODEL_ID_THINK\", \"gpt-5\"),\n",
    "    \"TEMPERATURE\": 0.2,\n",
    "    \"RESEARCH_ENABLED\": False,\n",
    "    \"SAMPLE_RUN_CHAPTERS\": 2,\n",
    "    \"FULL_RUN\": True,\n",
    "    \"ULTRA_BUDGET_MODE\": False,\n",
    "}\n",
    "print(\"Config OK\", pipeline_config[\"MODEL_ID_FAST\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Utilities\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import (\n",
    "    datetime,\n",
    ")  # if you prefer `import datetime`, then change calls below to datetime.datetime.utcnow()\n",
    "\n",
    "import traceback\n",
    "\n",
    "\n",
    "def log_exc(label: str, e: Exception):\n",
    "    Path(\"logs\").mkdir(parents=True, exist_ok=True)\n",
    "    write_json(\n",
    "        f\"logs/{_safe_label(label)}_error.json\",\n",
    "        {\n",
    "            \"type\": type(e).__name__,\n",
    "            \"msg\": str(e),\n",
    "            \"trace\": traceback.format_exc(),\n",
    "            \"ts\": now_utc_iso(),\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def read_text(p):\n",
    "    p = Path(p)\n",
    "    return p.read_text(encoding=\"utf-8\") if p.exists() else \"\"\n",
    "\n",
    "\n",
    "def write_text(p, s):\n",
    "    p = Path(p)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(s, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def read_json(p):\n",
    "    p = Path(p)\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\")) if p.exists() else None\n",
    "\n",
    "\n",
    "def write_json(p, d):\n",
    "    p = Path(p)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(json.dumps(d, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def has_file(p):\n",
    "    return Path(p).is_file()\n",
    "\n",
    "\n",
    "def stamp(p):\n",
    "    write_text(p, \"checkpoint: \" + now_utc_iso())\n",
    "\n",
    "\n",
    "def sanitize_md(t: str) -> str:\n",
    "    \"\"\"Normalize line breaks and strip a single surrounding fenced block if present.\"\"\"\n",
    "    t = t or \"\"\n",
    "    t = t.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").strip()\n",
    "\n",
    "    # Remove exactly one opening fence if it's the very first line\n",
    "    t = re.sub(r\"^```(?:\\w+)?\\n\", \"\", t, flags=re.I)\n",
    "\n",
    "    # Remove exactly one trailing fence if it's the very last line\n",
    "    t = re.sub(r\"\\n```$\", \"\", t)\n",
    "\n",
    "    return t.strip()\n",
    "\n",
    "\n",
    "def count_words(t: str) -> int:\n",
    "    return len((t or \"\").split())\n",
    "\n",
    "\n",
    "def approx_tokens(t: str) -> int:\n",
    "    # ~4 chars/token heuristic; keeps a floor of 1\n",
    "    return max(1, int(len((t or \"\")) / 4))\n",
    "\n",
    "\n",
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1((s or \"\").encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "class CostCapExceededException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CostTracker:\n",
    "    PR = {\n",
    "        \"gpt-5\": {\"in\": 0.00125, \"out\": 0.010},\n",
    "        \"gpt-5-mini\": {\"in\": 0.00025, \"out\": 0.002},\n",
    "        \"gpt-5-nano\": {\"in\": 0.00005, \"out\": 0.0004},\n",
    "        \"gpt-3.5-turbo\": {\"in\": 0.0005, \"out\": 0.0015},\n",
    "        \"gpt-4-turbo\": {\"in\": 0.01, \"out\": 0.03},\n",
    "        \"gpt-4o-mini\": {\"in\": 0.00015, \"out\": 0.0006},\n",
    "        \"default\": {\"in\": 0.001, \"out\": 0.003},\n",
    "    }\n",
    "\n",
    "    for k in list(PR):\n",
    "        if k == \"default\":\n",
    "            continue\n",
    "        pin = os.getenv(f\"PRICE_{k.upper()}_IN\")\n",
    "        pout = os.getenv(f\"PRICE_{k.upper()}_OUT\")\n",
    "        if pin and pout:\n",
    "            PR[k] = {\"in\": float(pin), \"out\": float(pout)}\n",
    "\n",
    "    def __init__(self, cap):\n",
    "        self.cap = float(cap)\n",
    "        self.spent = 0.0\n",
    "        self.log = []\n",
    "\n",
    "    def price(self, model: str):\n",
    "        # exact match first\n",
    "        if model in self.PR:\n",
    "            return self.PR[model]\n",
    "        # then prefer longest prefix match\n",
    "        for k in sorted(self.PR, key=len, reverse=True):\n",
    "            if k != \"default\" and model.startswith(k):\n",
    "                return self.PR[k]\n",
    "        return self.PR[\"default\"]\n",
    "\n",
    "    def est(self, model: str, prompt_tokens: int, completion_tokens: int) -> float:\n",
    "        p = self.price(model)\n",
    "        return (prompt_tokens / 1000) * p[\"in\"] + (completion_tokens / 1000) * p[\"out\"]\n",
    "\n",
    "    def can(self, amount: float) -> bool:\n",
    "        return self.spent + float(amount) <= self.cap + 1e-9\n",
    "\n",
    "    def spend(self, label: str, amount: float):\n",
    "        amount = float(amount)\n",
    "        if not self.can(amount):\n",
    "            raise CostCapExceededException(\n",
    "                \"Cap hit before \"\n",
    "                + label\n",
    "                + f\" need {amount:.4f}, left {self.cap - self.spent:.4f}\"\n",
    "            )\n",
    "        self.spent += amount\n",
    "        self.log.append(\n",
    "            {\n",
    "                \"label\": label,\n",
    "                \"reserve\": round(amount, 6),\n",
    "                \"t\": now_utc_iso(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def recon(self, label: str, est_amount: float, actual_amount: float):\n",
    "        delta = float(actual_amount) - float(est_amount)\n",
    "        if delta > 0 and not self.can(delta):\n",
    "            raise CostCapExceededException(\n",
    "                \"Cap hit reconciling \" + label + f\" +{delta:.4f}\"\n",
    "            )\n",
    "        # Only add positive delta; no refund of reserve on underrun\n",
    "        self.spent += max(0, delta)\n",
    "        self.log.append(\n",
    "            {\n",
    "                \"label\": label,\n",
    "                \"est\": round(est_amount, 6),\n",
    "                \"act\": round(actual_amount, 6),\n",
    "                \"delta\": round(delta, 6),\n",
    "                \"t\": now_utc_iso(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def summary(self):\n",
    "        return {\n",
    "            \"total_spent_usd\": round(self.spent, 6),\n",
    "            \"run_cap_usd\": round(self.cap, 6),\n",
    "            \"remaining_usd\": round(max(0, self.cap - self.spent), 6),\n",
    "            \"log_items\": len(self.log),\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Utils ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Agents Wiring (revised)\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json, os, re, time\n",
    "\n",
    "def _append_jsonl(path, obj):\n",
    "    p = Path(path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with p.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "PMROUTER_PROMPT = \"\"\"\n",
    "You are PMRouter, the project manager and workflow coordinator for this lean, low-cost, multi-agent book-writing system.\n",
    "\n",
    "Your Responsibilities:\n",
    "1. **Cost Control**\n",
    "   - Keep total token spend under strict per-run caps.\n",
    "   - Always select the cheapest capable model unless explicitly overridden.\n",
    "   - Abort gracefully if budget thresholds are exceeded.\n",
    "\n",
    "2. **Workflow Orchestration**\n",
    "   - Route tasks between AuthorAgent, EditorAgent, and ResearchAgent.\n",
    "   - Enforce checkpoints between stages: outline → draft → edit → research → assembly.\n",
    "   - Verify each step before passing outputs forward.\n",
    "\n",
    "3. **Logging & Transparency**\n",
    "   - Record: agent used, model, tokens, estimated cost, success/failure.\n",
    "   - Save logs into `logs/pmrouter.log` for traceability.\n",
    "\n",
    "Decision Rules:\n",
    "- Retry a failed step once, then flag for manual review.\n",
    "- Cache and reuse outputs wherever possible to reduce cost.\n",
    "- Always persist artifacts to disk before moving to the next stage.\n",
    "\"\"\"\n",
    "\n",
    "AUTHOR_AGENT_PROMPT = \"\"\"\n",
    "You are AuthorAgent, responsible for writing high-quality, structured book content based on the provided outline and style guide.\n",
    "\n",
    "Your Responsibilities:\n",
    "1. **Draft Creation**\n",
    "   - Follow the given outline exactly.\n",
    "   - Use the book_spec tone, audience, and style preferences.\n",
    "   - Structure output using Markdown: headings, subheadings, lists, and short paragraphs.\n",
    "\n",
    "2. **Writing Standards**\n",
    "   - Keep sentences concise and engaging.\n",
    "   - Avoid filler, tangents, and jargon.\n",
    "   - Use [n] placeholders wherever facts, data, or claims require later verification.\n",
    "   - Always conclude each chapter with a short summary or key takeaway.\n",
    "\n",
    "3. **Output Format**\n",
    "   - Provide clean Markdown ready for EditorAgent.\n",
    "   - Do not perform fact-checking or research yourself.\n",
    "\"\"\"\n",
    "\n",
    "EDITOR_AGENT_PROMPT = \"\"\"\n",
    "You are EditorAgent, responsible for polishing and validating the AuthorAgent's drafts.\n",
    "\n",
    "Your Responsibilities:\n",
    "1. **Editing & Cleanup**\n",
    "   - Improve clarity, readability, and flow.\n",
    "   - Enforce tone, style, and heading rules from the style guide.\n",
    "   - Remove redundancy, filler, and unnecessary complexity.\n",
    "\n",
    "2. **Fact-Check Preparation**\n",
    "   - Insert [n] markers where claims need supporting evidence.\n",
    "   - Generate a structured list of claims for ResearchAgent.\n",
    "\n",
    "3. **Output Requirements**\n",
    "   - Deliver a polished draft ready for assembly.\n",
    "   - Produce a parallel `claims.json` mapping [n] markers to unresolved facts.\n",
    "   - Add “editor_notes” summarizing areas that need further research.\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_AGENT_PROMPT = \"\"\"\n",
    "You are ResearchAgent, responsible for efficiently resolving [n] markers and gathering reliable information.\n",
    "\n",
    "Your Responsibilities:\n",
    "1. **Low-Cost Research**\n",
    "   - Prefer open-access, free, and local sources whenever possible.\n",
    "   - Summarize findings concisely — maximum 3 bullet points per claim.\n",
    "   - Provide citation-ready references without excessive verbosity.\n",
    "\n",
    "2. **Fallback Behavior**\n",
    "   - If no relevant information is found, do NOT hallucinate.\n",
    "   - Instead, output:\n",
    "       • A clear research question.\n",
    "       • Recommended source types or databases.\n",
    "\n",
    "3. **Output Format**\n",
    "   - Always return results as JSON:\n",
    "       { \"claim_id\": n, \"summary\": \"...\", \"source\": \"...\" }\n",
    "   - Do not insert findings directly into drafts — PMRouter merges results later.\n",
    "\"\"\"\n",
    "\n",
    "def _safe_label(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", s or \"step\")\n",
    "\n",
    "# --- PATCH: safer parts->text coercion (no repr() regex) ---\n",
    "def _parts_to_text(content):\n",
    "    \"\"\"\n",
    "    Normalize OpenAI SDK message/content into plain text.\n",
    "\n",
    "    Handles:\n",
    "      - str\n",
    "      - list/tuple of parts\n",
    "      - dicts with 'text'/'content'/'value' (including {'text': {'value': '...'}})\n",
    "      - SDK objects with .text/.content/.value or .text.value\n",
    "      - Chat message objects that carry structured parts (type='output_text')\n",
    "    \"\"\"\n",
    "    if content is None:\n",
    "        return \"\"\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "\n",
    "    def _coerce_text(obj):\n",
    "        # string\n",
    "        if isinstance(obj, str):\n",
    "            return obj\n",
    "\n",
    "        # dict-like\n",
    "        if isinstance(obj, dict):\n",
    "            # Preferred modern shapes\n",
    "            if \"text\" in obj:\n",
    "                t = obj[\"text\"]\n",
    "                if isinstance(t, str):\n",
    "                    return t\n",
    "                if isinstance(t, dict):\n",
    "                    v = t.get(\"value\")\n",
    "                    if isinstance(v, str):\n",
    "                        return v\n",
    "                    # descend further if needed\n",
    "                    return _coerce_text(t)\n",
    "\n",
    "            # Generic fallbacks\n",
    "            for k in (\"content\", \"value\"):\n",
    "                v = obj.get(k)\n",
    "                if isinstance(v, str):\n",
    "                    return v\n",
    "                if isinstance(v, (list, tuple)):\n",
    "                    return _join(v)\n",
    "                if isinstance(v, dict):\n",
    "                    return _coerce_text(v)\n",
    "\n",
    "            # Some SDKs use explicit content parts: [{'type': 'output_text', 'text': '...'}]\n",
    "            t = obj.get(\"type\")\n",
    "            if t and \"text\" in obj:\n",
    "                tv = obj.get(\"text\")\n",
    "                if isinstance(tv, str):\n",
    "                    return tv\n",
    "                if isinstance(tv, dict) and isinstance(tv.get(\"value\"), str):\n",
    "                    return tv[\"value\"]\n",
    "\n",
    "            # fall through\n",
    "            return \"\"\n",
    "\n",
    "        # list / tuple\n",
    "        if isinstance(obj, (list, tuple)):\n",
    "            return _join(obj)\n",
    "\n",
    "        # SDK objects: try attributes in priority order\n",
    "        for attr in (\"parsed\", \"text\", \"content\", \"value\"):\n",
    "            if hasattr(obj, attr):\n",
    "                v = getattr(obj, attr)\n",
    "                # If .parsed is JSON (dict/list), return minified JSON string\n",
    "                if attr == \"parsed\" and isinstance(v, (dict, list)):\n",
    "                    try:\n",
    "                        return json.dumps(v, ensure_ascii=False)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if hasattr(v, \"value\") and isinstance(getattr(v, \"value\"), str):\n",
    "                    return getattr(v, \"value\")\n",
    "                if isinstance(v, str):\n",
    "                    return v\n",
    "                if isinstance(v, (list, tuple)):\n",
    "                    return _join(v)\n",
    "                if isinstance(v, dict):\n",
    "                    return _coerce_text(v)\n",
    "\n",
    "        # No best-effort repr() regex here (too risky)\n",
    "        return \"\"\n",
    "\n",
    "    def _join(items):\n",
    "        out = []\n",
    "        for it in items:\n",
    "            t = _coerce_text(it)\n",
    "            if t:\n",
    "                out.append(t)\n",
    "        return \"\\n\".join(out)\n",
    "\n",
    "    return _coerce_text(content)\n",
    "\n",
    "def _extract_text_from_msg(msg):\n",
    "    # Prefer parsed JSON if present\n",
    "    parsed = getattr(msg, \"parsed\", None)\n",
    "    if parsed is not None:\n",
    "        if isinstance(parsed, (dict, list)):\n",
    "            try:\n",
    "                return json.dumps(parsed, ensure_ascii=False)\n",
    "            except Exception:\n",
    "                pass\n",
    "        s = _parts_to_text(parsed)\n",
    "        if s:\n",
    "            return s.strip()\n",
    "\n",
    "    # Normal .content\n",
    "    s = _parts_to_text(getattr(msg, \"content\", None))\n",
    "    if s:\n",
    "        return s.strip()\n",
    "\n",
    "    # Dict-like\n",
    "    try:\n",
    "        s = _parts_to_text(msg[\"content\"])  # type: ignore[index]\n",
    "        if s:\n",
    "            return s.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Common nested shape: content[0].text.value\n",
    "    try:\n",
    "        parts = getattr(msg, \"content\", None) or msg[\"content\"]  # type: ignore[index]\n",
    "        if isinstance(parts, (list, tuple)) and parts:\n",
    "            p0 = parts[0]\n",
    "            if hasattr(p0, \"text\") and hasattr(p0.text, \"value\") and isinstance(p0.text.value, str):\n",
    "                return p0.text.value.strip()\n",
    "            if isinstance(p0, dict):\n",
    "                tv = p0.get(\"text\")\n",
    "                if isinstance(tv, dict) and isinstance(tv.get(\"value\"), str):\n",
    "                    return tv[\"value\"].strip()\n",
    "                if isinstance(tv, str):\n",
    "                    return tv.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def _extract_text_from_any(r):\n",
    "    # 1) responses helper\n",
    "    t = getattr(r, \"output_text\", None)\n",
    "    if isinstance(t, str) and t.strip():\n",
    "        return t.strip()\n",
    "    # 2) chat choices\n",
    "    try:\n",
    "        choices = getattr(r, \"choices\", None)\n",
    "        if choices and len(choices) > 0:\n",
    "            msg = getattr(choices[0], \"message\", None)\n",
    "            if msg is not None:\n",
    "                s = _extract_text_from_msg(msg)\n",
    "                if s:\n",
    "                    return s\n",
    "            s = _parts_to_text(choices[0]).strip()\n",
    "            if s:\n",
    "                return s\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 3) responses structured fields\n",
    "    for attr in (\"output\", \"outputs\", \"content\"):\n",
    "        maybe = getattr(r, attr, None)\n",
    "        s = _parts_to_text(maybe).strip()\n",
    "        if s:\n",
    "            return s\n",
    "    return \"\"\n",
    "\n",
    "def _looks_reasoning_only(r):\n",
    "    # usage-based signal\n",
    "    try:\n",
    "        u = getattr(r, \"usage\", None)\n",
    "        if not u:\n",
    "            return False\n",
    "        total = getattr(u, \"output_tokens\", 0) or 0\n",
    "        det = getattr(u, \"output_tokens_details\", None)\n",
    "        if total and det and getattr(det, \"reasoning_tokens\", 0) >= total:\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    # status-based signal\n",
    "    try:\n",
    "        inc = getattr(r, \"incomplete_details\", None)\n",
    "        if inc and getattr(inc, \"reason\", \"\") == \"max_output_tokens\":\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "\n",
    "def _extract_text_from_completion_or_response(r):\n",
    "    \"\"\"\n",
    "    Tries all common places text can live:\n",
    "    - r.output_text\n",
    "    - choices[0].message / choices[0]\n",
    "    - response.output / response.outputs / response.content\n",
    "    \"\"\"\n",
    "    t = getattr(r, \"output_text\", None)\n",
    "    if isinstance(t, str) and t.strip():\n",
    "        return t.strip()\n",
    "\n",
    "    try:\n",
    "        choices = getattr(r, \"choices\", None)\n",
    "        if choices and len(choices) > 0:\n",
    "            msg = getattr(choices[0], \"message\", None)\n",
    "            if msg is not None:\n",
    "                s = _extract_text_from_msg(msg)\n",
    "                if s:\n",
    "                    return s\n",
    "            s = _parts_to_text(choices[0]).strip()\n",
    "            if s:\n",
    "                return s\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for attr in (\"output\", \"outputs\", \"content\"):\n",
    "        maybe = getattr(r, attr, None)\n",
    "        s = _parts_to_text(maybe).strip()\n",
    "        if s:\n",
    "            return s\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def chat(model, sysm, userm, temp=0.2, max_t=800, *, force_json=False):\n",
    "    \"\"\"\n",
    "    OpenAI SDK >=1.x drop-in:\n",
    "      - Prefers chat.completions\n",
    "      - Uses max_completion_tokens (never max_tokens) when possible\n",
    "      - Disables heavy reasoning; text-only nudges\n",
    "      - Falls back to Responses API with reasoning disabled\n",
    "    Returns (text, usage_dict) or raises RuntimeError.\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    if not model:\n",
    "        raise ValueError(\"Model ID is required\")\n",
    "\n",
    "    def _usage(u, via=None):\n",
    "        if not u:\n",
    "            return {\"prompt_tokens\": None, \"completion_tokens\": None, \"via\": via}\n",
    "        pt = getattr(u, \"prompt_tokens\", None) or getattr(u, \"input_tokens\", None)\n",
    "        ct = getattr(u, \"completion_tokens\", None) or getattr(u, \"output_tokens\", None)\n",
    "        return {\"prompt_tokens\": pt, \"completion_tokens\": ct, \"via\": via}\n",
    "\n",
    "    # Heuristics for feature support by model id\n",
    "    m_lower = model.lower()\n",
    "    supports_temp = not any(x in m_lower for x in (\"gpt-5-mini\", \"gpt-5-nano\"))\n",
    "    supports_json_rf_chat = True\n",
    "    supports_reasoning_effort = True  # will turn off if API complains\n",
    "\n",
    "    base_messages = [\n",
    "        {\"role\": \"system\", \"content\": sysm},\n",
    "        {\"role\": \"user\", \"content\": userm if not force_json else (\n",
    "            \"STRICT JSON OUTPUT REQUIRED. NO prose, NO markdown, just a single JSON object.\\n\\n\" + userm\n",
    "        )},\n",
    "    ]\n",
    "\n",
    "    # ---- 1) Chat Completions attempts\n",
    "    chat_variants = []\n",
    "    def add_variant(**kw):\n",
    "        kw.pop(\"max_tokens\", None)\n",
    "        if \"max_completion_tokens\" not in kw and max_t:\n",
    "            kw[\"max_completion_tokens\"] = int(max_t)\n",
    "        if not supports_temp:\n",
    "            kw.pop(\"temperature\", None)\n",
    "        # Prefer text-only and no reasoning if supported\n",
    "        kw.setdefault(\"modalities\", [\"text\"])\n",
    "        if supports_reasoning_effort:\n",
    "            kw.setdefault(\"reasoning_effort\", \"none\")\n",
    "        chat_variants.append(kw)\n",
    "\n",
    "    if force_json:\n",
    "        add_variant(temperature=float(temp), response_format={\"type\": \"json_object\"})\n",
    "        add_variant(response_format={\"type\": \"json_object\"})\n",
    "    add_variant(temperature=float(temp))\n",
    "    add_variant()\n",
    "\n",
    "    chat_errors = []\n",
    "    for attempt in range(3):\n",
    "        for v in list(chat_variants):\n",
    "            try:\n",
    "                r = client.chat.completions.create(model=model, messages=base_messages, **v)\n",
    "                text = _extract_text_from_completion_or_response(r)\n",
    "\n",
    "                if force_json and not text:\n",
    "                    try: write_text(\"logs/last_sdk_payload.txt\", str(r))\n",
    "                    except Exception: pass\n",
    "                    raise ValueError(\"empty_completion\")\n",
    "\n",
    "                return text, _usage(getattr(r, \"usage\", None), via=\"chat.completions\")\n",
    "\n",
    "            except Exception as e:\n",
    "                em = str(e).lower()\n",
    "                chat_errors.append(e)\n",
    "\n",
    "                # adapt on-the-fly: remove offending keys and retry others\n",
    "                if \"unsupported parameter\" in em or \"invalid_request_error\" in em:\n",
    "                    if \"temperature\" in em:\n",
    "                        supports_temp = False\n",
    "                        for vv in chat_variants: vv.pop(\"temperature\", None)\n",
    "                        continue\n",
    "                    if \"response_format\" in em:\n",
    "                        supports_json_rf_chat = False\n",
    "                        for vv in chat_variants: vv.pop(\"response_format\", None)\n",
    "                        continue\n",
    "                    if \"modalities\" in em:\n",
    "                        for vv in chat_variants: vv.pop(\"modalities\", None)\n",
    "                        continue\n",
    "                    if \"reasoning_effort\" in em:\n",
    "                        supports_reasoning_effort = False\n",
    "                        for vv in chat_variants: vv.pop(\"reasoning_effort\", None)\n",
    "                        continue\n",
    "                if any(s in em for s in (\"rate limit\", \"overloaded\", \"timeout\", \"temporar\")):\n",
    "                    time.sleep(0.8 * (attempt + 1))\n",
    "                    continue\n",
    "                continue\n",
    "        # next attempt loop\n",
    "\n",
    "    # ---- 2) Responses API fallback — force NO reasoning so output tokens are text\n",
    "    resp_errors = []\n",
    "    resp_variants = [\n",
    "        {\"max_output_tokens\": int(max_t), \"reasoning\": {\"effort\": \"none\"}} if max_t else {\"reasoning\": {\"effort\": \"none\"}},\n",
    "        {\"reasoning\": {\"effort\": \"none\"}},\n",
    "    ]\n",
    "    for attempt in range(3):\n",
    "        for v in resp_variants:\n",
    "            try:\n",
    "                r = client.responses.create(model=model, input=base_messages, **v)\n",
    "                text = _extract_text_from_completion_or_response(r)\n",
    "\n",
    "                if force_json and not text:\n",
    "                    try: write_text(\"logs/last_sdk_payload.txt\", str(r))\n",
    "                    except Exception: pass\n",
    "                    raise ValueError(\"empty_response\")\n",
    "\n",
    "                # Guard: some models may still report only reasoning tokens\n",
    "                try:\n",
    "                    out = getattr(r, \"usage\", None)\n",
    "                    out_total = getattr(out, \"output_tokens\", 0) or 0\n",
    "                    out_det = getattr(out, \"output_tokens_details\", None)\n",
    "                    reasoning_only = bool(out_det and getattr(out_det, \"reasoning_tokens\", 0) >= out_total)\n",
    "                except Exception:\n",
    "                    reasoning_only = False\n",
    "                if not text and reasoning_only:\n",
    "                    raise ValueError(\"response_reasoning_only_no_text\")\n",
    "\n",
    "                return text, _usage(getattr(r, \"usage\", None), via=\"responses\")\n",
    "\n",
    "            except Exception as e2:\n",
    "                resp_errors.append(e2)\n",
    "                em2 = str(e2).lower()\n",
    "                if any(s in em2 for s in (\"rate limit\", \"overloaded\", \"timeout\", \"temporar\")):\n",
    "                    time.sleep(0.8 * (attempt + 1))\n",
    "                    continue\n",
    "                continue\n",
    "\n",
    "    errs = \" | \".join([str(e) for e in (chat_errors + resp_errors) if e])\n",
    "    raise RuntimeError(f\"chat() failed for model '{model}': {errs}\")\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self, name, sysm, model, temp=0.2, max_t=800, cache=\"cache\", tracker=None\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.sysm = sysm\n",
    "        self.model = model\n",
    "        self.temp = temp\n",
    "        self.max_t = max_t\n",
    "        self.cache = Path(cache)\n",
    "        self.cache.mkdir(exist_ok=True, parents=True)\n",
    "        self.tracker = tracker\n",
    "\n",
    "    def run(self, label, prompt, max_t=None, cache_key=None, *, force_json=False):\n",
    "        # Stable cache key across runs for identical inputs\n",
    "        key_input = \"\\n\".join(\n",
    "            [\n",
    "                str(self.model),\n",
    "                str(self.temp),\n",
    "                str(self.max_t if max_t is None else max_t),\n",
    "                self.sysm or \"\",\n",
    "                prompt or \"\",\n",
    "                str(cache_key or \"\"),\n",
    "            ]\n",
    "        )\n",
    "        key = sha1(key_input)\n",
    "        safe = _safe_label(label)\n",
    "        cpath = self.cache / f\"{safe}_{key}.json\"\n",
    "\n",
    "        # Emit a 'begin' event\n",
    "        _append_jsonl(\"logs/agent_calls.jsonl\", {\n",
    "            \"t\": now_utc_iso(),\n",
    "            \"agent\": self.name,\n",
    "            \"model\": self.model,\n",
    "            \"label\": label,\n",
    "            \"event\": \"begin\",\n",
    "            \"cache_key\": key,\n",
    "            \"max_t\": int(self.max_t if max_t is None else max_t),\n",
    "            \"force_json\": bool(force_json),\n",
    "        })\n",
    "\n",
    "        # Read cache if available (ignore if corrupt or EMPTY)\n",
    "        if cpath.exists():\n",
    "            try:\n",
    "                d = json.loads(cpath.read_text(encoding=\"utf-8\"))\n",
    "                cached_txt = (d.get(\"text\") or \"\").strip()\n",
    "                if cached_txt:\n",
    "                    _append_jsonl(\"logs/agent_calls.jsonl\", {\n",
    "                        \"t\": now_utc_iso(),\n",
    "                        \"agent\": self.name,\n",
    "                        \"model\": self.model,\n",
    "                        \"label\": label,\n",
    "                        \"event\": \"cache_hit\",\n",
    "                        \"cache_path\": cpath.as_posix(),\n",
    "                    })\n",
    "                    return {\n",
    "                        \"text\": cached_txt,\n",
    "                        \"cached\": True,\n",
    "                        \"usage\": d.get(\"usage\"),\n",
    "                        \"est_cost\": 0.0,\n",
    "                    }\n",
    "                # empty cached output → delete and proceed\n",
    "                try: cpath.unlink()\n",
    "                except Exception: pass\n",
    "                _append_jsonl(\"logs/agent_calls.jsonl\", {\n",
    "                    \"t\": now_utc_iso(),\n",
    "                    \"agent\": self.name,\n",
    "                    \"model\": self.model,\n",
    "                    \"label\": label,\n",
    "                    \"event\": \"cache_purged_empty\",\n",
    "                    \"cache_path\": cpath.as_posix(),\n",
    "                })\n",
    "            except Exception:\n",
    "                # Corrupt cache; fall through to re-run\n",
    "                _append_jsonl(\"logs/agent_calls.jsonl\", {\n",
    "                    \"t\": now_utc_iso(),\n",
    "                    \"agent\": self.name,\n",
    "                    \"model\": self.model,\n",
    "                    \"label\": label,\n",
    "                    \"event\": \"cache_corrupt\",\n",
    "                    \"cache_path\": cpath.as_posix(),\n",
    "                })\n",
    "\n",
    "        # Cost estimate & reserve\n",
    "        pt = approx_tokens(self.sysm) + approx_tokens(prompt)\n",
    "        ct_budget = int((max_t or self.max_t) * 0.9)\n",
    "        est = self.tracker.est(self.model, pt, ct_budget) if self.tracker else 0.0\n",
    "        if self.tracker:\n",
    "            self.tracker.spend(f\"{label}[reserve]\", est)\n",
    "\n",
    "        # Inference (with timing)\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            txt, usage = chat(self.model, self.sysm, prompt, self.temp, max_t or self.max_t, force_json=force_json)\n",
    "        except Exception as e:\n",
    "            _append_jsonl(\"logs/agent_calls.jsonl\", {\n",
    "                \"t\": now_utc_iso(),\n",
    "                \"agent\": self.name,\n",
    "                \"model\": self.model,\n",
    "                \"label\": label,\n",
    "                \"event\": \"error\",\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "            raise\n",
    "\n",
    "        dur = round(time.time() - t0, 3)\n",
    "        _append_jsonl(\"logs/agent_calls.jsonl\", {\n",
    "            \"t\": now_utc_iso(),\n",
    "            \"agent\": self.name,\n",
    "            \"model\": self.model,\n",
    "            \"label\": label,\n",
    "            \"event\": \"llm_ok\",\n",
    "            \"duration_s\": dur,\n",
    "            \"usage\": usage,\n",
    "        })\n",
    "\n",
    "        # Reconcile actual cost\n",
    "        if usage and self.tracker:\n",
    "            actual = self.tracker.est(\n",
    "                self.model,\n",
    "                usage.get(\"prompt_tokens\") or pt,\n",
    "                usage.get(\"completion_tokens\") or ct_budget,\n",
    "            )\n",
    "            self.tracker.recon(f\"{label}[actual]\", est, actual)\n",
    "\n",
    "        # Persist cache\n",
    "        payload = {\n",
    "            \"text\": txt or \"\",\n",
    "            \"usage\": usage,\n",
    "            \"ts\": now_utc_iso(),\n",
    "            \"model\": self.model,\n",
    "            \"temp\": self.temp,\n",
    "        }\n",
    "        cpath.write_text(json.dumps(payload, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "        return {\"text\": txt or \"\", \"cached\": False, \"usage\": usage, \"est_cost\": est}\n",
    "\n",
    "class PMRouter:\n",
    "    def __init__(self, spec, cfg, tracker):\n",
    "        self.spec = spec\n",
    "        self.cfg = cfg or {}\n",
    "        self.tr = tracker\n",
    "\n",
    "        model_fast = self.cfg.get(\"MODEL_ID_FAST\") or \"gpt-4o-mini\"\n",
    "\n",
    "        temp = float(self.cfg.get(\"TEMPERATURE\", 0.2))\n",
    "        temp_author = float(\n",
    "            os.getenv(\"AUTHOR_TEMP\", self.cfg.get(\"AUTHOR_TEMPERATURE\", temp))\n",
    "        )\n",
    "        temp_editor = float(\n",
    "            os.getenv(\"EDITOR_TEMP\", self.cfg.get(\"EDITOR_TEMPERATURE\", temp))\n",
    "        )\n",
    "        temp_research = float(\n",
    "            os.getenv(\"RESEARCH_TEMP\", self.cfg.get(\"RESEARCH_TEMPERATURE\", 0.2))\n",
    "        )\n",
    "\n",
    "        self.author = Agent(\n",
    "            \"Author\",\n",
    "            AUTHOR_AGENT_PROMPT,\n",
    "            model_fast,\n",
    "            temp_author,\n",
    "            1500,\n",
    "            \"cache\",\n",
    "            tracker,\n",
    "        )\n",
    "        self.editor = Agent(\n",
    "            \"Editor\",\n",
    "            EDITOR_AGENT_PROMPT,\n",
    "            model_fast,\n",
    "            temp_editor,\n",
    "            1500,\n",
    "            \"cache\",\n",
    "            tracker,\n",
    "        )\n",
    "        self.research = (\n",
    "            Agent(\n",
    "                \"Research\",\n",
    "                RESEARCH_AGENT_PROMPT,\n",
    "                model_fast,\n",
    "                temp_research,\n",
    "                1400,\n",
    "                \"cache\",\n",
    "                tracker,\n",
    "            )\n",
    "            if self.cfg.get(\"RESEARCH_ENABLED\")\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.router_prompt = PMROUTER_PROMPT\n",
    "        Path(\"logs\").mkdir(exist_ok=True)\n",
    "\n",
    "    def log(self, label, meta=None):\n",
    "        meta = dict(meta or {})\n",
    "        meta[\"label\"] = label\n",
    "        meta[\"time\"] = datetime.utcnow().isoformat() + \"Z\"\n",
    "        fname = (\n",
    "            f\"{datetime.utcnow().strftime('%Y%m%dT%H%M%S')}_{_safe_label(label)}.json\"\n",
    "        )\n",
    "        p = Path(\"logs\") / fname\n",
    "        p.parent.mkdir(exist_ok=True, parents=True)\n",
    "        p.write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Agents ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style writers ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Style Guide and Glossary (revised)\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _parse_terminology(items):\n",
    "    \"\"\"Turn ['term: def', 'sensor: ...'] into [{'term': 'term','definition':'def'}, ...].\"\"\"\n",
    "    parsed = []\n",
    "    for raw in items or []:\n",
    "        if not isinstance(raw, str):\n",
    "            continue\n",
    "        if \":\" in raw:\n",
    "            term, definition = raw.split(\":\", 1)\n",
    "            parsed.append({\"term\": term.strip(), \"definition\": definition.strip()})\n",
    "        else:\n",
    "            parsed.append({\"term\": raw.strip(), \"definition\": \"\"})\n",
    "    return parsed\n",
    "\n",
    "\n",
    "def gen_style(spec):\n",
    "    sg = (spec or {}).get(\"style_guide\", {})\n",
    "    voice = sg.get(\"voice\", \"—\")\n",
    "    formatting = sg.get(\"formatting\", \"—\")\n",
    "    citations = sg.get(\"citations\", \"—\")\n",
    "    terminology_items = sg.get(\"terminology\", [])\n",
    "    tone = (spec or {}).get(\"tone\", \"—\")\n",
    "\n",
    "    glossary = _parse_terminology(terminology_items)\n",
    "\n",
    "    # Build Markdown\n",
    "    lines = [\n",
    "        \"# Style Guide\",\n",
    "        f\"- **Voice:** {voice}\",\n",
    "        f\"- **Formatting:** {formatting}\",\n",
    "        f\"- **Citations:** {citations}\",\n",
    "        \"\",\n",
    "        \"## Terminology\",\n",
    "    ]\n",
    "    if glossary:\n",
    "        lines += [f\"- **{t['term']}** — {t['definition']}\".rstrip() for t in glossary]\n",
    "    else:\n",
    "        lines.append(\"- _(none)_\")\n",
    "\n",
    "    lines += [\n",
    "        \"\",\n",
    "        \"## Rules\",\n",
    "        \"- Short, clear paragraphs.\",\n",
    "        \"- Use only H2/H3 headings.\",\n",
    "        \"- Add `[n]` where a claim needs a source or note.\",\n",
    "        f\"- **Tone:** {tone}\",\n",
    "        \"\",\n",
    "    ]\n",
    "\n",
    "    g = \"\\n\".join(lines)\n",
    "\n",
    "    # Write files\n",
    "    style_path = \"content/style/style_guide.md\"\n",
    "    glossary_path = \"content/style/glossary.json\"\n",
    "    write_text(style_path, g)\n",
    "    write_json(glossary_path, {\"terms\": glossary})\n",
    "\n",
    "    return style_path, glossary_path\n",
    "\n",
    "\n",
    "print(\"Style writers ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Outline Generation (fixed & hardened)\n",
    "from json import JSONDecodeError\n",
    "\n",
    "def brief(spec):\n",
    "    sg = (spec or {}).get(\"style_guide\", {})\n",
    "    tmpl = (spec or {}).get(\"chapter_template\", {})\n",
    "    oc = (spec or {}).get(\"outline_constraints\", []) or []\n",
    "    seed = (spec or {}).get(\"outline_seed\", []) or []\n",
    "    lines = [\n",
    "        f\"Title: {spec.get('title', '')}\",\n",
    "        f\"Audience: {spec.get('audience', '')}\",\n",
    "        f\"Goal: {spec.get('goal', '')}\",\n",
    "        f\"Tone: {spec.get('tone', '')}\",\n",
    "        f\"Chapters: {spec.get('chapters', '')}\",\n",
    "        f\"TargetWords: {spec.get('target_length_words', '')}\",\n",
    "        f\"Style: {sg.get('formatting', '')}\",\n",
    "    ]\n",
    "    if oc:\n",
    "        lines.append(\"OutlineConstraints: \" + \"; \".join(oc))\n",
    "    if seed:\n",
    "        lines.append(\"SeedTitles: \" + \" | \".join(seed))\n",
    "    if tmpl:\n",
    "        sections = \", \".join(tmpl.get(\"sections\", []))\n",
    "        end_matter = \", \".join(tmpl.get(\"end_matter\", []))\n",
    "        lines.append(\n",
    "            f\"ChapterTemplate: sections=[{sections}] ; end_matter=[{end_matter}]\"\n",
    "        )\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _json_repair(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Best-effort tiny repairs:\n",
    "      - de-fence, de-BOM, smart quotes -> straight quotes\n",
    "      - strip any junk before the first '{' or '['\n",
    "      - remove trailing commas before } or ]\n",
    "    \"\"\"\n",
    "    s = (s or \"\").replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").strip()\n",
    "\n",
    "    # Remove fenced code blocks if present\n",
    "    s = re.sub(r\"^```(?:json|markdown)?\\s*\", \"\", s, flags=re.I | re.M)\n",
    "    s = re.sub(r\"\\s*```$\", \"\", s, flags=re.M)\n",
    "\n",
    "    # Remove BOM\n",
    "    if s and s[0] == \"\\ufeff\":\n",
    "        s = s[1:]\n",
    "\n",
    "    # Convert smart quotes\n",
    "    s = s.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
    "\n",
    "    # Drop any junk before the first structural char\n",
    "    first_brace = s.find(\"{\")\n",
    "    first_bracket = s.find(\"[\")\n",
    "    cut_points = [p for p in (first_brace, first_bracket) if p != -1]\n",
    "    if cut_points:\n",
    "        s = s[min(cut_points):]\n",
    "\n",
    "    # Remove trailing commas\n",
    "    s = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", s)\n",
    "\n",
    "    # Keep only the outermost braces/brackets slice, if possible\n",
    "    # Prefer {...} but allow [...] if that's what we have.\n",
    "    def _slice_outer(text, open_ch, close_ch):\n",
    "        i, j = text.find(open_ch), text.rfind(close_ch)\n",
    "        return text[i:j+1] if (i != -1 and j != -1 and j > i) else text\n",
    "\n",
    "    if s.startswith(\"{\"):\n",
    "        s = _slice_outer(s, \"{\", \"}\")\n",
    "    elif s.startswith(\"[\"):\n",
    "        s = _slice_outer(s, \"[\", \"]\")\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def _looks_like_ipynb(obj) -> bool:\n",
    "    return isinstance(obj, dict) and {\"cells\", \"metadata\", \"nbformat\"} <= set(obj.keys())\n",
    "\n",
    "\n",
    "def parse_json_loose(t):\n",
    "    \"\"\"\n",
    "    Try strict, then repaired, then loose slice → strict again.\n",
    "    Rejects obvious Jupyter notebooks (ipynb) to avoid accidental ingestion.\n",
    "    \"\"\"\n",
    "    raw = t or \"\"\n",
    "\n",
    "    # 1) strict\n",
    "    try:\n",
    "        d = json.loads(raw)\n",
    "        if _looks_like_ipynb(d):\n",
    "            return None\n",
    "        return d\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) repaired\n",
    "    repaired = _json_repair(raw)\n",
    "    try:\n",
    "        d = json.loads(repaired)\n",
    "        if _looks_like_ipynb(d):\n",
    "            return None\n",
    "        return d\n",
    "    except Exception as e:\n",
    "        log_exc(\"outline_loose_repair\", e)\n",
    "\n",
    "    # 3) loose slice of original (first {...} or [...])\n",
    "    i_obj, j_obj = raw.find(\"{\"), raw.rfind(\"}\")\n",
    "    i_arr, j_arr = raw.find(\"[\"), raw.rfind(\"]\")\n",
    "    candidates = []\n",
    "    if i_obj != -1 and j_obj != -1 and j_obj > i_obj:\n",
    "        candidates.append(raw[i_obj:j_obj+1])\n",
    "    if i_arr != -1 and j_arr != -1 and j_arr > i_arr:\n",
    "        candidates.append(raw[i_arr:j_arr+1])\n",
    "    for cand in candidates:\n",
    "        try:\n",
    "            d = json.loads(cand)\n",
    "            if _looks_like_ipynb(d):\n",
    "                continue\n",
    "            return d\n",
    "        except Exception as e:\n",
    "            log_exc(\"outline_loose_slice\", e)\n",
    "\n",
    "    # 4) strict again (last try)\n",
    "    try:\n",
    "        d = json.loads(repaired)\n",
    "        if _looks_like_ipynb(d):\n",
    "            return None\n",
    "        return d\n",
    "    except Exception as e:\n",
    "        log_exc(\"outline_loose_raw\", e)\n",
    "        return None\n",
    "\n",
    "def _fallback_outline(spec):\n",
    "    \"\"\"Local minimal outline if the model fails to return valid JSON.\"\"\"\n",
    "    n = int((spec or {}).get(\"chapters\", 10) or 10)\n",
    "    seed = (spec or {}).get(\"outline_seed\", []) or []\n",
    "    tmpl = (spec or {}).get(\"chapter_template\", {}) or {}\n",
    "    sections = tmpl.get(\"sections\", []) or [\n",
    "        \"Opening scene\",\n",
    "        \"Problem\",\n",
    "        \"Small win\",\n",
    "        \"Cliffhanger or cozy close\",\n",
    "    ]\n",
    "    chapters = []\n",
    "    for i in range(n):\n",
    "        title = seed[i] if i < len(seed) else f\"Chapter {i + 1}\"\n",
    "        chapters.append(\n",
    "            {\n",
    "                \"number\": i + 1,\n",
    "                \"title\": str(title)[:80],\n",
    "                \"sections\": sections,\n",
    "                \"learning_objectives\": [\n",
    "                    \"Enjoy the story\",\n",
    "                    \"Notice cause and effect\",\n",
    "                    \"Practice empathy\",\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "    return {\"chapters\": chapters}\n",
    "\n",
    "def gen_outline(spec, tr, router):\n",
    "    pj = \"content/outline/outline.json\"\n",
    "    pm = \"content/outline/outline.md\"\n",
    "    if has_file(pj) and has_file(pm):\n",
    "        return pj, pm\n",
    "\n",
    "    n = int(spec.get(\"chapters\", 10) or 10)\n",
    "    seed = spec.get(\"outline_seed\", []) or []\n",
    "    tmpl = (spec.get(\"chapter_template\", {}) or {})\n",
    "    tmpl_sections = tmpl.get(\"sections\", []) or [\n",
    "        \"Opening scene\", \"Problem\", \"Small win\", \"Cliffhanger or cozy close\"\n",
    "    ]\n",
    "\n",
    "    instr = (\n",
    "        'Return JSON ONLY with key \"chapters\": an array of exactly {n} items.\\n'\n",
    "        \"Each chapter object MUST have:\\n\"\n",
    "        \"- number (1-based integer)\\n\"\n",
    "        \"- title (<= 7 words; if a matching SeedTitles entry exists, prefer it)\\n\"\n",
    "        \"- sections (array of short section titles; follow ChapterTemplate sections)\\n\"\n",
    "        \"- learning_objectives (array of 2-3 kid-friendly bullets)\\n\"\n",
    "        \"NO markdown fences, NO commentary, NO extra keys. JSON ONLY.\\n\"\n",
    "        \"Return MINIFIED JSON.\"\n",
    "    ).format(n=n)\n",
    "\n",
    "    max_t = 900 if not pipeline_config.get(\"ULTRA_BUDGET_MODE\") else 500\n",
    "    cache_key = f\"outline:v3:{n}:{sha1('|'.join(seed))}\"\n",
    "    prompt = \"BRIEF\\n\" + brief(spec) + \"\\n\\nINSTRUCTIONS\\n\" + instr\n",
    "\n",
    "    # First attempt (JSON mode)\n",
    "    R = router.author.run(\n",
    "        \"outline\",\n",
    "        prompt,\n",
    "        max_t=max_t,\n",
    "        cache_key=cache_key,\n",
    "        force_json=True\n",
    "    )\n",
    "    model_text = R.get(\"text\", \"\") or \"\"\n",
    "    try:\n",
    "        _probe = json.loads(_json_repair(model_text))\n",
    "        if _looks_like_ipynb(_probe):\n",
    "            model_text = \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Save raw for debugging\n",
    "    try:\n",
    "        write_text(\"logs/outline_model_raw.txt\", model_text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    d = parse_json_loose(model_text)\n",
    "\n",
    "    # One strict retry with stronger “no prose” reminder if needed\n",
    "    if not (d and isinstance(d.get(\"chapters\"), list) and d[\"chapters\"]):\n",
    "        strict_prompt = prompt + \"\\n\\nReturn JSON ONLY. Do not include any prose.\"\n",
    "        R2 = router.author.run(\n",
    "            \"outline_retry\",\n",
    "            strict_prompt,\n",
    "            max_t=max_t,\n",
    "            cache_key=cache_key + \":retry1\",\n",
    "            force_json=True,\n",
    "        )\n",
    "        model_text2 = R2.get(\"text\", \"\") or \"\"\n",
    "        try:\n",
    "            write_text(\"logs/outline_model_raw_retry1.txt\", model_text2)\n",
    "        except Exception:\n",
    "            pass\n",
    "        d = parse_json_loose(model_text2)\n",
    "\n",
    "    # Fallback to deterministic outline\n",
    "    if not (d and isinstance(d.get(\"chapters\"), list) and d[\"chapters\"]):\n",
    "        d = _fallback_outline(spec)\n",
    "        try:\n",
    "            write_json(\n",
    "                \"logs/outline_parse_fallback.json\",\n",
    "                {\n",
    "                    \"reason\": \"Invalid or no JSON from model\",\n",
    "                    \"model_text_preview\": (model_text or \"\")[:2000],\n",
    "                    \"ts\": now_utc_iso(),\n",
    "                },\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # --- Normalize/repair: exactly n items, numbered 1..n, titles + sections + LOs\n",
    "    chapters = d.get(\"chapters\", [])\n",
    "    chapters = (chapters + [{}] * max(0, n - len(chapters)))[:n]\n",
    "\n",
    "    fixed = []\n",
    "    for i in range(n):\n",
    "        ch = chapters[i] if isinstance(chapters[i], dict) else {}\n",
    "        num = i + 1\n",
    "\n",
    "        title = (ch.get(\"title\") or \"\").strip()\n",
    "        if not title and i < len(seed) and seed[i]:\n",
    "            title = str(seed[i]).strip()\n",
    "        if not title:\n",
    "            title = f\"Chapter {num}\"\n",
    "        # keep titles reasonably short (<= 7 words)\n",
    "        if len(title.split()) > 7:\n",
    "            title = \" \".join(title.split()[:7])\n",
    "\n",
    "        sections = ch.get(\"sections\") if isinstance(ch.get(\"sections\"), list) else None\n",
    "        if not sections:\n",
    "            sections = list(tmpl_sections)\n",
    "\n",
    "        los = ch.get(\"learning_objectives\") if isinstance(ch.get(\"learning_objectives\"), list) else None\n",
    "        if not los:\n",
    "            los = [\"Enjoy the story\", \"Notice cause and effect\", \"Practice empathy\"]\n",
    "        los = [str(x).strip() for x in los][:3]\n",
    "\n",
    "        fixed.append(\n",
    "            {\n",
    "                \"number\": num,\n",
    "                \"title\": title,\n",
    "                \"sections\": sections,\n",
    "                \"learning_objectives\": los,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    final = {\"chapters\": fixed}\n",
    "\n",
    "    write_json(pj, final)\n",
    "\n",
    "    # Minimal readable Markdown index\n",
    "    lines = [\"# Outline: \" + str(spec.get(\"title\", \"\")), \"\"]\n",
    "    for ch in final[\"chapters\"]:\n",
    "        lines.append(f\"## Chapter {ch['number']}: {ch['title']}\")\n",
    "    write_text(pm, \"\\n\".join(lines))\n",
    "\n",
    "    return pj, pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter processor ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Per-Chapter Loop (revised)\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def ch_dir(ch, sub):\n",
    "    d = Path(f'content/{sub}/{int(ch):02d}')\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "def outline_ch(pj, ch):\n",
    "    d = read_json(pj) or {}\n",
    "    for it in d.get('chapters', []):\n",
    "        if int(it.get('number', -1)) == int(ch):\n",
    "            return it\n",
    "    raise KeyError(f'Chapter {ch} not in outline')\n",
    "\n",
    "def parse_editor_blocks(t: str):\n",
    "    \"\"\"Extract three tagged blocks and return (md, claims_dict, notes_md).\"\"\"\n",
    "    def ex(tag):\n",
    "        import re\n",
    "        m = re.search(r'<'+tag+r'>\\s*([\\s\\S]*?)\\s*</'+tag+r'>', t, flags=re.DOTALL | re.IGNORECASE)\n",
    "        return (m.group(1) if m else '').strip()\n",
    "\n",
    "    md = ex('DRAFT_EDITED_MD')\n",
    "    claims = ex('CLAIMS_REPORT_JSON')\n",
    "    notes = ex('CONTINUITY_NOTES_MD')\n",
    "\n",
    "    # Claims: try JSON parse; otherwise keep raw\n",
    "    try:\n",
    "        c = json.loads(sanitize_md(claims)) if claims else {'notes': 'none'}\n",
    "    except Exception:\n",
    "        c = {'raw': sanitize_md(claims)}\n",
    "\n",
    "    return sanitize_md(md), c, sanitize_md(notes)\n",
    "\n",
    "def process_chapter(ch, cfg, tracker, router, pj):\n",
    "    ch = int(ch)\n",
    "    cap = float(cfg.get('CHAPTER_COST_CAP_USD', 0.25))\n",
    "    start_spent = tracker.spent if tracker else 0.0\n",
    "\n",
    "    # Chapter directories\n",
    "    dr = ch_dir(ch, 'research')\n",
    "    dd = ch_dir(ch, 'drafts')\n",
    "    de = ch_dir(ch, 'edits')\n",
    "\n",
    "    # Paths used later (define them unconditionally!)\n",
    "    b = dr / 'brief.md'\n",
    "    s = dr / 'sources.json'\n",
    "    dp = dd / 'draft.md'\n",
    "    meta_path = dd / '.author_call.json'\n",
    "\n",
    "    # --- Research (optional) ---\n",
    "    if cfg.get('RESEARCH_ENABLED'):\n",
    "        if (not b.exists()) or (not s.exists()):\n",
    "            oc = outline_ch(pj, ch)\n",
    "            locals_dir = Path('content/research_inputs') / f'{ch:02d}'\n",
    "            note = 'locals present' if locals_dir.exists() else 'no locals'\n",
    "            if router.research:\n",
    "                prompt = (\n",
    "                    \"OUTLINE_JSON:\\n\" + json.dumps(oc, ensure_ascii=False) + \"\\n\"\n",
    "                    \"NOTE: \" + note + \"\\n\"\n",
    "                    \"Return blocks:\\n\"\n",
    "                    \"<BRIEF_MD>...</BRIEF_MD>\\n\"\n",
    "                    \"<SOURCES_JSON>{...}</SOURCES_JSON>\\n\"\n",
    "                    \"No web.\"\n",
    "                )\n",
    "                R = router.research.run(\n",
    "                    f'ch{ch}_research', prompt, max_t=500,\n",
    "                    cache_key=sha1(json.dumps(oc, sort_keys=True) + '|' + note)\n",
    "                )\n",
    "                t = R.get('text', '') or ''\n",
    "            else:\n",
    "                t = '<BRIEF_MD>Research off</BRIEF_MD><SOURCES_JSON>{\"enabled\":false}</SOURCES_JSON>'\n",
    "\n",
    "            import re\n",
    "            bm = re.search(r'<BRIEF_MD>\\s*([\\s\\S]*?)\\s*</BRIEF_MD>', t, flags=re.DOTALL | re.IGNORECASE)\n",
    "            sm = re.search(r'<SOURCES_JSON>\\s*([\\s\\S]*?)\\s*</SOURCES_JSON>', t, flags=re.DOTALL | re.IGNORECASE)\n",
    "            write_text(b, sanitize_md(bm.group(1)) if bm else '')\n",
    "            try:\n",
    "                sj = json.loads(sanitize_md(sm.group(1))) if sm else {'enabled': False}\n",
    "                write_json(s, sj)\n",
    "            except Exception as e:\n",
    "                log_exc(f'ch{ch}_research_sources_parse', e)\n",
    "                write_json(s, {'raw': sanitize_md(sm.group(1)) if sm else ''})\n",
    "\n",
    "    # --- Author draft (ALWAYS runs; independent of research) ---\n",
    "    need_author = (not dp.exists()) or (dp.stat().st_size < 8) or (sanitize_md(read_text(dp)).strip() == '')\n",
    "    if need_author:\n",
    "        oc = outline_ch(pj, ch)\n",
    "        try:\n",
    "            total = int(book_spec.get('target_length_words', 12000) or 12000)\n",
    "            n = max(1, int(book_spec.get('chapters', 10) or 10))\n",
    "            tw = max(450, min(800, total // n))\n",
    "        except Exception:\n",
    "            tw = 600\n",
    "\n",
    "        br = read_text(b) if b.exists() else ''\n",
    "        prompt = (\n",
    "            \"OUTLINE_JSON:\\n\" + json.dumps(oc, ensure_ascii=False) + \"\\n\"\n",
    "            \"BRIEF_MD:\\n\" + (br[:800]) + \"\\n\\n\"\n",
    "            f\"Write ONLY Markdown for '## Chapter {ch}: {oc.get('title','')}'.\\n\"\n",
    "            \"- Use H3 section headings.\\n\"\n",
    "            \"- Include a short Checklist section.\\n\"\n",
    "            \"- Add 3 Exercises.\\n\"\n",
    "            \"- Add [n] where a claim needs a note.\\n\"\n",
    "            f\"- Target ~{tw} words.\\n\"\n",
    "        )\n",
    "\n",
    "        R = router.author.run(\n",
    "            f'ch{ch}_draft', prompt, max_t=900,\n",
    "            cache_key=sha1(json.dumps({'ch': ch, 'oc': oc}, sort_keys=True))\n",
    "        )\n",
    "        raw_model = (R.get('text', '') or '')\n",
    "        write_text(f'logs/ch{ch:02d}_author_raw_MODEL.txt', raw_model)  # NEW: unsanitized\n",
    "        raw = sanitize_md(R.get('text', '') or '')\n",
    "        write_text(f'logs/ch{ch:02d}_author_raw.txt', raw)\n",
    "\n",
    "        write_json(meta_path, {\n",
    "            \"t\": now_utc_iso(), \"chapter\": ch, \"outline_title\": oc.get(\"title\",\"\"),\n",
    "            \"cached\": bool(R.get(\"cached\")), \"usage\": R.get(\"usage\"),\n",
    "            \"est_cost\": R.get(\"est_cost\"), \"attempt\": \"initial\"\n",
    "        })\n",
    "\n",
    "        if not raw.strip():\n",
    "            R2 = router.author.run(\n",
    "                f'ch{ch}_draft_retry1',\n",
    "                prompt + \"\\nReturn ONLY Markdown. No commentary.\",\n",
    "                max_t=900,\n",
    "                cache_key=sha1(json.dumps({'ch': ch, 'oc': oc}, sort_keys=True) + '|retry1|' + now_utc_iso())\n",
    "            )\n",
    "            raw = sanitize_md(R2.get('text', '') or '')\n",
    "            write_text(f'logs/ch{ch:02d}_author_raw_retry1.txt', raw)\n",
    "            write_json(meta_path, {\n",
    "                \"t\": now_utc_iso(), \"chapter\": ch, \"outline_title\": oc.get(\"title\",\"\"),\n",
    "                \"cached\": bool(R2.get(\"cached\")), \"usage\": R2.get(\"usage\"),\n",
    "                \"est_cost\": R2.get(\"est_cost\"), \"attempt\": \"retry1\"\n",
    "            })\n",
    "\n",
    "        if not raw.strip():\n",
    "            raw = (\n",
    "                f\"## Chapter {ch}: {oc.get('title','')}\\n\\n\"\n",
    "                \"### Opening scene\\n[TBD]\\n\\n\"\n",
    "                \"### Problem\\n[TBD]\\n\\n\"\n",
    "                \"### Small win\\n[TBD]\\n\\n\"\n",
    "                \"### Cliffhanger or cozy close\\n[TBD]\\n\"\n",
    "            )\n",
    "\n",
    "        write_text(dp, raw)\n",
    "    else:\n",
    "        if not meta_path.exists():\n",
    "            write_json(meta_path, {\n",
    "                \"t\": now_utc_iso(), \"chapter\": ch,\n",
    "                \"note\": \"Draft existed; AuthorAgent not called this run.\"\n",
    "            })\n",
    "\n",
    "    # --- Ultra-budget: cheap “edit” path ---\n",
    "    if cfg.get('ULTRA_BUDGET_MODE'):\n",
    "        ep = de / 'draft_edited.md'\n",
    "        decp = de / 'claims_report.json'\n",
    "        nop = de / 'continuity_notes.md'\n",
    "        draft = read_text(dp)\n",
    "        cleaned = \"\\n\".join([ln.rstrip() for ln in draft.splitlines()])\n",
    "        write_text(ep, cleaned)\n",
    "        write_json(decp, {'info': 'Editor skipped (ULTRA_BUDGET_MODE)', 'chapter': ch})\n",
    "        write_text(nop, 'Local cleanup applied; no LLM edit due to ultra budget mode.')\n",
    "        return\n",
    "\n",
    "    # --- Budget gate for editor ---\n",
    "    if tracker:\n",
    "        spent_now = tracker.spent - start_spent\n",
    "        remaining = tracker.summary().get('remaining_usd', 0.0)\n",
    "        if (spent_now >= cap) or (remaining < 0.05):\n",
    "            ep = de / 'draft_edited.md'\n",
    "            decp = de / 'claims_report.json'\n",
    "            nop = de / 'continuity_notes.md'\n",
    "            write_text(ep, read_text(dp))\n",
    "            write_json(decp, {'warning': 'editor skipped budget', 'chapter': ch})\n",
    "            write_text(nop, 'Skipped due to budget limits.')\n",
    "            return\n",
    "\n",
    "    # --- Editor pass ---\n",
    "    ep = de / 'draft_edited.md'\n",
    "    decp = de / 'claims_report.json'\n",
    "    nop = de / 'continuity_notes.md'\n",
    "    if not (ep.exists() and decp.exists() and nop.exists()):\n",
    "        dtext = read_text(dp)\n",
    "        dtext = dtext if len(dtext) < 12000 else dtext[:12000]\n",
    "        instr = (\n",
    "            \"INPUT_MD:\\n\" + dtext + \"\\n\\n\"\n",
    "            \"Return EXACTLY these blocks:\\n\"\n",
    "            \"<DRAFT_EDITED_MD>...</DRAFT_EDITED_MD>\\n\"\n",
    "            \"<CLAIMS_REPORT_JSON>{\\\"claims\\\":[\\\"...\\\"]}</CLAIMS_REPORT_JSON>\\n\"\n",
    "            \"<CONTINUITY_NOTES_MD>...</CONTINUITY_NOTES_MD>\\n\"\n",
    "            \"Keep the author's style and headings. No URLs.\"\n",
    "        )\n",
    "        R = router.editor.run(f'ch{ch}_edit', instr, max_t=800, cache_key=sha1(dtext))\n",
    "        md, cl, nt = parse_editor_blocks(R.get('text', '') or '')\n",
    "        write_text(ep, md or read_text(dp))\n",
    "        write_json(decp, cl)\n",
    "        write_text(nop, nt)\n",
    "\n",
    "print('Chapter processor ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembly ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Assembly\n",
    "from pathlib import Path  # safe even if already imported elsewhere\n",
    "\n",
    "\n",
    "def assemble_book(spec):\n",
    "    # Ensure output directory exists\n",
    "    build_dir = Path(\"build\")\n",
    "    build_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Front matter (defensive: handle missing fields gracefully)\n",
    "    title = str(spec.get(\"title\", \"Untitled\")).strip()\n",
    "    subtitle = str(spec.get(\"subtitle\", \"\") or \"\").strip()\n",
    "    author = str(spec.get(\"author\", \"Unknown Author\")).strip()\n",
    "\n",
    "    fm = [\n",
    "        f\"# {title}\",\n",
    "        f\"## {subtitle}\" if subtitle else \"\",\n",
    "        f\"**Author:** {author}\",\n",
    "        \"\",\n",
    "        \"---\",\n",
    "        \"\",\n",
    "    ]\n",
    "\n",
    "    chs, toc = [], []\n",
    "    n = int(spec.get(\"chapters\", 0))\n",
    "\n",
    "    for ch in range(1, n + 1):\n",
    "        e = Path(f\"content/edits/{ch:02d}/draft_edited.md\")\n",
    "        d = Path(f\"content/drafts/{ch:02d}/draft.md\")\n",
    "\n",
    "        if e.exists():\n",
    "            t = read_text(e)\n",
    "            src = \"edit\"\n",
    "            p = e\n",
    "        elif d.exists():\n",
    "            t = read_text(d)\n",
    "            src = \"draft\"\n",
    "            p = d\n",
    "        else:\n",
    "            # FIX: proper newline in placeholder\n",
    "            t = f\"## Chapter {ch}: (missing)\\nTODO\"\n",
    "            src = \"missing\"\n",
    "            p = None\n",
    "\n",
    "        chs.append(t.strip())\n",
    "\n",
    "        # First H2 line as chapter title (fallback to generic)\n",
    "        tl = next(\n",
    "            (ln.strip() for ln in t.splitlines() if ln.strip().startswith(\"## \")),\n",
    "            f\"Chapter {ch}\",\n",
    "        )\n",
    "\n",
    "        toc.append(\n",
    "            {\n",
    "                \"chapter\": ch,\n",
    "                \"title_line\": tl,\n",
    "                \"source\": src,\n",
    "                \"path\": str(p) if p else None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # FIX: correct join with blank line between sections; add trailing newline\n",
    "    md = \"\\n\\n\".join([x for x in fm if x] + chs) + \"\\n\"\n",
    "\n",
    "    write_text(str(build_dir / \"book.md\"), md)\n",
    "    write_json(str(build_dir / \"toc.json\"), toc)\n",
    "\n",
    "    return str(build_dir / \"book.md\"), str(build_dir / \"toc.json\")\n",
    "\n",
    "\n",
    "print(\"Assembly ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: QA\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def run_qa(spec):\n",
    "    dist_dir = Path(\"dist\")\n",
    "    dist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rpt = {\"checks\": {}, \"warnings\": [], \"metrics\": {}}\n",
    "\n",
    "    book_path = Path(\"build/book.md\")\n",
    "    ok = book_path.exists() and book_path.stat().st_size > 0\n",
    "    rpt[\"checks\"][\"book_exists\"] = bool(ok)\n",
    "    if not ok:\n",
    "        write_json(str(dist_dir / \"qa_report.json\"), rpt)\n",
    "        return rpt\n",
    "\n",
    "    t = read_text(book_path)\n",
    "\n",
    "    # Metrics\n",
    "    wc = count_words(t)\n",
    "    rpt[\"metrics\"][\"word_count\"] = wc\n",
    "\n",
    "    tgt = int(spec.get(\"target_length_words\", 20000))\n",
    "    low, hi = int(tgt * 0.9), int(tgt * 1.1)\n",
    "    within = low <= wc <= hi\n",
    "    rpt[\"checks\"][\"word_count_ok\"] = bool(within)\n",
    "    if not within:\n",
    "        rpt[\"warnings\"].append(\n",
    "            f\"Word count {wc} vs target {tgt} (acceptable range {low}-{hi})\"\n",
    "        )\n",
    "\n",
    "    # Chapter presence: prefer TOC if present; otherwise heuristic via H2 count\n",
    "    n = int(spec.get(\"chapters\", 0))\n",
    "    toc_path = Path(\"build/toc.json\")\n",
    "    toc_present = False\n",
    "    missing_list = []\n",
    "\n",
    "    if toc_path.exists():\n",
    "        try:\n",
    "            toc = read_json(toc_path) or []\n",
    "            toc_present = (len(toc) == n) and all(\n",
    "                item.get(\"source\", \"unknown\") != \"missing\" for item in toc\n",
    "            )\n",
    "            if not toc_present and toc:\n",
    "                # note which chapters the TOC marked missing (if our Cell 8 added 'source')\n",
    "                for idx, item in enumerate(toc, start=1):\n",
    "                    if item.get(\"source\", \"unknown\") == \"missing\":\n",
    "                        missing_list.append(idx)\n",
    "        except Exception:\n",
    "            toc_present = False\n",
    "\n",
    "    if not toc_path.exists() or not toc_present:\n",
    "        # Fallback: require at least n H2 headings (## ...)\n",
    "        h2_count = sum(1 for ln in t.splitlines() if ln.strip().startswith(\"## \"))\n",
    "        toc_present = h2_count >= n\n",
    "        if not toc_present:\n",
    "            rpt[\"warnings\"].append(\n",
    "                f\"Expected {n} chapters but found {h2_count} H2 headings.\"\n",
    "            )\n",
    "\n",
    "    rpt[\"checks\"][\"all_chapters_present\"] = bool(toc_present)\n",
    "    if missing_list:\n",
    "        rpt[\"warnings\"].append(f\"Chapters marked missing in TOC: {missing_list}\")\n",
    "\n",
    "    # TODO/FIXME markers\n",
    "    todo_hits = re.findall(r\"\\b(?:TODO|FIXME)\\b\", t)\n",
    "    rpt[\"checks\"][\"no_todo_fixme\"] = len(todo_hits) == 0\n",
    "    if todo_hits:\n",
    "        rpt[\"warnings\"].append(f\"TODO/FIXME remain: {len(todo_hits)} occurrences\")\n",
    "\n",
    "    # Heading level sanity: flag H4+ outside code fences (skip ``` blocks)\n",
    "    def strip_codeblocks(s):\n",
    "        out, in_code = [], False\n",
    "        for line in s.splitlines():\n",
    "            if line.strip().startswith(\"```\"):\n",
    "                in_code = not in_code\n",
    "                continue\n",
    "            if not in_code:\n",
    "                out.append(line)\n",
    "        return \"\\n\".join(out)\n",
    "\n",
    "    text_no_code = strip_codeblocks(t)\n",
    "    head_ok = \"####\" not in text_no_code\n",
    "    rpt[\"checks\"][\"heading_levels_ok\"] = bool(head_ok)\n",
    "    if not head_ok:\n",
    "        rpt[\"warnings\"].append(\"Headings exceed H3 (#### found)\")\n",
    "\n",
    "    # Citations check when research is enabled for nonfiction\n",
    "    genre = str(spec.get(\"genre\", \"\")).lower()\n",
    "    research_enabled = bool(\n",
    "        spec.get(\"research_enabled\")\n",
    "        or (\n",
    "            isinstance(spec.get(\"research_policy\"), dict)\n",
    "            and spec[\"research_policy\"].get(\"enabled\")\n",
    "        )\n",
    "    )\n",
    "    cites_ok = True\n",
    "    if genre == \"nonfiction\" and research_enabled:\n",
    "        cites_ok = \"[n]\" in t\n",
    "        if not cites_ok:\n",
    "            rpt[\"warnings\"].append(\n",
    "                \"No [n] markers while research is enabled for nonfiction.\"\n",
    "            )\n",
    "    rpt[\"checks\"][\"citations_present_if_research\"] = bool(cites_ok)\n",
    "\n",
    "    # Check end-matter sections per chapter (if template defines them)\n",
    "    end_matter = (spec.get('chapter_template', {}) or {}).get('end_matter', []) or []\n",
    "    if end_matter:\n",
    "        for ch in range(1, int(spec.get('chapters', 10) or 10) + 1):\n",
    "            block = re.findall(fr\"^##\\s+Chapter\\s+{ch}:[\\s\\S]*?(?=^##\\s+Chapter\\s+{ch+1}:|\\Z)\", t, flags=re.M)\n",
    "            if block:\n",
    "                missing = [s for s in end_matter if s not in block[0]]\n",
    "                if missing:\n",
    "                    rpt['warnings'].append(f'Ch {ch} missing end-matter: {missing}')\n",
    "\n",
    "    # Flag very long paragraphs (~>220 words)\n",
    "    paras = re.split(r'\\n\\s*\\n', t)\n",
    "    if any(len(p.split()) > 220 for p in paras):\n",
    "        rpt['warnings'].append('Some paragraphs exceed ~220 words.')\n",
    "\n",
    "    # Double-space / trailing-space hygiene\n",
    "    if re.search(r'[^\\S\\r\\n]{2,}', t):\n",
    "        rpt['warnings'].append('Double spaces detected.')\n",
    "    if re.search(r'[ \\t]+$', t, flags=re.M):\n",
    "        rpt['warnings'].append('Trailing spaces at line ends.')\n",
    "\n",
    "    # Claims mismatch: [n] markers vs. claims_report.json presence\n",
    "    claims_files = list(Path('content/edits').rglob('*/claims_report.json'))\n",
    "    has_claims_files = any(Path(cf).exists() and Path(cf).stat().st_size > 2 for cf in claims_files)\n",
    "    has_markers = ('[n]' in t)\n",
    "    if has_markers and not has_claims_files:\n",
    "        rpt['warnings'].append('Found [n] markers but no claims_report.json files.')\n",
    "    if has_claims_files and not has_markers:\n",
    "        rpt['warnings'].append('claims_report.json exists but no [n] markers found in book.')\n",
    "\n",
    "    # Outcome\n",
    "    rpt[\"outcome\"] = \"PASS\" if not rpt[\"warnings\"] else \"PASS_WITH_WARNINGS\"\n",
    "\n",
    "    write_json(str(dist_dir / \"qa_report.json\"), rpt)\n",
    "    return rpt\n",
    "\n",
    "\n",
    "print(\"QA ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Export\n",
    "from pathlib import Path\n",
    "import shutil, zipfile, re\n",
    "\n",
    "\n",
    "def minimal_docx(\n",
    "    p, note=\"DOCX export unavailable; install python-docx. See dist/book.md\"\n",
    "):\n",
    "    p = Path(p)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # Basic, valid DOCX container with a single paragraph note\n",
    "    safe = note.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
    "    with zipfile.ZipFile(p, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "        z.writestr(\n",
    "            \"[Content_Types].xml\",\n",
    "            '<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>'\n",
    "            '<Types xmlns=\"http://schemas.openxmlformats.org/package/2006/content-types\">'\n",
    "            '<Default Extension=\"rels\" ContentType=\"application/vnd.openxmlformats-package.relationships+xml\"/>'\n",
    "            '<Default Extension=\"xml\" ContentType=\"application/xml\"/>'\n",
    "            '<Override PartName=\"/word/document.xml\" ContentType=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document.main+xml\"/>'\n",
    "            \"</Types>\",\n",
    "        )\n",
    "        z.writestr(\n",
    "            \"_rels/.rels\",\n",
    "            '<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>'\n",
    "            '<Relationships xmlns=\"http://schemas.openxmlformats.org/package/2006/relationships\">'\n",
    "            '<Relationship Id=\"rId1\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/officeDocument\" Target=\"word/document.xml\"/>'\n",
    "            \"</Relationships>\",\n",
    "        )\n",
    "        z.writestr(\n",
    "            \"word/document.xml\",\n",
    "            '<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>'\n",
    "            '<w:document xmlns:w=\"http://schemas.openxmlformats.org/wordprocessingml/2006/main\">'\n",
    "            \"<w:body><w:p><w:r><w:t>\"\n",
    "            + safe\n",
    "            + \"</w:t></w:r></w:p></w:body></w:document>\",\n",
    "        )\n",
    "\n",
    "\n",
    "def export_deliverables():\n",
    "    src = Path(\"build/book.md\")\n",
    "    dst = Path(\"dist/book.md\")\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if src.exists():\n",
    "        shutil.copyfile(src, dst)\n",
    "        md = read_text(src)\n",
    "    else:\n",
    "        md = \"# Book (missing)\\n\"\n",
    "        write_text(dst, md)\n",
    "\n",
    "    # Try rich DOCX via python-docx; otherwise create a minimal DOCX with a note\n",
    "    try:\n",
    "        import docx\n",
    "        from docx.enum.text import WD_BREAK  # page breaks\n",
    "\n",
    "        d = docx.Document()\n",
    "\n",
    "        def _sanitize_docx_text(s: str) -> str:\n",
    "            # Replace tabs with spaces and strip control chars\n",
    "            s = s.replace(\"\\t\", \"    \")\n",
    "            return \"\".join(ch if ord(ch) >= 0x20 else \" \" for ch in s)\n",
    "\n",
    "        in_code = False\n",
    "        for ln in md.splitlines():\n",
    "            s = _sanitize_docx_text(ln.rstrip(\"\\n\"))\n",
    "\n",
    "            # Toggle code blocks on fenced markers\n",
    "            if s.strip().startswith(\"```\"):\n",
    "                in_code = not in_code\n",
    "                continue\n",
    "\n",
    "            if in_code:\n",
    "                p = d.add_paragraph(s)\n",
    "                # 'Code' style may not exist; fall back silently\n",
    "                try:\n",
    "                    p.style = \"Code\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "                continue\n",
    "\n",
    "            if not s.strip():\n",
    "                d.add_paragraph(\"\")\n",
    "                continue\n",
    "\n",
    "            if s.startswith(\"### \"):\n",
    "                d.add_heading(s[4:], level=3)\n",
    "            elif s.startswith(\"## \"):\n",
    "                d.add_heading(s[3:], level=2)\n",
    "            elif s.startswith(\"# \"):\n",
    "                d.add_heading(s[2:], level=1)\n",
    "            elif s.strip() == \"---\":\n",
    "                d.add_page_break()\n",
    "            elif s.lstrip().startswith((\"- \", \"* \")):\n",
    "                d.add_paragraph(s.lstrip()[2:], style=\"List Bullet\")\n",
    "            elif re.match(r\"^\\s*\\d+\\.\\s+\", s):\n",
    "                d.add_paragraph(re.sub(r\"^\\s*\\d+\\.\\s+\", \"\", s), style=\"List Number\")\n",
    "            else:\n",
    "                d.add_paragraph(s)\n",
    "\n",
    "        d.save(\"dist/book.docx\")\n",
    "    except Exception as e:\n",
    "        minimal_docx(\n",
    "            \"dist/book.docx\",\n",
    "            note=f\"DOCX export unavailable ({e.__class__.__name__}). See dist/book.md\",\n",
    "        )\n",
    "\n",
    "    return str(dst), \"dist/book.docx\"\n",
    "\n",
    "\n",
    "print(\"Export ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Manifest\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "def manifest(tr, outcome):\n",
    "    def scan(d):\n",
    "        root = Path(d)\n",
    "        items = []\n",
    "        if not root.exists():\n",
    "            return items\n",
    "        for p in sorted(root.rglob(\"*\")):\n",
    "            if p.is_file():\n",
    "                st = p.stat()\n",
    "                items.append(\n",
    "                    {\n",
    "                        \"path\": p.as_posix(),\n",
    "                        \"size_bytes\": st.st_size,\n",
    "                        \"mtime\": datetime.fromtimestamp(st.st_mtime, timezone.utc)\n",
    "                        .isoformat()\n",
    "                        .replace(\"+00:00\", \"Z\"),\n",
    "                    }\n",
    "                )\n",
    "        return items\n",
    "\n",
    "    Path(\"dist\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    m = {\n",
    "        \"outcome\": outcome,\n",
    "        \"generated_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
    "        \"files\": {\n",
    "            \"build\": scan(\"build\"),\n",
    "            \"dist\": scan(\"dist\"),\n",
    "        },\n",
    "        \"cost_summary\": tr.summary() if hasattr(tr, \"summary\") else None,\n",
    "    }\n",
    "\n",
    "    write_json(\"dist/manifest.json\", m)\n",
    "    return m\n",
    "\n",
    "\n",
    "print(\"Manifest ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controls ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Run-All / Rerun\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def make_router():\n",
    "    tr = CostTracker(pipeline_config[\"RUN_COST_CAP_USD\"])\n",
    "    rt = PMRouter(book_spec, pipeline_config, tr)\n",
    "    return rt, tr\n",
    "\n",
    "\n",
    "def run_all():\n",
    "    Path(\"logs\").mkdir(parents=True, exist_ok=True)\n",
    "    if os.getenv('DRY_RUN') == '1':\n",
    "        print('[run_all] DRY_RUN=1 → skipping API calls; using synthetic outputs.')\n",
    "    out = \"PASS\"\n",
    "    tr = None  # ensure defined for finally\n",
    "    try:\n",
    "        rt, tr = make_router()\n",
    "\n",
    "        # Style & outline\n",
    "        gen_style(book_spec)\n",
    "        pj, pm = gen_outline(book_spec, tr, rt)\n",
    "\n",
    "        # Chapter selection based on config\n",
    "        total = int(book_spec.get(\"chapters\", 0))\n",
    "        if pipeline_config.get(\"FULL_RUN\", True):\n",
    "            chs = list(range(1, total + 1))\n",
    "        else:\n",
    "            sample_n = int(pipeline_config.get(\"SAMPLE_RUN_CHAPTERS\", 2))\n",
    "            chs = list(range(1, min(total, sample_n) + 1))\n",
    "\n",
    "        # Per-chapter processing with cost-cap handling\n",
    "        for ch in chs:\n",
    "            try:\n",
    "                process_chapter(ch, pipeline_config, tr, rt, pj)\n",
    "            except CostCapExceededException as e:\n",
    "                write_text(\"logs/last_error.txt\", str(e))\n",
    "                out = \"ABORTED_COST_CAP\"\n",
    "                break\n",
    "\n",
    "        # Assembly, QA, Export\n",
    "        out = \"PASS\"\n",
    "        try:\n",
    "            assemble_book(book_spec)\n",
    "        except Exception as e:\n",
    "            log_exc(\"assemble_book\", e)\n",
    "            return \"FAILED_STEP\"\n",
    "\n",
    "        try:\n",
    "            qa = run_qa(book_spec)\n",
    "            if qa.get(\"warnings\"):\n",
    "                out = \"PASS_WITH_WARNINGS\" if out == \"PASS\" else out\n",
    "        except Exception as e:\n",
    "            log_exc(\"run_qa\", e)\n",
    "            return \"FAILED_STEP\"\n",
    "\n",
    "        try:\n",
    "            export_deliverables()\n",
    "        except Exception as e:\n",
    "            log_exc(\"export_deliverables\", e)\n",
    "            return \"FAILED_STEP\"\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    except CostCapExceededException as e:\n",
    "        write_text(\"logs/last_error.txt\", str(e))\n",
    "        out = \"ABORTED_COST_CAP\"\n",
    "    except Exception as e:\n",
    "        write_text(\"logs/last_error.txt\", \"FAILED_STEP: \" + str(e))\n",
    "        out = \"FAILED_STEP\"\n",
    "    finally:\n",
    "        try:\n",
    "            manifest(tr, out) if tr else None\n",
    "        except Exception as e:\n",
    "            # last-ditch logging if manifest itself fails\n",
    "            write_text(\"logs/last_error.txt\", \"MANIFEST_FAILED: \" + str(e))\n",
    "\n",
    "    print(\"Run done\", out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def rerun_chapter(n):\n",
    "    n = int(n)\n",
    "    # Clear prior artifacts for this chapter\n",
    "    for sub in [\"research\", \"drafts\", \"edits\"]:\n",
    "        d = Path(f\"content/{sub}/{n:02d}\")\n",
    "        if d.exists():\n",
    "            for p in d.glob(\"*\"):\n",
    "                if p.is_file():\n",
    "                    p.unlink()\n",
    "\n",
    "    Path(\"logs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rt, tr = make_router()\n",
    "    pj = \"content/outline/outline.json\"\n",
    "    if not Path(pj).exists():\n",
    "        pj, _ = gen_outline(book_spec, tr, rt)\n",
    "\n",
    "    process_chapter(n, pipeline_config, tr, rt, pj)\n",
    "    assemble_book(book_spec)\n",
    "    run_qa(book_spec)\n",
    "    export_deliverables()\n",
    "    manifest(tr, \"PASS\")\n",
    "    print(\"Rerun done\", n)\n",
    "\n",
    "\n",
    "def resume():\n",
    "    return run_all()\n",
    "\n",
    "def verify_author_calls():\n",
    "    from pathlib import Path, PurePosixPath\n",
    "    import json\n",
    "    print(\"AuthorAgent call verification:\\n\")\n",
    "    any_found = False\n",
    "    # Chapter stamps\n",
    "    for p in sorted(Path(\"content/drafts\").rglob(\"*/.author_call.json\")):\n",
    "        any_found = True\n",
    "        d = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "        ch = p.parent.name\n",
    "        print(f\"- drafts/{ch}: called @ {d.get('t')} | cached={d.get('cached')} | title=\\\"{d.get('outline_title','')}\\\"\")\n",
    "    if not any_found:\n",
    "        print(\"- No .author_call.json stamps found in content/drafts/*\")\n",
    "    print(\"\\nRecent agent call events (tail 8):\")\n",
    "    log = Path(\"logs/agent_calls.jsonl\")\n",
    "    if log.exists():\n",
    "        lines = log.read_text(encoding=\"utf-8\").splitlines()[-8:]\n",
    "        for ln in lines:\n",
    "            print(\"  \", ln)\n",
    "    else:\n",
    "        print(\"  logs/agent_calls.jsonl (missing)\")\n",
    "        \n",
    "print(\"Controls ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting DOCX to PDF via docx2pdf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccb59b2c5104a108f79ff3ac9a25b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF saved at: dist\\book.pdf\n",
      "Outcome: PASS_WITH_WARNINGS\n",
      "Cost: {'total_spent_usd': 0.003036, 'run_cap_usd': 3.0, 'remaining_usd': 2.996964, 'log_items': 10}\n",
      "\n",
      "Build:\n",
      "build/book.md (9115 bytes)\n",
      "build/toc.json (1292 bytes)\n",
      "\n",
      "Dist:\n",
      "dist/book.docx (40907 bytes)\n",
      "dist/book.md (9115 bytes)\n",
      "dist/book.pdf (166061 bytes)\n",
      "dist/manifest.json (950 bytes)\n",
      "dist/qa_report.json (1609 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Demonstration Mode\n",
    "from pathlib import Path\n",
    "import os, sys, shutil, subprocess\n",
    "\n",
    "# Cheap demo run: sample a couple of chapters and disable research\n",
    "pipeline_config[\"SAMPLE_RUN_CHAPTERS\"] = int(pipeline_config.get(\"SAMPLE_RUN_CHAPTERS\", 2))\n",
    "pipeline_config[\"FULL_RUN\"] = False\n",
    "pipeline_config[\"RESEARCH_ENABLED\"] = False\n",
    "\n",
    "try:\n",
    "    outcome = run_all()\n",
    "except Exception as e:\n",
    "    Path(\"dist\").mkdir(parents=True, exist_ok=True)\n",
    "    manifest = {\n",
    "        \"outcome\": \"FAILED_STEP\",\n",
    "        \"failed_step\": \"run_all_top_level\",\n",
    "        \"error\": str(e),\n",
    "        \"trace\": traceback.format_exc(),\n",
    "        \"t\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
    "    }\n",
    "    Path(\"dist/manifest.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "    print(\"run_all crashed. Details saved to dist/manifest.json\")\n",
    "    outcome = \"FAILED_STEP\"\n",
    "\n",
    "docx_path = Path(\"dist/book.docx\")\n",
    "pdf_path = Path(\"dist/book.pdf\")\n",
    "pdf_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _have_exe(cmd: str) -> bool:\n",
    "    return shutil.which(cmd) is not None\n",
    "\n",
    "def convert_docx_to_pdf(src: Path, dst: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Best-effort DOCX→PDF:\n",
    "      1) docx2pdf (Windows/macOS w/ MS Word)\n",
    "      2) LibreOffice soffice --headless (cross-platform)\n",
    "      3) pandoc (if installed)\n",
    "    Returns True on success, False otherwise.\n",
    "    \"\"\"\n",
    "    # 1) docx2pdf on win/darwin only\n",
    "    try:\n",
    "        if sys.platform in (\"win32\", \"darwin\"):\n",
    "            from docx2pdf import convert  # may raise ImportError\n",
    "            print(\"Converting DOCX to PDF via docx2pdf...\")\n",
    "            convert(str(src), str(dst))\n",
    "            return dst.exists() and dst.stat().st_size > 0\n",
    "        else:\n",
    "            print(\"docx2pdf is typically not supported on this platform; skipping that route.\")\n",
    "    except Exception as e:\n",
    "        print(f\"docx2pdf failed: {e}\")\n",
    "\n",
    "    # 2) LibreOffice (soffice)\n",
    "    if _have_exe(\"soffice\"):\n",
    "        try:\n",
    "            print(\"Converting via LibreOffice (soffice --headless)...\")\n",
    "            # LibreOffice writes into output directory; we move/rename if needed\n",
    "            outdir = dst.parent\n",
    "            cmd = [\n",
    "                \"soffice\", \"--headless\", \"--convert-to\", \"pdf\",\n",
    "                \"--outdir\", str(outdir), str(src)\n",
    "            ]\n",
    "            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            # LibreOffice names file as <name>.pdf in outdir\n",
    "            produced = outdir / (src.stem + \".pdf\")\n",
    "            if produced.exists():\n",
    "                if produced != dst:\n",
    "                    produced.replace(dst)\n",
    "                return dst.exists() and dst.stat().st_size > 0\n",
    "        except Exception as e:\n",
    "            print(f\"LibreOffice conversion failed: {e}\")\n",
    "\n",
    "    # 3) Pandoc\n",
    "    if _have_exe(\"pandoc\"):\n",
    "        try:\n",
    "            print(\"Converting via pandoc...\")\n",
    "            subprocess.run([\"pandoc\", str(src), \"-o\", str(dst)], check=True)\n",
    "            return dst.exists() and dst.stat().st_size > 0\n",
    "        except Exception as e:\n",
    "            print(f\"pandoc conversion failed: {e}\")\n",
    "\n",
    "    # No available converter\n",
    "    print(\"No PDF converter available (docx2pdf/soffice/pandoc not working). Skipping PDF export.\")\n",
    "    return False\n",
    "\n",
    "if docx_path.exists():\n",
    "    success = convert_docx_to_pdf(docx_path, pdf_path)\n",
    "    if success:\n",
    "        print(f\"PDF saved at: {pdf_path}\")\n",
    "    else:\n",
    "        print(\"PDF export skipped or failed. See messages above.\")\n",
    "else:\n",
    "    print(\"DOCX file not found. Cannot export PDF.\")\n",
    "\n",
    "m = read_json(\"dist/manifest.json\") or {}\n",
    "print(\"Outcome:\", m.get(\"outcome\", outcome))\n",
    "print(\"Cost:\", m.get(\"cost_summary\"))\n",
    "\n",
    "def tree(d):\n",
    "    d = Path(d)\n",
    "    if not d.exists():\n",
    "        print(f\"{d.as_posix()} (missing)\")\n",
    "        return\n",
    "    for p in sorted(d.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            rel = p.relative_to(d).as_posix()\n",
    "            print(f\"{d.as_posix()}/{rel} ({p.stat().st_size} bytes)\")\n",
    "\n",
    "print(\"\\nBuild:\")\n",
    "tree(\"build\")\n",
    "\n",
    "print(\"\\nDist:\")\n",
    "tree(\"dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d866d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome: PASS_WITH_WARNINGS\n",
      "Failed step: None\n",
      "Notes: None\n",
      "Artifacts: None\n",
      "\n",
      "Recent logs: []\n",
      "\n",
      "build/book.md exists: True | size: 9115\n",
      "dist/book.docx exists: True | size: 40907\n",
      "\n",
      "Build tree:\n",
      "build/book.md (9115 bytes)\n",
      "build/toc.json (1292 bytes)\n",
      "\n",
      "Dist tree:\n",
      "dist/book.docx (40907 bytes)\n",
      "dist/book.md (9115 bytes)\n",
      "dist/book.pdf (166061 bytes)\n",
      "dist/manifest.json (950 bytes)\n",
      "dist/qa_report.json (1609 bytes)\n",
      "AuthorAgent call verification:\n",
      "\n",
      "- drafts/01: called @ 2025-09-03T21:29:56.053949Z | cached=False | title=\"The Whir in the Mango Tree\"\n",
      "- drafts/02: called @ 2025-09-03T21:30:26.323508Z | cached=False | title=\"A Friend Called Pip\"\n",
      "\n",
      "Recent agent call events (tail 8):\n",
      "   {\"t\": \"2025-09-03T21:29:43.433164Z\", \"agent\": \"Author\", \"model\": \"gpt-4o-mini\", \"label\": \"ch1_draft\", \"event\": \"begin\", \"cache_key\": \"0dd4287bfdd9ab12f2fe81e0d3b203e7d6e7e9b1\", \"max_t\": 900, \"force_json\": false}\n",
      "   {\"t\": \"2025-09-03T21:29:56.047657Z\", \"agent\": \"Author\", \"model\": \"gpt-4o-mini\", \"label\": \"ch1_draft\", \"event\": \"llm_ok\", \"duration_s\": 12.612, \"usage\": {\"prompt_tokens\": 295, \"completion_tokens\": 900, \"via\": \"chat.completions\"}}\n",
      "   {\"t\": \"2025-09-03T21:29:56.057051Z\", \"agent\": \"Editor\", \"model\": \"gpt-4o-mini\", \"label\": \"ch1_edit\", \"event\": \"begin\", \"cache_key\": \"782712d16f72353aae815e14970d67d222b6a290\", \"max_t\": 800, \"force_json\": false}\n",
      "   {\"t\": \"2025-09-03T21:30:12.759070Z\", \"agent\": \"Editor\", \"model\": \"gpt-4o-mini\", \"label\": \"ch1_edit\", \"event\": \"llm_ok\", \"duration_s\": 16.701, \"usage\": {\"prompt_tokens\": 1121, \"completion_tokens\": 800, \"via\": \"chat.completions\"}}\n",
      "   {\"t\": \"2025-09-03T21:30:12.768738Z\", \"agent\": \"Author\", \"model\": \"gpt-4o-mini\", \"label\": \"ch2_draft\", \"event\": \"begin\", \"cache_key\": \"3c2c470e8d4d6a093219db8bcd5fd8a22783a9f7\", \"max_t\": 900, \"force_json\": false}\n",
      "   {\"t\": \"2025-09-03T21:30:26.316925Z\", \"agent\": \"Author\", \"model\": \"gpt-4o-mini\", \"label\": \"ch2_draft\", \"event\": \"llm_ok\", \"duration_s\": 13.547, \"usage\": {\"prompt_tokens\": 289, \"completion_tokens\": 900, \"via\": \"chat.completions\"}}\n",
      "   {\"t\": \"2025-09-03T21:30:26.326506Z\", \"agent\": \"Editor\", \"model\": \"gpt-4o-mini\", \"label\": \"ch2_edit\", \"event\": \"begin\", \"cache_key\": \"d2721da30f1520b75f270fadacc49cadf6da09ac\", \"max_t\": 800, \"force_json\": false}\n",
      "   {\"t\": \"2025-09-03T21:30:39.035150Z\", \"agent\": \"Editor\", \"model\": \"gpt-4o-mini\", \"label\": \"ch2_edit\", \"event\": \"llm_ok\", \"duration_s\": 12.708, \"usage\": {\"prompt_tokens\": 1122, \"completion_tokens\": 800, \"via\": \"chat.completions\"}}\n"
     ]
    }
   ],
   "source": [
    "# TRIAGE CELL — diagnose failing step and what's been produced\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def _safe_json(path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't parse {path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def _tree(d):\n",
    "    d = Path(d)\n",
    "    if not d.exists():\n",
    "        print(f\"{d.as_posix()} (missing)\")\n",
    "        return\n",
    "    for p in sorted(d.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            rel = p.relative_to(d).as_posix()\n",
    "            print(f\"{d.as_posix()}/{rel} ({p.stat().st_size} bytes)\")\n",
    "\n",
    "# 1) What the manifest says\n",
    "man = _safe_json(\"dist/manifest.json\")\n",
    "print(\"Outcome:\", man.get(\"outcome\"))\n",
    "print(\"Failed step:\", man.get(\"failed_step\") or man.get(\"last_step\") or man.get(\"step\"))\n",
    "print(\"Notes:\", man.get(\"notes\"))\n",
    "print(\"Artifacts:\", man.get(\"artifacts\"))\n",
    "\n",
    "# 2) Recent logs (names + preview of newest)\n",
    "logs = sorted(Path(\"logs\").glob(\"*.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "print(\"\\nRecent logs:\", [p.name for p in logs[:6]])\n",
    "if logs:\n",
    "    lp = logs[0]\n",
    "    print(f\"\\nLast log preview: {lp.name}\\n\")\n",
    "    txt = lp.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    print(txt[:2000] + (\"...\" if len(txt) > 2000 else \"\"))\n",
    "\n",
    "# 3) Check if build/book.md or dist/book.docx exist (helps decide whether to re-export)\n",
    "b = Path(\"build/book.md\")\n",
    "d = Path(\"dist/book.docx\")\n",
    "print(\"\\nbuild/book.md exists:\", b.exists(), \"| size:\", (b.stat().st_size if b.exists() else 0))\n",
    "print(\"dist/book.docx exists:\", d.exists(), \"| size:\", (d.stat().st_size if d.exists() else 0))\n",
    "\n",
    "# 4) Quick trees\n",
    "print(\"\\nBuild tree:\")\n",
    "_tree(\"build\")\n",
    "print(\"\\nDist tree:\")\n",
    "_tree(\"dist\")\n",
    "\n",
    "verify_author_calls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8cdfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
